#### Kubernetes RBAC for Secure Access
 - Create a Service Account for Jenkins - helm-chart/templates/serviceaccount.yaml
 - Create RBAC Role & Binding - helm-chart/templates/rbac-role.yaml & helm-chart/templates/rbac-rolebinding.yaml , Now, Jenkins will use this service account to deploy securely.
 - Ingress Controller for External Access - helm-chart/templates/ingress.yaml

#### Static & Dynamic IP's
- **Static IP Address**
     - A fixed, permanent IP address assigned to a device.
     - Doesn't change unless manually reconfigured.
     - We use  them for servers, remote access, and hosting services.
     - those are more reliable for applications needing constant connectivity (e.g., web servers, VPNs).
     - Can be more expensive since ISPs charge extra for static IPs.
- **Dynamic IP Address**
     - Automatically assigned by a DHCP server.
     - Changes periodically when a device reconnects or after lease expiration.
     - Used by most home users and businesses for general internet access.
     - More secure by default since the IP changes regularly.
     - Cost-effective and widely used by ISPs.

#### Docker architecture
- its a client-server architecture with three main components:
     - **Docker Client**
     - **Docker Daemon**(dockerd)
     - **Docker engine**
     - other Docker Objects (Images, Containers, Volumes, registry and Networks).
- The Client interacts with the Daemon using REST APIs, the Daemon manages containers using namespaces and cgroups.

- Docker components
     -  **Docker Client (CLI)**		# Sends commands to the Docker Daemon.
     -  **Docker Daemon (dockerd)**	# Manages images, containers, networks, and volumes.
     -  **Docker Images**		# Read-only templates used to create containers.
     -  **Docker Containers**		# Running instances of images, providing an isolated environment.
     -  **Docker Registry**		# Stores and distributes images (e.g., Docker Hub, private registries).
     -  **Docker Storage**		# Includes Volumes (persistent data storage) and Bind Mounts (direct host folder access).
     -  **Docker Networking**		# Connects containers using different network types (Bridge, Host, Overlay, etc.).
     -  **Docker Compose**		# Manages multi-container applications using a YAML file.
     -  **Docker Orchestration**	# Tools like Docker Swarm and Kubernetes for container management at scale.
- Docker compose
     - For complex applications which requires multiple services, we can use docker compose ( docker-compose.ynl )
- Docker Networking
     - Docker provides bridge, host, and overlay networks. 
     - By default, containers use bridge networking, allowing communication with each other via a virtual subnet.
     - Overlay networks are used in Docker Swarm for cross-host networking.
- Docker Deployment Workflow
     - Writing a Dockerfile
     - Building a image
     - running a docker image
     - pushing the docker image to the registry ( Docker Hub or AWS ECR )
- Overlay Network docker
     - its a virtual network that runs on top of an existing physical network.
     - It enables communication between containers, VMs, services across multiple hosts without requiring changes to the underlying infrastructure.
     - Creates a logical layer over a physical network.
     - Provides isolated and secure connectivity.
     - Supports multi-host communication for containerized environments.

#### Docker Labels
- Those are like key-value metadata assigned to Docker images, containers, volumes, networks.
- They help organize, document, manage resources.
     - **Metadata tagging** – Add versioning, authorship, and environment details.
     - **Automation** – Labels help tools like Prometheus, Swarm, and Kubernetes manage resources.
     - **Container discovery** – Helps monitoring tools like Datadog and ELK filter logs by labels.
     - **Resource management** – Helps with cost tracking, debugging, and compliance.

#### Docker Client
- The Docker client is a command-line tool (docker CLI) used to interact with the Docker daemon.
     - It sends commands (docker build, docker run, docker pull, etc.) to the Docker daemon.
     - The client communicates with the daemon using REST API, UNIX sockets, or TCP.

#### Docker Daemon (dockerd) & Enginer
- it runs on the host machine and is responsible for managing Docker objects like images, containers, networks, volumes.
     - It listens for requests from the Docker client and processes them.

- Docker Daemon: Handles requests and manages containers.
     - REST API: Allows communication between the client and daemon.
     - CLI (Command Line Interface): Used to interact with Docker via commands.

#### Docker Images
- its a lightweight, standalone, executable software package that includes everything needed to run an application.
     - Images are built using a Dockerfile.
     - Stored in Docker Hub or private registries.
- Commands
   -  **docker images**	Lists available Docker images.**
   -  **docker pull <image>**	Downloads an image from Docker Hub.
   -  **docker push <image>**	Uploads an image to Docker Hub.
   -  **docker rmi <image>**	Removes an image from the local system.
   -  **docker tag <image> <new_image>**	Tags an image with a new name.
   -  **docker build - t <image_name> .**	Builds an image from a Dockerfile.
   -  **docker save - o <image.tar> <image>**	Saves an image as a tar archive.
   -  **docker load - i <image.tar>**	Loads an image from a tar archive.
   -  **docker history <image>**	Shows the history of an image's layers.

#### Docker Containers & registry
- A container is a running instance of a Docker image.
     - It is isolated, lightweight, and portable.
     - Containers share the host OS kernel but run independently.

- A Docker registry stores and distributes images.
     - Examples: Docker Hub (public), AWS ECR, Azure ACR, Google GCR, Harbor (private registries).

#### Docker Volume & Storage
- Used for persistent storage for containers.
     - Volumes allow data to persist even if a container is stopped or deleted.
- Commands
   -  **docker volume ls**	Lists all volumes.
   -  **docker volume create <volume_name>**	Creates a new volume.
   -  **docker volume inspect <volume_name>**	Displays details of a volume.
   -  **docker volume rm <volume_name>**	Deletes a volume.
   -  **docker volume prune**	Removes all unused volumes.

#### Docker Networking
- Allows containers to communicate with each other and external services.
- Types of Docker networks:
     - Bridge (default, for container-to-container communication on the same host)
     - Host (shares the host’s network namespace)
     - Overlay (used in Docker Swarm for multi-host networking)
     - None (no network)
- Commands
   -  **docker network ls**	Lists all Docker networks.
   -  **docker network create <network_name>**	Creates a new Docker network.
   -  **docker network inspect <network_name>**	Displays information about a network.
   -  **docker network connect <network> <container>**	Connects a container to a network.
   -  **docker network disconnect <network> <container>**	Disconnects a container from a network.
   -  **docker network rm <network_name>**	Removes a Docker network.

#### Docker Workflow
- Build → Use a Dockerfile to create an image (docker build -t myapp .).
     - Push → Store the image in a registry (docker push myapp).
     - Pull → Download the image from a registry (docker pull myapp).
     - Run → Create a container from an image (docker run -d myapp).
     - Manage → Monitor, scale, orchestrate containers.

#### Sample dockerfile Node.js 
```
FROM node:18-alpine		# Use the official Node.js image
WORKDIR /app			# Set working directory inside the container
COPY package*.json ./		# Copy package files first (for better caching)
RUN npm install			# Install dependencies
COPY . .			# Copy the rest of the app
EXPOSE 3000			# Expose the port the app runs on
CMD ["npm", "start"]		# Start the app
```

#### Example Dockerfile Java
- Dockerfile in the root of your repo
```
FROM openjdk:17-jdk-alpine			# Use an official Java runtime as the base image
WORKDIR /app					# Set the working directory inside the container
COPY myapp.jar app.jar				# Copy the jar file into the container
ENTRYPOINT ["java", "-jar", "app.jar"]		# Set the command to run the jar
```

#### Dockerfile contents
- in-detail
     - **FROM**		# Specifies the base image to use for the container.
     - **LABEL**	# Adds metadata to the image (e.g., version, maintainer).
     - **RUN**		# Executes commands in the container, typically to install packages or perform system configuration.
     - **COPY**		# Copies files or directories from the host into the container.
     - **ADD**		# Similar to COPY, but with additional features like extracting compressed files and fetching files from URLs.
     - **CMD**		# Specifies the default command to run when the container starts.
     - **ENTRYPOINT**	# Sets the command that is always executed when the container starts (can be overridden by CMD).
     - **ENV**		# Sets environment variables inside the container.
     - **EXPOSE**	# Specifies the ports the container will listen on at runtime.
     - **VOLUME**	# Defines mount points for volumes (persistent storage).
     - **USER**		# Sets the user name or UID to use when running the container.
     - **WORKDIR**	# Sets the working directory for any RUN, CMD, ENTRYPOINT, COPY, and ADD instructions that follow.
     - **ARG**		# Defines build  - time variables that can be passed to the Docker build process.
     - **STOPSIGNAL**	# Sets the system call signal to stop the container.
     - **SHELL**	# Specifies the shell to use for the RUN commands.

#### Docker Container Lifecycle Management
- process of creating, running, monitoring, updating, and eventually deleting containers in an automated and consistent way. It ensures that containers are well-managed throughout their existence.
   -  **docker run <image>** Creates and starts a new container from an image.
   -  **docker run   - d <image>**	Runs a container in detached mode (background).
   -  **docker run   - it <image> /bin/bash**	Runs a container interactively with a shell.
   -  **docker ps**	Lists running containers.
   -  **docker ps   - a**	Lists all containers (including stopped ones).
   -  **docker start <container_id>**	Starts a stopped container.
   -  **docker stop <container_id>**	Stops a running container.
   -  **docker restart <container_id>** Restarts a container.
   -  **docker pause <container_id>**	Pauses a running container.
   -  **docker unpause <container_id>**	Unpauses a paused container.
   -  **docker rm <container_id>**	Removes a stopped container.
   -  **docker kill <container_id>**	Forcefully stops a container.
   -  **docker attach <container_id>**	Attaches to a running container.
   -  **docker wait <container_id>**	Blocks until a container stops and returns its exit code.
   -  **docker logs <container_id>**	Fetches logs of a container.
   -  **docker inspect <container_id>**	Displays detailed information about a container.
   -  **docker exec - it <container_id>** bash	Runs a command inside a running container (interactive shell).
   -  **docker export <container_id> - o <file.tar>**	Exports a container's filesystem as a tar archive.

#### Docker Compose
- tool used to define and run multi-container Docker applications. Instead of running containers one-by-one manually, Compose lets you define them all in a single YAML file (docker-compose.yml) and manage them as a group.
- Key Features:
     - Define multiple services (containers), networks, volumes in one file.
     - Spin up the entire environment with docker-compose up.
     - Supports environment variables, build instructions, health checks, and volumes.
     - Easy to manage service dependencies and startup order.
- Use case
     - Integration Testing - In CI/CD pipelines (e.g., GitHub Actions, Azure DevOps), we can use Docker Compose to spin up dependent services like databases before tests run.
- (for Multi  - Container Applications)
   -  **docker - compose up**	Starts containers defined in docker  - compose.yml.
   -  **docker - compose up   - d**	Starts containers in detached mode.
   -  **docker - compose down**	Stops and removes containers and networks.
   -  **docker - compose ps**	Lists containers managed by Compose.
   -  **docker - compose logs**	Shows logs for services in Compose.
   -  **docker - compose exec <service> <command>**	Runs a command inside a running service container.

#### Docker System Cleanup
   -  **docker system df**	Shows disk usage by Docker.
   -  **docker system prune**	Removes unused containers, networks, and images.
   -  **docker container prune**	Removes stopped containers.
   -  **docker image prune**	Removes unused images.
   -  **docker network prune**	Removes unused networks.
     
#### Docker Security
   -  **docker scan <image>**	Scans an image for vulnerabilities.
   -  **docker trust inspect <image>**	Shows trust data for an image.

#### Port Forwarding
Accessing a pod directly from your local machine without exposing a service externally. This is useful for debugging or testing applications running inside a pod.
   -  Using kubectl port-forward
   -  Forwarding a Deployment, Service, or StatefulSet
   -  sing a NodePort Service

####  Azure services majorly used

- **Compute & Container Services**
     - **Azure Virtual Machines (VMs)** - For self-managed Kubernetes clusters (kubeadm) or jump/bastion hosts, Host a self-managed Kubernetes control plane (if not using AKS)
     - **Azure Kubernetes Service (AKS)** - Fully managed Kubernetes with autoscaling, networking, and security integrations, Deploy microservices-based applications with Helm, Ingress, and Service Mesh
     - **Azure Container Registry (ACR)** - Private registry to store container images securely, push Docker images from CI/CD pipelines before deploying to AKS
- **DevOps & CI/CD**
     - **Azure DevOps (Pipelines, Repos, Artifacts)** - End-to-end CI/CD solution for AKS workloads, Build & deploy apps to AKS using YAML pipelines with Helm/Kustomize.
     - **GitHub Actions (with Azure integrations)** - GitHub-hosted runners and Azure service authentication, Build & deploy apps to AKS using YAML pipelines with Helm/Kustomize.
- **Security & Identity Management**
     - **Azure Active Directory (Azure AD)** - Secure authentication and role-based access
     - **Azure Key Vault** - Secure storage of secrets, certificates, and encryption keys
     - **Azure Policy & Defender for Cloud** - Enforce governance and security best practices
- **Networking & Traffic Management**
     - **Azure Application Gateway (AGIC)** - Layer 7 Ingress Controller with built-in Web Application Firewall (WAF) , Securely expose AKS workloads to the internet with SSL termination
     - **Azure Front Door** - Global traffic distribution, multi-region disaster recovery 
     - **Azure Private Link** - Secure connections to Azure services from AKS without public exposure
     - **Azure Load Balancer** - Distribute traffic to AKS nodes at Layer 4 (TCP/UDP). Load balance microservices exposed via NodePort.
- **Monitoring & Logging**
     - **Azure Monitor & Log Analytics** - Centralized logging, alerts, and dashboards for AKS , Monitor pod-level CPU/memory usage and trigger alerts if thresholds are breached
     - **Azure Application Insights** - Distributed tracing and application monitoring , Track request latency and failure rates for microservices in AKS
     - **Azure Sentinel (SIEM)** - Cloud-native security information & event management
- **Storage & Databases**
     - **Azure Files & Azure Blob Storage** - Persistent storage solutions, Store logs, backups, and application data
     - **Azure Managed Databases** (PostgreSQL, CosmosDB, SQL Server) - Fully managed, scalable database services

#### Azure Pipelines
Azure Pipelines is a cloud-based CI/CD service that automates the build, test, and deployment process. It supports multiple languages, frameworks, and deployment targets like Azure, Kubernetes, VMs, containers, and on-prem servers
- **key components**
     - Pipelines – The main CI/CD workflow.
     - Stages – Logical groupings of jobs (e.g., Build, Test, Deploy).
     - Jobs – A set of steps executed on an agent.
     - Steps – Individual tasks such as running a script or deploying an app.
     - Triggers – Define when a pipeline runs (e.g., push, PR, schedule).
     - Artifacts – Build outputs stored for deployments.
     - Agents & Pools – Machines running the pipeline tasks.
     - Environments – Used for approvals, gates, and tracking deployments.
- types of Azure Pipelines
     - Build Pipelines (CI) – Automates compiling code, running tests, and creating artifacts.
     - Release Pipelines (CD) – Automates deployments to different environments.
     - Multi-Stage Pipelines – Combines CI/CD into a single YAML pipeline.

#### Azure CI/CD Pipeline Flow
- work flow
     - Code Commit – Developer pushes code to Azure Repos/GitHub.
     - Trigger CI Pipeline – The pipeline starts on commit or pull request.
     - Build & Test – The code is compiled, unit tests are run, and an artifact is generated.
     - Publish Artifact – The built application is stored in Azure Artifacts or a container registry.
     - Deploy to Staging – The application is deployed to a test/staging environment.
     - Approval Process – Manual or automatic approvals before production deployment.
     - Deploy to Production – The final release is deployed to users.

#### Agent
- An agent is a machine that executes the pipeline tasks. There are two types:
     - Microsoft-hosted agents – Provided by Azure, includes Windows, Linux, macOS.
     - Self-hosted agents – Installed on-premises or in a cloud VM.

### Databases
- database management includes deployment, availability, performance optimization, backup, security, and observability of databases in production environments.
- Relational Databases (RDBMS)
     - MySQL, PostgreSQL, Microsoft SQL Server, Oracle DB, Amazon RDS, Azure SQL
- NoSQL Databases
     - MongoDB, Cassandra, DynamoDB, CosmosDB, Redis
- In-Memory Databases
     - Redis, Memcached
- Time-Series Databases
     - InfluxDB, TimescaleDB, Prometheus (for metrics)

|Database|Best For|
|-|-|
|MySQL|Web applications, e-commerce|
|PostgreSQL|Data analytics, financial transactions|
|SQL Server|Enterprise applications, Microsoft ecosystem|
|Oracle DB|Mission-critical enterprise apps|
|Amazon RDS|Managed SQL with cloud scalability|
|Azure SQL|AI-powered SQL with Azure integration|
|MongoDB|NoSQL document storage, flexible schemas|
|Cassandra|High availability, distributed data|
|DynamoDB|Serverless key-value storage|
|CosmosDB|Global-scale, multi-model NoSQL|
|Redis|In-memory caching|

#### SQL and NoSQL

|Feature|SQL (Relational) Databases|NoSQL (Non-Relational) Databases|
|-|-|-|
|Data Model|Structured (Tables, Rows, Columns)|Flexible (Key-Value, Document, Column-Family, Graph)|
|Schema|Predefined, Rigid Schema|Dynamic Schema (Schema-less)|
|Scalability|Vertical Scaling (Scale-Up)|Horizontal Scaling (Scale-Out)|
|Transactions|ACID (Atomicity, Consistency, Isolation, Durability) Compliant|BASE (Basically Available, Soft state, Eventually consistent)|
|Best Use Cases|Structured data, Financial transactions, Reporting|Big data, Real-time analytics, Distributed applications|
|Examples|MySQL, PostgreSQL, Microsoft SQL Server, Oracle, MariaDB|MongoDB, Cassandra, DynamoDB, Redis, CouchDB|

#### Jenkins Architecture
- Jenkins consists of:
     - **Master node (Controller )** – Manages the overall orchestration, job scheduling, and pipeline execution.
     - **Worker Nodes (Agents)** – Execute jobs in parallel based on workload distribution.
     - **Distributed Build System** – Supports multiple agents for efficient job execution.
     - **Plugins** – Extends Jenkins functionalities for integrations with tools like Docker, Kubernetes, Terraform, Ansible, and cloud providers.

**Main components:**
- **Jenkins Master (Controller)**
    - Manages the overall pipeline execution and Provides the web UI and handles user requests.
    - Schedules jobs and assigns them to agents and Stores configurations, plugins, and job details.
- **Jenkins Agent** (Worker Node/Slave)
- **Executes build tasks assigned by the master.**
    - Runs on different machines to distribute workload.
    - Can be static (always running) or dynamic (spun up on demand).
    - Supports various connection methods like SSH, JNLP, and Docker containers.
- **Workflow in Jenkins Architecture**
    - Developer commits code → Triggers Jenkins (via webhooks/polling).
    - Jenkins Master receives the trigger and schedules a job.
    - Jenkins Agent picks up the job, executes build/test steps.
    - Results (build success/failure, test reports) are sent back to the master.
    - Notifications & Deployment happen based on pipeline configuration.
- **Additional Components**
    - Job/Project: Defines what needs to be executed.
    - Build Executors: Agents where the build actually runs.
    - Plugins: Extend Jenkins' functionality (Git, Docker, Kubernetes, etc.).
    - Pipeline: Defines CI/CD workflows using scripted or declarative pipelines.
- **Scalability & High Availability**
    - Jenkins can scale horizontally by adding more agents.
    - Cloud-based agents (AWS, Kubernetes) provide dynamic scaling.
    - High Availability (HA) can be achieved using multiple masters with backup strategies.

#### CICDCD
- in-detail
    - **CI (Continuous Integration)** : Developers frequently merge code changes into a shared repository, and Jenkins automatically builds & tests the code to detect errors early.
    - **CD (Continuous Deployment/Delivery):** 
    - **Continuous Delivery** Code is automatically tested and prepared for release but requires manual approval for deployment.
    - **Continuous Deployment:** Every successful build is automatically deployed to production without manual intervention
- We have built and maintained pipelines using:
    - **Jenkins** – Developed shared libraries for standardized pipelines.
    - **GitHub Actions** – Implemented event-driven workflows for automated testing and deployments.
    - **Azure DevOps Pipelines** – Designed multi-stage YAML-based pipelines for cloud-native apps.
    - **ArgoCD & FluxCD** – Enabled GitOps for Kubernetes-based deployments.
    - **Terraform & Ansible** – Integrated infrastructure provisioning into pipelines.

#### Jenkins pipelie with Declarative & Scripted
**Declarative Pipeline**: A structured block-syntax approach to defining multiple stages in the pipeline makes it easier to write and maintain.
- pipeline  / - agent / - stage / - steps

**Scripted Pipeline**: An approach to define pipeline stages using Groovy-like syntax. It allows the application of complex and customized pipeline configurations that require a deeper understanding of Jenkins DSL.
- node / - stage (build) / - stage (test) / - stage (deploy)

#### Jenkinsfile 
- it tells Jenkins what steps to run, in what order, and under what conditions when building, testing, or deploying your application.
- its the blueprint for entire CI/CD pipeline — it brings structure, version control, automation to environment
- **Stages:**
     - **Checkout** → Clones the repository.
     - **Build** → compiling the application.
     - **Test** → running tests.
     - **Deploy** → deploying the application.

```pipeline {
    agent any  // Runs on any available agent
    stages {
        stage('Checkout') {
            steps {
                git 'https://github.com/example/repo.git'  // Clone the repository
            }
        }
        stage('Build') {
            steps {
                sh 'echo "Building the application..."'  // Simulated build step
            }
        }
        stage('Test') {
            steps {
                sh 'echo "Running tests..."'  // Simulated test step
            }
        }
        stage('Deploy') {
            steps {
                sh 'echo "Deploying the application..."'  // Simulated deployment step
            }
        }
    }
}
```
- **pipeline**		: Defines the entire Jenkins pipeline.
- **agent**		: Specifies where the pipeline or a stage will run (e.g., on a specific node, Docker container, etc.).
- **stages**		: Contains multiple stages that define different steps in the pipeline.
- **stage**		: Defines a specific phase or step in the pipeline (e.g., Build, Test, Deploy).
- **steps**		: Contains the steps to be executed within a stage.
- **script**		: Executes custom Groovy code or shell commands.
- **sh**		: Runs shell commands in a Unix/Linux environment.
- **bat**		: Runs batch commands in a Windows environment.
- **checkout**		: Checks out source code from a version control system (e.g., Git).
- **archiveArtifacts**	: Archives build artifacts (files or directories) to make them available later.
- **stash**		: Saves files or directories for later use in the pipeline.
- **unstash**		: Restores previously stashed files for use in a subsequent stage.
- **input**		: Pauses the pipeline to request user input.
- **timeout**		: Specifies a timeout for a stage or step.
- **environment**	: Sets environment variables for the pipeline or specific stages.
- **tools**		: Specifies tools like Java, Maven, etc., to be used in the pipeline.
- **when**		: Defines conditions for when a stage or step should run.
- **post**		: Specifies actions to be executed after the pipeline or stage (e.g., cleanup, notifications).
- **failure**		: A post-condition that runs if the pipeline or stage fails.
- **success**		: A post-condition that runs if the pipeline or stage succeeds.
- **always**		: A post-condition that runs regardless of the pipeline or stage result.
- **catchError**	: Catches errors in a stage and optionally marks the build as unstable.
- **retry**		: Retries a specific step a specified number of times if it fails

#### Jenkins pipeline fully step-by-step
- Stages in the Pipeline
     - Clone Code - Pull the latest code from GitHub/GitLab.
     - Infrastructure Setup - Use Terraform to provision AKS, storage, networking.
     - Build & Test - Compile code, run unit tests.
     - Security Scanning - Check vulnerabilities using Trivy, SonarQube, Snyk.
     - Docker Build & Push - Create a container and push it to Azure Container Registry (ACR).
     - Helm Deployment - using Helm charts to package, configure, and deploy applications to Kubernetes clusters in a repeatable and manageable way..
     - GitOps Deployment - declarative approach where infrastructure and application changes are triggered automatically from Git repositories using continuous synchronization., Trigger ArgoCD sync.
     - Post-Deployment Monitoring - Ensure app health using Prometheus & Grafana.
     - Cleanup & Notification - Send build status updates via Slack.

#### creating Jenkins pipeline
- fetching the code from the git/bitbucket
- writing a Jenkins file with stages
- creating a pipeline using declarative or scripted
- trigger a pipeline or when changes done to the repo it automatically triggers the pipeline

#### Sample Jenkinsfile Workflow
- it tells Jenkins what steps to run, in what order, under what conditions when building, testing, deploying your application.
- its the blueprint for entire CI/CD pipeline — it brings structure, version control, automation to environment
```
1️⃣ Checkout Code → Pull latest code from GitHub.
2️⃣ Build & Package → Compile the Java app into a JAR file.
3️⃣ Code Analysis → Run SonarQube for quality checks.
4️⃣ Security Scan → Scan dependencies for vulnerabilities using Snyk.
5️⃣ Containerization → Build a Docker image.
6️⃣ Push to Registry → Upload the image to Azure Container Registry (ACR).
7️⃣ Deploy to AKS → Deploy/update the app using Helm.
```
- Full pipeline
```
pipeline {
    agent any
    stages {
        stage('Checkout') {
            steps {
                git 'https://github.com/your-repo/fullstack-java-devops.git'
        stage('Build') {
            steps {
                sh 'mvn clean package'
        stage('Code Analysis') {
            steps {
                sh 'sonar-scanner -Dsonar.projectKey=fullstack-app'
        stage('Security Scan') {
            steps {
                sh 'snyk test'
        stage('Build Docker Image') {
            steps {
                sh 'docker build -t devopsacr.azurecr.io/fullstack-app:v1 .'
        stage('Push to ACR') {
            steps {
                sh 'docker push devopsacr.azurecr.io/fullstack-app:v1'
        stage('Deploy to AKS') {
            steps {
                sh 'helm upgrade --install fullstack-app helm-chart/'
}
```

Pipeline Breakdown
1. Checkout Source Code - **Pulls the latest source code from GitHub into the Jenkins workspace.**
```
stage('Checkout') {
    steps {
        git 'https://github.com/your-repo/fullstack-java-devops.git'
    }
}
```

2. Build Java Application 
     - Uses Maven to clean and build the application
     -  Generates a JAR file inside the target/ directory.
```
stage('Build') {
    steps {
        sh 'mvn clean package'
    }
}
```

3. Code Quality Analysis (SonarQube)
     - Runs SonarQube to analyze code quality and detect bugs, vulnerabilities, and code smells.
     - Requires SonarQube server & authentication token to be preconfigured.
```
stage('Code Analysis') {
    steps {
        sh 'sonar-scanner -Dsonar.projectKey=fullstack-app'
    }
}
```

4. Security Scan (Snyk)
     - Runs Snyk to check for security vulnerabilities in dependencies.
     - Requires Snyk API token configured in Jenkins.
```
stage('Security Scan') {
    steps {
        sh 'snyk test'
    }
}
```

5. Build Docker Image
     - Uses the Dockerfile to create a Docker image for the application.
     - Tags the image as devopsacr.azurecr.io/fullstack-app:v1.
```
stage('Build Docker Image') {
    steps {
        sh 'docker build -t devopsacr.azurecr.io/fullstack-app:v1 .'
    }
}
```

6. Push Docker Image to Azure Container Registry (ACR)
     - Authenticates with Azure Container Registry (ACR) and pushes the Docker image.
```
stage('Push to ACR') {
    steps {
        sh 'docker push devopsacr.azurecr.io/fullstack-app:v1'
    }
}
```

7. Deploy to Azure Kubernetes Service (AKS)
     - Uses Helm to deploy the application to AKS.
     - If the deployment already exists, it upgrades it.
```
stage('Deploy to AKS') {
    steps {
        sh 'helm upgrade --install fullstack-app helm-chart/'
    }
}
```

#### Azure Jenkinsfile Full CICD Pipeline
```
pipeline {
    agent any
    environment {
        AZURE_SUBSCRIPTION_ID = credentials('AZURE_SUBSCRIPTION_ID')  // Azure Subscription ID
        AZURE_CLIENT_ID = credentials('AZURE_CLIENT_ID')
        AZURE_CLIENT_SECRET = credentials('AZURE_CLIENT_SECRET')
        AZURE_TENANT_ID = credentials('AZURE_TENANT_ID')
        ACR_NAME = "myacr.azurecr.io"
        IMAGE_NAME = "myapp"
        IMAGE_TAG = "v1"
        KUBECONFIG = "$WORKSPACE/kubeconfig"
        SLACK_WEBHOOK = credentials('SLACK_WEBHOOK')  // Slack notifications
    }
    stages {

        stage('Clone Repository') {
            steps {
                git 'https://github.com/user/repo.git'
            }
        }
        stage('Terraform Init & Apply') {
            steps {
                dir('terraform') {
                    sh '''
                    terraform init
                    terraform apply -auto-approve
                    '''
                }
            }
        }
        stage('Build & Test') {
            steps {
                sh '''
                echo "Running Unit Tests..."
                npm install
                npm test
                '''
            }
        }
        stage('Security Scanning') {
            parallel {
                stage('Trivy Scan') {
                    steps {
                        sh 'trivy image ${ACR_NAME}/${IMAGE_NAME}:${IMAGE_TAG}'
                    }
                }
                stage('SonarQube Analysis') {
                    steps {
                        withSonarQubeEnv('SonarQube') {
                            sh 'mvn sonar:sonar'
                        }
                    }
                }
                stage('Snyk Scan') {
                    steps {
                        sh 'snyk test'
                    }
                }
            }
        }
        stage('Docker Build & Push') {
            steps {
                sh '''
                docker login ${ACR_NAME} -u $(az acr credential show --name myacr --query "username" -o tsv) -p $(az acr credential show --name myacr --query "passwords[0].value" -o tsv)
                docker build -t ${ACR_NAME}/${IMAGE_NAME}:${IMAGE_TAG} .
                docker push ${ACR_NAME}/${IMAGE_NAME}:${IMAGE_TAG}
                '''
            }
        }
        stage('Helm Deployment to AKS') {
            steps {
                sh '''
                az aks get-credentials --resource-group my-resource-group --name my-aks-cluster --file ${KUBECONFIG}
                export KUBECONFIG=${KUBECONFIG}
                helm upgrade --install myapp helm/ --namespace production
                '''
            }
        }
        stage('GitOps Sync with ArgoCD') {
            steps {
                sh '''
                argocd login my-argocd-server --username admin --password $(kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath="{.data.password}" | base64 -d)
                argocd app sync myapp
                '''
            }
        }
        stage('Post-Deployment Monitoring') {
            steps {
                sh '''
                kubectl get pods -n production
                kubectl get services -n production
                '''
            }
        }
    }
    post {
        success {
            sh '''
            curl -X POST --data-urlencode "payload={\"channel\": \"#devops-alerts\", \"text\": \"Deployment successful ✅\", \"username\": \"jenkins\", \"icon_emoji\": \":rocket:\"}" ${SLACK_WEBHOOK}
            '''
        }
        failure {
            sh '''
            curl -X POST --data-urlencode "payload={\"channel\": \"#devops-alerts\", \"text\": \"Deployment failed ❌\", \"username\": \"jenkins\", \"icon_emoji\": \":x:\"}" ${SLACK_WEBHOOK}
            '''
        }
    }
}
```

#### Rollback in Jenkins
- **Re-deploy Previous Build**	 – Jenkins keeps old builds; you can manually or automatically deploy the last successful build.
- **Use Versioned Artifacts**	 – Store artifacts in Artifactory/Nexus and deploy the last stable version if needed.
- **Git Revert** 			 – Rollback by reverting to a stable commit in Git and triggering a new build.
- **Blue-Green Deployment**		 – If a new version fails, switch traffic back to the stable environment.
- **Automated Rollback in Pipeline** – Add a rollback step that runs if deployment fails

#### Jenkins deploying an application
- we would setup an automated pipeline that builds, tests, and deploys the application. The process starts with configuring the Jenkins job, which can be a freestyle job or a pipeline, depending on the complexity of the deployment.
- First, we integrate the source code repository - Git repo, with Jenkins. We configure a webhook so that any new code commit triggers a build.
- Jenkins pulls the latest changes and initiates the build process, which may involve tools like Maven, Gradle, or a simple shell script, depending on the application stack.
- Once the build is successful, unit tests and other quality checks, like static code analysis using tools like SonarQube, are executed.
- If everything passes, Jenkins proceeds to package the application.
- Then it builds a Docker image using a Dockerfile and tagging it with a version number.
- The image is then pushed to a container registry like Docker Hub, AWS ECR, or Azure Container Registry.
- For deployment, Jenkins can use different strategies depending on the infrastructure. \
- If we're deploying to Kubernetes, we use tools like Helm or kubectl to apply the manifests.
- Jenkins executes these commands in a post-build step or within a dedicated deployment stage in a pipeline.
- if are doing it with VM-based deployment, we use Ansible, SSH scripts, or cloud-native services like AWS CodeDeploy.
- We can implement additional checks, such as verifying the deployment status using health checks, monitoring logs, and rolling back in case of failures.
- Blue-green or canary deployments are also common strategies that we integrate into Jenkins pipelines for minimal downtime.
- We can setup a notification on Jenkins job success, to Slack, email, or monitoring dashboards upon successful deployment.
- We also archive logs and artifacts for troubleshooting and audit purposes.
- few
- Helm chart are reusable . In a Jenkins pipeline, we can integrate Helm to package, deploy, and upgrade applications efficiently.
- In a declarative Jenkins pipeline, after the build and containerization stages, we can add Helm deployment stage

#### Helm  & Helm Charts
- Helm : pre-configured Kubernetes resources which helps manage Kubernetes applications
- **Components** :
     - Chart – A collection of YAML templates defining Kubernetes resources.
     - Release – A deployed instance of a Helm chart.
     - Repository – A collection of charts stored in a remote or local locatio
- **Helm chart** - its a package of Kubernetes YAML templates that define how an application is deployed
- **helm chart structure** 
- those are templated Kubernetes manifests packaged together for easy deployment, versioning, and management of applications in Kubernetes.
- They help automate complex deployments, making application deployment repeatable, scalable, and maintainable.
```helm-chart/
│── charts/                 # Dependency charts (if any)
│── templates/              # Kubernetes manifests with templating
│   ├── deployment.yaml     # Defines Deployment resource
│   ├── service.yaml        # Defines Service resource
│   ├── ingress.yaml        # Defines Ingress resource
│   ├── _helpers.tpl        # Contains reusable template functions
│   ├── hpa.yaml            # Defines Horizontal Pod Autoscaler (optional)
│   ├── configmap.yaml      # ConfigMap resource
│   ├── secret.yaml         # Secret resource
│── values.yaml             # Default configuration values
│── Chart.yaml              # Chart metadata (name, version, dependencies)
│── README.md               # Documentation for the chart
```
- Specific Experience Areas
1. Helm Chart Management
     - Developed and customized Helm charts for deploying microservices-based applications.
     - Used Chart.yaml and values.yaml to parameterize configurations.
     - Managed Helm dependencies using requirements.yaml or Chart.yaml.
2. Helm in CI/CD Pipelines
     - Integrated Helm with Jenkins, GitHub Actions, and ArgoCD to automate deployments.
     - Implemented GitOps workflows using Helm + ArgoCD for declarative application management.
     - Used Helmfile to manage multiple Helm charts across environments.
3. Helm Upgrades and Rollbacks
     - Used helm upgrade --install for seamless application upgrades.
     - Managed rollbacks using helm rollback in case of failed deployments.
     - Implemented Helm hooks for pre- and post-deployment tasks.
5. Helm and Kubernetes Security
     - Used Helm Secrets and sealed-secrets for managing sensitive configurations.
     - Applied RBAC policies to restrict Helm releases to specific namespaces.
     - Scanned Helm charts for security vulnerabilities using Trivy and Checkov.
6. Helm for Multi-Tenant Kubernetes Clusters
     - Managed multi-tenant Kubernetes environments with Helm namespaces and releases.
     - Implemented Helm templating for reusable and scalable Kubernetes configurations.

#### Helm Chart architecture
- A Helm chart is a package format that bundles Kubernetes resources to deploy applications in a repeatable manner.
![image](https://github.com/user-attachments/assets/c3bf750d-4a76-40f1-9071-049659e6977f)
- It provides all necessary configurations, dependencies, and templates for setting up applications in a Kubernetes cluster, acting as a blueprint for reusable, customizable deployments.
![image](https://github.com/user-attachments/assets/e044b9a4-ef59-42b2-9552-3a9a7fc82a7b)

### Blue-Green Deployment
- In Blue-Green deployment, we maintain two identical environments:
     - Blue (Current/Live Production Environment)
     - Green (New Version Deployment Environment)
- The new version (Green) is deployed alongside the existing version (Blue).
- Once validated, traffic is switched from Blue to Green instantly. If an issue occurs, we roll back by redirecting traffic back to Blue.
- How It Works:
     - Deploy the new version (Green) while users continue using the existing (Blue) environment.
     - Perform tests and validation in the Green environment.
     - Switch traffic to Green using a load balancer or DNS change.
     - If issues arise, revert traffic back to Blue.

#### Canary Deployment
- In a Canary Deployment, a small percentage of traffic is routed to the new version first.
- If the new version is stable, the percentage is gradually increased until 100% of users are on the new version.
- If issues arise, the deployment is stopped or rolled back.
- How It Works:
     - Deploy the new version (Canary) to only 10% of users while 90% still use the old version.
     - Monitor for errors, latency, CPU spikes, and user feedback.
     - Gradually increase the percentage (e.g., 25%, 50%, 75%) if stable.
     - If issues occur, route all traffic back to the stable version.

#### Rolling Deployment
- A Rolling Deployment gradually replaces old pods with new ones without downtime.
- Instead of deploying everything at once, it replaces instances one by one.
- Ensures that a portion of the application remains available during the rollout.
- How It Works:
     - A new version is deployed incrementally, replacing one pod at a time.
     - Kubernetes ensures the desired number of instances are always running.
     - If issues arise, Kubernetes automatically stops the rollout.

**Blue-Green, Canary, & Rolling Deployments**

|Feature	|Blue-Green Deployment	|Canary Deployment	|Rolling Deployment|
|-| -|-|-|
|Definition	|Two identical environments (Blue & Green); switch traffic between them.|	Gradually release the new version to a small subset of users before full rollout.	|Deploy new version in batches, replacing old instances gradually.|
|Traffic Switching	|Instant switch from Blue (old) to Green (new).	|Small percentage of traffic is routed to new version, then gradually increased.	|Instances are updated in controlled batches.|
|Risk Management	|Low risk (quick rollback).	|Medium risk (detect issues early with small user exposure).|	Medium risk (gradual transition).|
|Downtime|	Zero downtime.	|Minimal downtime.	|Minimal downtime.|
|Rollback Complexity|	Instant rollback (switch back to Blue).|	Easy rollback (redirect traffic back to old version).|	Rollback is slower (redeploy old version).|
|Infrastructure Cost|	High (needs two full environments).|	Medium (requires load balancing but not full duplication).	|Low (replaces old instances gradually).|
|Resource Utilization|	Higher (since both environments run in parallel).|	Efficient (only a fraction of traffic is routed to new version).|	Efficient (replaces instances one by one).|
|Best Use Cases	|Critical applications requiring zero downtime (banking, healthcare, high-availability systems).|	Apps where early testing on live traffic is beneficial (feature rollouts, ML models, A/B testing).|	Standard application updates where full duplication is unnecessary.|

### Observability - Monitoring
- it is to measure, analyze, troubleshoot a system’s performance by collecting and analyzing metrics, logs, and traces.
- we ensures system reliability, proactive monitoring, and rapid incident response.
- Three Pillars of Observability - **Metrics / Logs / Traces**

|Pillar|Definition|Examples|
|-|-|-|
|**Metrics**|performance data (CPU, memory, latency, error rates)|Prometheus, CloudWatch, Datadog|
|**Logs**|system activity (application logs, system logs)|ELK Stack, Splunk, Loki|
|**Traces**|End-to-end request tracking across microservices|Jaeger, AWS X-Ray, OpenTelemetry|
|**Visualization**| Raw metrics, logs, traces into dashboards |Grafana, Kibana|
|**Alerting**| we get notified about system |Alertmanager, PagerDuty, VictorOps|

- **Metrics to Monitor**
     - **Infrastructure Metrics** - CPU / Memory / Disk / Disk I/O (Read/Write Speed, Latency) / Network Bandwidth (Inbound/Outbound Traffic) / Network Packet Loss & ErrorsSystem Load Average
     - **Kubernetes Metrics** - Pod CPU & Memory Usage / Pod Restart Count / Container Uptime & Status / Node Resource Utilization / Kube API Server Latency & Request Rate / Kubernetes Scheduler Performance / Kubernetes etcd Health & Latency
     - **Application Performance** - HTTP Status Codes (2xx,4xx,5xxerrors)/ Request Latency / Request Throughput / Thread/Connection Pool Utilization / Cache Hit Ratio / Database Query Latency
     - **Database** - Database Disk Usage / Query Execution Time / Active Connections / Cache Miss Ratio / Replication Lag / Transaction Commit/Rollback Count
     - **Security** - SSL/TLS Certificate Expiry / Unauthorized Access Attempts / IAM Policy Changes / API Gateway Unauthorized Errors / Unusual Login Patterns
     - **Network & API** - API Request Success & Failure Rate / DNS Resolution Time / Load Balancer Connection Count / Rate of 4xx and 5xx Errors / TCP Retransmissions & Packet Drops
     - **Log-Based** - Error Rate (Number of ERROR Logs/Min) / Application Crash Frequency / Slow Query Logs Count / Critical System Events (Service Restart, Failures)
     - **Business & SLA** - Uptime (SLA Compliance %) / SLO (Service Level Objective) Violations / Customer Transactions Per Second / Revenue Impacting Errors

#### Datadog 
- its a full-stack observability platform that help to monitor, troubleshoot, optimize applications and infrastructure.
     - **Datadog Agent** : lightweight process which rung on hosts to collect metrics, logs, and traces and sends data to the Datadog backend for analysis
     - **Infrastructure Monitoring** : Provides real-time monitoring of cloud, on-prem
     - Application Performance Monitoring (APM & Tracing)
     - Log Management
     - Synthetic Monitoring
     - Security Monitoring (CSPM, SIEM)
     - Real User Monitoring (RUM)

- **Infrastructure Monitoring:**		# CPU, memory, disk, networking, cloud services.
- **Application Performance Monitoring (APM):**	# Tracing distributed requests, service dependencies.
- **Log Management:**				# Aggregating, analyzing, and searching logs.
- **Security Monitoring:**			# Threat detection, vulnerability scanning.
- **Synthetic & Real User Monitoring (RUM):**	# User experience tracking for web applications.
- Best For: Multi-cloud, AIOps, Advanced Tracing
- Type: SaaS-based Monitoring & APM
- Key Features
     - Auto-instrumentation for logs, metrics, traces (APM).
     - Real-time dashboards with AI-driven insights.
     - Log management with anomaly detection.
     - Integrations: AWS, Azure, Kubernetes, Docker, MySQL, Redis.
- Common Datadog Monitors
     - Infrastructure: Host metrics (CPU, Memory, Disk, Network).
     - APM (Application Performance Monitoring): Request latency, DB queries.
     - Security Monitoring: Unauthorized access attempts, log anomalies.

#### ELK (Elasticsearch, Logstash, Kibana)
- Best For: Log Aggregation, Real-time Log Analysis
- componentes & port
     - **Elasticsearch** : Stores and indexes log data for fast querying. - **9200** (REST API), **9300** (cluster comms) - **/etc/elasticsearch/elasticsearch.yml**
     - **Logstash** : Processes incoming logs and sends them to Elasticsearch - - **5044** (default Beats input) - **/etc/logstash/conf.d/logstash.conf**
     - **Kibana** : Provides a web interface to visualize log data and create dashboards. - **5601** - **/etc/kibana/kibana.yml**
     - **Filebeat** : Collects and forwards logs to Logstash or Elasticsearch. -  **filebeat.yml**
- Logs
     - **Application Logs:** Error logs, access logs, debug logs.
     - **System Logs:** Syslogs, authentication logs, security logs.
     - **Network Traffic Logs:** Firewall logs, DNS queries, packet analysis.
     - **Business Intelligence:** Customer behavior, website analytics.
     - **Anomaly Detection:** Identifying security threats or system failures.
- Common Log Queries in Kibana
     - Find All 500 Errors
     - Monitor Kubernetes Pod Failures
     - Detect Unauthorized Access
- Example Indices
     - **filebeat-***: Logs from Filebeat (e.g., web server logs, application logs).
     - **logstash-***: Logs ingested by Logstash.
     - **metricbeat-***: Metrics collected by Metricbeat (e.g., server performance metrics).
     - **auditbeat-***: Security-related logs captured by Auditbeat (e.g., user logins, file access).

#### Overview of ELK Stack:
1. Overview of ELK Stack
     - **Elasticsearch**: This is the core of the ELK stack, used for indexing and searching large volumes of log data. It's highly scalable and can handle huge datasets.
     - **Logstash**: A data processing pipeline that collects, parses, and forwards log data to Elasticsearch. It can process data from various sources, such as files, syslog, or Beats.
     - **Kibana**: Provides a web interface to visualize and explore the data stored in Elasticsearch. It’s typically used for creating dashboards and visualizations.
2. Architecture & Infrastructure Setup:
- In our setup, we configured the ELK stack with a distributed architecture for better scalability and fault tolerance.
     - **Elasticsearch Nodes**: We used multiple Elasticsearch data nodes to distribute the storage and compute load.
     - **Master Node(s)**: One dedicated master node was responsible for cluster management (e.g., maintaining metadata, managing node communication).
     - **Data Nodes**: We had 2-3 data nodes to handle large amounts of log data and index it into Elasticsearch.
     - **Ingestion Nodes**: Depending on the environment, we either used dedicated Logstash nodes or leveraged Beats (e.g., Filebeat) for direct log ingestion into Elasticsearch.
3. Infrastructure Details (Server Size & Configuration):
- The servers were provisioned based on the expected volume of logs, performance requirements, and redundancy needs. Here’s the breakdown of the infrastructure:
- Elasticsearch Nodes:
     - CPU: 4 to 8 vCPUs (dedicated to Elasticsearch)
     - RAM: 16 GB to 32 GB RAM per node (Elastic recommends allocating 50% of available memory to the JVM heap for Elasticsearch)
     - Disk: SSD disks with 500 GB to 2 TB (depending on data retention and expected growth)
     - Network: 1 Gbps or 10 Gbps (for internal communication between nodes)
- Logstash:
     - CPU: 4 vCPUs (since Logstash does heavy processing on the incoming data)
     - RAM: 8 to 16 GB RAM
     - Disk: Disk space was configured based on the log ingestion rate, typically 100 GB (SSD).
- Kibana:
     - CPU: 2 to 4 vCPUs (Kibana is lightweight compared to Elasticsearch)
     - RAM: 8 GB RAM (depends on the number of concurrent users and dashboards)
     - Disk: 50 GB SSD disk space for logs and visualizations
4. Does this Stack Run on Multiple Servers or a Single Server?
     - **Production Setup**: We used multiple servers/nodes for scalability and fault tolerance. The setup was distributed with Elasticsearch running on 3-5 nodes (including master, data, and coordination nodes).
     - **High Availability**: Elasticsearch had replicas of the data to ensure high availability. For instance, each index had 2 replicas to prevent data loss if a node went down.
     - **Logstash** was usually scaled horizontally depending on the log volume. We had 2-3 Logstash nodes in the pipeline.
     - **Development Setup**: For smaller environments (e.g., dev or test), we ran a single node that handled everything (Elasticsearch, Logstash, Kibana), but it was scaled later in production.
5. Configuration Steps I Followed:
- Elasticsearch Configuration:
     - Configured elasticsearch.yml for each node, ensuring that the master nodes were aware of the data nodes and vice versa.
     - Set up Zen Discovery for master node discovery and network.host for external access.
     - Tuned JVM heap settings to avoid memory bottlenecks: Xmx and Xms values were set based on RAM.
- Logstash Configuration:
     - Used logstash.conf for defining input (from files or Beats), filter (e.g., grok for parsing), and output (to Elasticsearch).
     - Configured pipeline workers to optimize performance depending on the volume of data.
- Kibana Configuration:
     - In the kibana.yml, configured the Elasticsearch host and set Kibana’s host and port.
     - Made sure that security was enabled when required, especially in production environments.
6. Monitoring & Scaling:
     - Used Elasticsearch APIs (_cat/health, _cat/nodes, etc.) to monitor the health and performance of the cluster.
     - Set up Sharding and Replication appropriately to ensure data was evenly distributed and redundant.
     - Used Elastic APM for performance monitoring and Filebeat to forward logs from the infrastructure to Logstash and Elasticsearch.
7. Scalability:
     - The stack was designed to scale horizontally. As data volume increased, we added more Elasticsearch data nodes and Logstash instances to keep up with the load.
     - We also configured auto-scaling groups (if running on cloud infrastructure like AWS/Azure) to automatically scale the resources based on usage.

#### Application Logs Not Appearing in ELK
- Issue: Developers report that application logs are not showing up in Kibana.
- **Investigation:**
     - Checked Logstash logs and found backpressure issues.
     - Used curl -XGET 'localhost:9200/_cluster/health?pretty' and saw Elasticsearch red status.
     - Found that log ingestion was overwhelming Elasticsearch disk space.
- **Resolution:**
     - Increased disk space and added more Elasticsearch data nodes.
     - Set up log retention policies to delete old logs.
     - Implemented log sampling to reduce excessive log volume

#### SPLUNK Architecture
- its a platform for searching, monitoring, and analyzing machine-generated big data through a web interface. It collects data from IT systems, logs, to gain insights into system performance, security, and operations.
- **Key Components:**
     - **Splunk Indexer:** Processes and stores data in an optimized format for fast searches.
     - **Splunk Forwarder:**
          - **Universal Forwarder (UF)**: Lightweight agent that sends data to the indexer.
          - **Heavy Forwarder (HF)**: Resource-intensive forwarder that parses, indexes, and forwards data.
     - **Splunk Search Head:** Interface for querying data, creating dashboards, and generating reports.
     - **Splunk Deployment Server:** Manages configurations for Splunk instances.
- **How Splunk Works:**
     - **Data Collection:** Collects logs, metrics, and machine data in real-time or from historical logs.
     - **Data Indexing**: Processes and indexes data into structured events for easy searching.
     - **Search & Analysis**: Users query indexed data with Splunk Processing Language (SPL).
     - **Visualization & Reporting**: Dashboards, charts, and alerts help interpret data for actionable insights.
     - **Alerts & Monitoring**: Configured to monitor thresholds and send real-time alerts for issue detection.
- **Use Cases:**
     - **Log Management:** Collect, analyze, and visualize logs from various systems.
     - **SIEM:** Detect and investigate security incidents.
     - **APM:** Analyze application performance and troubleshoot issues.
     - **IT Operations:** Monitor infrastructure and detect operational issues.
     - **Business Intelligence:** Gain insights into business metrics like customer activity.
- **Benefits:**
     - **Real-Time Monitoring:** Detect and alert on issues immediately.
     - **Scalability:** Splunk can scale horizontally to meet the needs of both small and large enterprises.
     - **Flexibility:** Supports various data sources and integrates with third-party tools.
     - **Search Power:** SPL enables powerful searches across large datasets.
- **Splunk in DevOps:**
     - Log and Metrics Monitoring: Ingests logs from multiple services to monitor system health.
     - **Incident Detection & Resolution:** Quickly identifies issues to reduce downtime.
     - **Automation:** Integrates with DevOps tools for automated workflows based on detected patterns (e.g., triggering alerts in CI/CD pipelines).

- log management and observability platform used for monitoring, analyzing, visualizing machine-generated data.
- It consists of multiple components that work together to collect, index, search, analyze logs from various sources
- **working process**
     - Forwarders (UF/HF) collect logs from applications, servers, Kubernetes clusters, and cloud platforms.
     - Indexers receive and store logs for efficient retrieval.
     - Search Heads allow SREs to analyze logs, debug issues, and create alerts.
     - Deployment Server ensures configurations are consistent across all Splunk agents.
     - Splunk Web/Dashboards provide real-time monitoring and insights

#### Prometheus & Grafana Architecture
- Prometheus
     -  its an open-source monitoring and alerting toolkit designed for reliability and scalability.
     -  It collects time-series data from configured targets (like applications or infrastructure), stores it locally, provides powerful query language (PromQL) for real-time analysis.
     -  It also supports alerting through tools like Alertmanager and integrates seamlessly with Grafana for visualization.

- Grafana 
     - its an open-source analytics and visualization platform that displays data from various sources like Prometheus, InfluxDB, Elasticsearch.
     - It creates interactive, customizable dashboards to monitor system metrics, application performance, logs in real-time.
     - It supports alerting, role-based access control, is widely used for observability in DevOps and SRE environments.
       
- Data from Prometheus, InfluxDB, Elasticsearch, etc.
     - **Time-series analytics:** Trends over time for system performance.
     - **Real-time dashboards:** Monitoring business KPIs, server health, and app performance.
     - **Alerting & Notifications:** Trigger alerts based on metric thresholds.
- Collects
     - **Infrastructure Metrics:** -  CPU, memory, disk usage, network latency, etc.
     - **Application Performance:** -  API response times, request rates, error rates.
     - **Kubernetes Monitoring:** - Pod status, resource utilization, node health.
     - **Database Performance:** - Query latency, connection pool usage.
     - **Custom Business Metrics:** - User activity, transactions per second, etc.

- Dashboards contents - Creating dashboard involves several core components:
     - **Panels**:		# Panels are the individual visualizations within a Grafana dashboard, like graphs, tables, and gauges.
     - **Data Sources**:	# Grafana supports numerous data sources; you need to configure these to pull in relevant data.
     - **Variables**:	        # Variables are dynamic filters that let you update data across the dashboard in real-time.
     - **Queries**:		# Each panel uses a query to retrieve data from the selected data source, enabling customization of metrics displayed.
     - **Alerts**:		# Grafana allows setting alerts that notify users when data crosses specific thresholds, helping teams stay proactive.

#### Setting up Prometheus & Grafana
1. **Setting Up Prometheus**
     - **prometheus.yml** (Prometheus Configuration)
     - Create a prometheus.yml file to define scrape jobs for monitoring Kubernetes nodes, services, and applications
2. **Setting Up Grafana**
     - Grafana Configuration File (grafana.ini)
     - Grafana requires a configuration file to define the database and security settings.
3. **Deploying on AWS/Azure**
     - Docker-Compose File
     - we can use docker-compose to deploy both Prometheus and Grafana.
4. **Deploying in Kubernetes (AWS EKS / Azure AKS)**
     - Prometheus Deployment (prometheus-deployment.yaml)
     - Prometheus ConfigMap (prometheus-configmap.yaml)
     - Grafana Deployment (grafana-deployment.yaml)
5. **Accessing Monitoring Dashboard**
     - Prometheus UI → **http://public-ip:9090**
     - Grafana UI → **http://public-ip:3000** ( login: admin/admin123 )
6. **Connecting Grafana to Prometheus**
     - Log in to Grafana (http://public-ip:3000).
     - Navigate to Configuration → Data Sources.
     - Add a new data source and select Prometheus.
     - Set http://prometheus:9090 as the data source URL.
     - Save & Test.
7. **Optional: Monitoring AWS/Azure Cloud Services**
     - Use AWS CloudWatch Exporter to monitor AWS services.
     - Use Azure Monitor Exporter for Azure services.
     - Set up Prometheus Alertmanager for alerts.

#### Grafana Dashboards creation
- creation
     - Open Grafana and navigate to Dashboards → Create → New Dashboard.
     - Click "Add a new panel" and select Prometheus (or other data source).
     - Choose Visualization Type (Graph, Gauge, Heatmap, etc.).
     - Click Save & Apply to add it to the dashboard.
  
1. **Install Grafana**
     - There are several methods to install Grafana:
     - Using Docker: docker run -d — name=grafana -p 3000:3000 grafana/grafana
     - Using a Package Manager: For instance, brew install grafana on macOS.
     - Manual Download: You can download and install Grafana from the official website.
     - Once installed, Grafana can be accessed at http://localhost:3000 (the default port) and logged into with default credentials (admin/admin).
2. **Add a Data Source**
     - In Grafana, go to Configuration > Data Sources.
     - Choose the data source you need, such as Prometheus, MySQL, or Elasticsearch.
     - Enter the required connection details, like the URL for Prometheus or credentials for MySQL.
     - Click Save & Test to confirm the connection.
3. **Create a New Dashboard**
     - Click the + icon on the left-hand menu and select Dashboard.
     - Choose Add new panel to start creating your first panel.
     - Select a visualization type (such as graph, gauge, pie chart) based on your data requirements.
4. **Configure the Panel**
     - In the panel, select the data source.
     - Write a query to retrieve the desired data. For example, in Prometheus, a query might be rate(http_requests_total[5m]).
     - Customize the panel options to suit your needs:
          - Set the title, description, and display options.
          - Adjust the visualization style, including axes, colors, and legends.
5. **Set Variables (Optional)**
     - Variables allow you to create dynamic dashboards:
     - Go to Dashboard Settings > Variables > New.
     - Define the variable name and select the type (e.g., Query).
     - Create a query based on the data source. For instance, with Prometheus, a variable query for different instance values could include server IPs or application labels.
     - Save the variable. The dashboard will now have a dropdown to filter data based on this variable.
6. **Adding Alerts (Optional)**
     - Grafana allows users to set up alerts to proactively monitor their data:
     - Go to the Alert tab within the panel settings.
     - Define an alert condition (e.g., “CPU load average exceeds 80%”).
     - Set a time range and frequency for checking the alert.
     - Specify a Notification Channel (such as email, Slack, or PagerDuty).
7. **Save the Dashboard**
     - Click Save Dashboard in the top-right corner.
     - Name and save your dashboard, which can be shared with team members as needed.

#### Dashboard in Amazon CloudWatch
     - Open CloudWatch Console → Go to Dashboards.
     - we have to Create or Select existing dashboard.
     - we have to Add a Widget and select Graph, Number, or Text.
     - we have to Choose Metrics and select a service (e.g., EC2, Lambda, EKS).
     - we have to search filters to find the required metric.
     - we can Customize Widget like Setting time range, statistic type (Average, Sum ).
     - we need to Create Widget and then Save Dashboard.
       
#### Dasshboard Kibana
     - Open Kibana and go to Dashboard → Create New Dashboard.
     - Click "Add a visualization" and choose a chart type (e.g., Line, Bar, Pie).
     - Select Elasticsearch Index and configure filters (e.g., status:500).
     - Use KQL query for custom metrics, ( KQL Kibana Query Language)
     - Save and add it to the dashboard.

#### Dynatrace 
- it is an full-stack observability platform that provides application performance monitoring (APM), infrastructure monitoring, digital experience management (DEM). It enables DevOps, SREs, and IT teams to detect, analyze, and optimize application performance in real time.

- Dynatrace uses a single-agent architecture with OneAgent, which collects telemetry data from applications, services, and infrastructure. This data is processed in the Dynatrace Smartscape to visualize dependencies and relationships. Davis AI automatically analyzes performance metrics, detects anomalies, and provides actionable insights.

- Key Components:
     - **Dynatrace OneAgent** – Installed on hosts to collect monitoring data automatically.
     - **Dynatrace Smartscape** – Provides real-time visualization of dependencies across applications and infrastructure.
     - **Davis AI Engine** – Detects problems, performs anomaly detection, and conducts automatic root cause analysis.
     - **Dynatrace Managed (On-Prem) or SaaS** – Deployment options for cloud and on-prem environments.

- Use Cases

|Use Case|Description|
|-|-|
|Automated Incident Detection|Dynatrace automatically detects performance bottlenecks, anomalies, and failures, reducing MTTR (Mean Time to Repair).|
|Continuous Monitoring in CI/CD|Integrates with Jenkins, Azure DevOps, GitLab, and other CI/CD tools to detect issues before production.|
|Kubernetes & Container Monitoring|Provides real-time insights into Kubernetes clusters, pods, and microservices running in AWS EKS, Azure AKS, and GCP GKE.|
|SLOs and SLIs Tracking|Helps define and monitor Service Level Objectives (SLOs) and Service Level Indicators (SLIs) for reliability.|
|Cloud Cost Optimization|Analyzes cloud resource utilization and recommends cost-saving optimizations.|
|Security Observability|Detects security vulnerabilities in runtime applications and ensures compliance with security standards.|
|Log Monitoring & Analytics|Collects and analyzes logs from applications and infrastructure for troubleshooting.|

- Key Features of Dynatrace

|Feature	|Description|
|-|-|
|Full-Stack Monitoring|Monitors applications, infrastructure, networks, logs, and real-user interactions in a single platform.|
|AI-Powered Root Cause Analysis|Uses Davis AI to automatically detect anomalies and determine the root cause of issues.|
|Distributed Tracing|Tracks transactions across microservices, containers, and cloud environments.|
|Kubernetes & Cloud Monitoring|Provides deep observability into AWS, Azure, Google Cloud, Kubernetes, and hybrid environments.|
|Synthetic Monitoring|Simulates user interactions to test application availability and performance proactively.|
|Real User Monitoring (RUM)|Tracks user experience, analyzing performance across browsers, mobile apps, and IoT devices.|
|Log Monitoring & Analysis|Collects and analyzes logs for troubleshooting and performance optimization.|
|Security & Vulnerability Management|Detects runtime application vulnerabilities and helps enforce compliance standards.|
|Infrastructure Monitoring|Monitors hosts, virtual machines, containers, databases, and networks.|

#### New Relic 
- it is a cloud-based observability and application performance monitoring (APM) platform used by DevOps and SRE teams to monitor, troubleshoot, and optimize application and infrastructure performance in real-time.
- It provides end-to-end visibility across applications, infrastructure, logs, metrics, and traces—helping teams reduce downtime, improve reliability, and optimize system performance.
- New Relic is crucial in DevOps and Site Reliability Engineering (SRE) for:
     - **Proactive Incident Detection** – Detects anomalies and performance issues before impacting users.
     - **Faster Troubleshooting** – Helps identify root causes of latency, errors, or downtime across applications and infrastructure.
     - **End-to-End Observability** – Monitors full-stack apps, microservices, Kubernetes clusters, and cloud resources.
     - **SLO/SLA Compliance** – Helps track SLIs (Service Level Indicators) and ensure compliance with SLOs and SLAs.
     - **Infrastructure Optimization** – Monitors CPU, memory, disk, and network usage to optimize cloud resource costs.
     - **CI/CD Pipeline Monitoring** – Ensures deployments do not introduce performance degradations by integrating with Jenkins, Azure DevOps, GitHub Actions, etc.
     - **Incident Response & Automation** – Triggers alerts, auto-remediation scripts, and integrates with PagerDuty or Opsgenie for automated response.

#### Load Balancer
- Load balancing ensuring high availability, fault tolerance, and optimal performance for applications.
- It distributes incoming network traffic across multiple backend servers, preventing any single server from being overwhelmed and ensuring application reliability.
     - **Application Load Balancer (ALB):** For HTTP/HTTPS traffic and content-based routing.
     - **Network Load Balancer (NLB):** For TCP or UDP traffic with low latency and high throughput.
     - **Classic Load Balancer:** For older applications but typically replaced by ALB or NLB.
- STEPS
     - **Configure Security Settings:** SSL certificate , security group
     - **Define Target Groups:**	ec2 , container IP

#### Load Balancer for a web application on AWS
- info
     - I configure an Application Load Balancer (ALB) to distribute traffic across backend instances running in an Auto Scaling Group (ASG) or Kubernetes (EKS).
     - created a Target Group with health checks to ensure requests go to healthy instances. I use path-based routing (e.g., /api → API, / → UI) for microservices-based applications. Security is enforced using AWS WAF and SSL termination with ACM certificates.
     - For scalability, the ALB integrates with Auto Scaling to handle dynamic traffic loads, ensuring high availability and 99.99% uptime. 

#### Load Balancer for a web application on Azure
- info
     - I configure an Application Gateway (AGW) to distribute traffic across backend instances running in VM Scale Sets (VMSS) or Azure Kubernetes Service (AKS).
     - I create a Backend Pool with my instances, configure path-based routing (e.g., /api → API, /web → UI), and enable SSL termination using Azure Key Vault certificates. Security is enhanced with Azure WAF to block malicious traffic.
     - For scalability, the AGW integrates with VMSS and AKS Autoscaler, ensuring high availability and 99.99% uptime.

#### k8s Kubernetes Architecture
- its a **master nodes and worker nodes** concept
- Kubernetes is a cluster system, it is  used to automate the deployment, scaling, and management of containerized applications. 
- **Master Node (Control Plane)**: The master node is like the brain of the cluster. It controls and manages the entire system.
     - **API Server**		: The communication hub that exposes the Kubernetes API for all interactions.
     - **Scheduler**		: Decides where to deploy your application’s containers based on available resources.
     - **Controller Manager**	: Ensures the desired state of the system is maintained, such as managing scaling and node health.
     - **Etcd**			: A distributed database that stores all configuration and state data of the cluster.
- **Worker Nodes (Data Plane)**: They are the machines where your actual applications run. Each node is responsible for running containers.
     - **Kubelet**		: Ensures that containers are running as expected. manages containers on a node and communicates with the API server.
     - **Kube-proxy**		: its a service proxy that load-balances traffic and manages routing rules to forward requests to backend pods..
     - **Container Runtime**	:  it pulls images and runs containers based on specifications.
- The master node manages and monitors everything, while the worker nodes actually run the workloads.
- This architecture allows Kubernetes to provide automated scaling, self-healing, and high availability for your applications.
     - **Pods** are the smallest deployable units in Kubernetes, similar to lightweight VMs. They can contain one or more containers & are ephemeral, receiving new IPs each time they restart.
     - **kubectl** is a command-line tool for interacting with the Kubernetes cluster, supporting both imperative and declarative management.

1. Workload Objects (Pod & Controllers)
     - **Pod** – The smallest deployable unit, consisting of one or more containers.
     - **ReplicaSet** – Ensures a specified number of pod replicas are running.
     - **Deployment** – it manages ReplicaSets for rolling updates and rollbacks.
     - **StatefulSet** – it manages stateful applications, ensuring stable network identities and persistent storage.
     - **DaemonSet** – Ensures a pod runs on every node (e.g., logging and monitoring agents).
     - **Job** – Runs a batch job and ensures it completes successfully.
     - **CronJob** – Schedules jobs to run at specific times, similar to cron jobs in Linux.
2. Service & Networking Objects
     - **Service** – Exposes a set of pods as a network service (ClusterIP, NodePort, LoadBalancer, ExternalName).
     - **Ingress** – it mean traffic entering the Kubernetes cluster from an external source.
     - **Egress** - Egress is traffic leaving the Kubernetes cluster (e.g., accessing external services like APIs, databases, or internet ).
     - **Endpoint** – Represents network endpoints for a service.
     - **NetworkPolicy** – Controls traffic flow between pods.
3. Configuration & Storage Objects
     - **ConfigMap** – Stores configuration data as key-value pairs.
     - **Secret** – Stores sensitive information like passwords and API keys securely.
     - **PersistentVolume (PV)** – Represents storage in the cluster.
     - **PersistentVolumeClaim (PVC)** – Requests specific storage from a PV.
     - **StorageClass** – Defines dynamic storage provisioning.
4. Cluster Management Objects
     - **Namespace** – Provides isolation between groups of resources in a cluster.
     - **Node** – Represents a worker machine in the cluster.
     - **Role & RoleBinding** – Defines and applies permissions within a namespace.
     - **ClusterRole & ClusterRoleBinding** – Assigns permissions across the cluster.
     - **ServiceAccount** – Provides identity for processes running in a pod.

#### Scheduler 
- Scheduler assigns new pods to appropriate worker nodes based on available resources, affinity rules, and constraints.
     - It considers CPU, memory, affinity, taints, and tolerations when selecting a node.
- Use Case:
     - When a new deployment happens, the scheduler automatically places pods on the best-fit node.
     - In a multi-node cluster, it ensures efficient utilization of resources.

#### Controller Manager
- its a component that runs controller loops to maintain the desired state of the cluster.
     - It includes controllers like Node, Replication, ServiceAccount, and Endpoints Controllers.
- Use Case:
     - Ensures high availability by replacing failed pods.
     - Maintains desired replica count for Deployments, ReplicaSets, etc.

#### Etcd
- its a key-value store that maintains the entire cluster state.
     - Used for storing pod, service, and node configurations.
- Use Case:
     - Ensures cluster consistency and state recovery in case of node failures.
     - Used in multi-master setups for high availability.

#### Kube-proxy
- its a networking component that handles request forwarding and maintains network rules.
     - Uses iptables, IPVS, or eBPF for routing traffic.
- Use Case:
     - Ensures traffic from a Service reaches the correct Pod.
     - Implements load balancing for Services inside the cluster.

#### Container Runtime
- it is responsible for running containers.
     - Supported runtimes include Docker, containerd, and CRI-O.
- Use Case:
     - Runs application workloads inside Kubernetes.
     - Supports container networking and storage.

#### ReplicaSet
- its a specified number of identical pods are running.
     - Automatically scales up/down based on desired replica count.
- Use Case:
     - Ensures fault tolerance by replacing failed pods.
     - Used in Deployments to achieve rolling update

#### DaemonSet
- it see's pod runs on every node in the cluster.
     - Used for running monitoring and logging agents.
- Use Case:
     - Deploying Prometheus node exporter on every node.
     - Running Fluentd or Logstash to collect logs from all nodes.

#### Job in kubernetes
- it Runs a one-time task and see's it completes successfully.
- Use Case:
     - Data migration before deploying a new application version.
     - Database backup jobs that should run only once.

#### CronJob
- it is a  scheduled Job that runs at a specific time, similar to a Linux cron job.
- Use Case:
     - Running a daily database backup at midnight.
     - Sending automated reports every week.

#### Service
- it Exposes a group of pods to the network.
     - Types: ClusterIP, NodePort, LoadBalancer, ExternalName.
- Use Case:
     - ClusterIP: Internal communication between services.
     - NodePort: Exposing services to the external network.
     - LoadBalancer: Integrating with cloud-based load balancers.

#### Egress
- Controls outbound network traffic from the cluster.
     - Ensures security policies for external communication.
- Use Case:
     - Restricting external API calls from certain namespaces.
     - Allowing access only to trusted external services.

#### Endpoint
- Represents the IP addresses of pods associated with a Service.
- Use Case:
     - When a pod is added or removed, the endpoint list is updated automatically.

#### NetworkPolicy
- Defines rules to control traffic between pods.
- Use Case:
     - Allow only frontend pods to communicate with backend pods.
     - Block traffic from unauthorized namespaces.

#### ConfigMap
- Stores configuration data in key-value format.
- Use Case:
     - Store database connection strings or environment variables.
     - Used for externalizing configurations instead of hardcoding values.

#### Kubernetes Secret
- Stores sensitive data (passwords, API keys) securely.
- Use Case:
     - Storing Docker registry credentials for private images.
     - Managing TLS certificates for secure communication.

#### StorageClass
- Defines dynamic storage provisioning in Kubernetes.
- Use Case:
     - Provisioning persistent volumes automatically.
     - Using different storage backends like AWS EBS, Azure Disk, NFS.

#### Kubernetes Namespace
- Provides logical isolation for different teams or environments.
- Use Case:
     - Separate namespaces for dev, staging, and production.
     - Multi-tenant clusters where different teams use different namespaces.

#### Role & RoleBinding
- Role: Defines permissions within a namespace.
- RoleBinding: Grants access to a specific user or service account.
- Use Case:
     - Giving read-only access to developers for a namespace.
     - Allowing CI/CD pipeline to modify only specific resources.

#### ClusterRole & ClusterRoleBinding
- ClusterRole: Provides permissions across the entire cluster.
- ClusterRoleBinding: Assigns a ClusterRole to a user or service.
- Use Case:
    - Admin users need cluster-wide access to all namespaces.
    - Granting monitoring tools (like Prometheus) access to all namespaces.

#### ServiceAccount
- Provides identity to pods for interacting with Kubernetes API.
- Use Case:
     - Granting permissions to a CI/CD pipeline running inside Kubernetes.
     - Authenticating pods without using user credentials.

#### Node Pool
- A group of worker nodes in Kubernetes with the same configuration. Used for workload separation (e.g., general workloads vs GPU workloads).
- Use Case: Running GPU workloads separately from general workloads.
     - Example : az aks nodepool add

#### Resources in k8s
- Defines CPU & memory limits for containers. Helps in optimizing performance & cost.
- Use Case: Prevents over-utilization of cluster resources.
     - Example : requests, limits

#### Requests
- Minimum resources required for a container. Helps scheduler in placing pods on suitable nodes.
- Use Case: Ensures pods run without resource starvation.
     - Example : requests.cpu, requests.memory 

#### MatchLabels & Selector
- Used for selecting pods based on labels. Used in Services, Deployments, ReplicaSets, etc.
- Use Case: Selects only pods labeled with app=nginx for deployment.
     - Example : matchLabels: app: nginx

#### Metadata
- Stores name, labels, and annotations of Kubernetes objects.
- Use Case: Helps categorizing and filtering resources efficiently.
     - Example : labels, annotations
       
#### Spec (Specification)
- Defines configuration details for objects like Pods, Deployments, Services, etc.
- Use Case: Defines how Kubernetes objects behave.
     - Example : spec: replicas: 3

#### Sidecar Container & Init Container
- Sidecar Containers: Extend the functionality of the main container.
     - Logs application traffic.
     - Example : fluentd logging sidecar
- Init Containers: Run before the main container starts.
     - Ensures database migration is completed before app starts.
     - Example : db-migration init container

#### Imperative vs Declarative
- Direct commands to create/update resources
     - kubectl create deployment nginx --image=nginx
     - Example : kubectl create deployment
- Uses YAML files to define desired state
     - kubectl apply -f deployment.yaml
     - Example : kubectl apply -f

#### Node Selector
- Schedules pods on specific nodes.
     - Example : nodeSelector: disktype: ssd
- Use Case: Ensures high-performance workloads run on SSD-backed nodes.

#### Container Network Interface (CNI) Plugin
- Manages network connectivity between pods in Kubernetes. Popular CNIs include Calico, Flannel, Cilium.
     - Example : Calico, Flannel, Cilium
- Use Case: Enables pod-to-pod networking and network policies.

#### Cloud-Lint
- A cloud configuration scanning tool that detects misconfigurations in Terraform, AWS, Azure, GCP.
     - Example : tflint --init
- Use Case: Prevents security misconfigurations in IaC deployments.

#### Elastic HA (High Availability)
- Ensures redundancy and fault tolerance in cloud environments.
     - Example : RollingUpdate strategy 
- Use Case: Ensures zero downtime during deployments.

#### Content Delivery Network (CDN)
- Distributes cached static content closer to users for faster loading speeds.
     - Example : AWS CloudFront
- Use Case: Speeds up content delivery for globally distributed applications

#### Front-end Pod
- A front-end pod hosts UI-related services like web applications. Typically exposed via a Service or Ingress Controller.
- Use Case:
     - Hosting React/Angular-based web applications.
     - Exposing UI via LoadBalancer or Ingress.
- Example : An E-commerce website where the frontend pod serves static HTML/JS files while the backend pod handles business logic.

#### Back-end Pod
- A back-end pod handles business logic, APIs, and database interactions.
- Use Case:
     - Running APIs (REST, GraphQL, gRPC).
     - Processing user requests before sending data to frontend.
- Example : A Node.js backend API that interacts with MongoDB or PostgreSQL.

#### ETL Pipeline
- ETL - (Extract, Transform, Load) - An ETL pipeline extracts data from a source, processes it, and loads it into a data warehouse.
- Use Case:
     - Processing large amounts of data for analytics.
     - Automating data transformation workflows.
- Example : Data pipelines for ML models, where raw data is processed, transformed, and stored in a data warehouse.
     - Apache Airflow CronJob

#### Labels
- Labels are key-value pairs used to identify and group Kubernetes objects.
- Use Case:
     - Grouping pods for deployments and services.
     - Filtering logs in monitoring tools (Prometheus, Grafana).
- Example : CI/CD pipelines use labels to deploy only specific environments.
     - labels: env: production

#### Key-Value Pairs
- Used in ConfigMaps, Secrets, and Labels.
- Use Case: Storing environment variables for applications.
- Example : Using ConfigMaps to manage database connection strings.
     - ConfigMaps and Secrets

#### Kind
- The kind field in Kubernetes YAML defines the type of resource being created.
- Use Case: Distinguishing different Kubernetes resources (e.g., Pod, Service, Deployment).
- Example : Managing multiple Kubernetes resources efficiently in YAML files.
     - kind: Deployment, kind: Service

#### Socket Layer
- The socket layer is a critical component in networking that provides an interface for communication between applications over a network. In Kubernetes, sockets enable inter-pod and external communication.
- How It Works in Kubernetes
     - Used for service-to-service communication inside a Kubernetes cluster.
     - Relies on container networking (CNI plugins) to establish connections.
     - Used in Ingress, LoadBalancers, and Services for external communication.
- Use Case:
     - Microservices architecture where one pod communicates with another over a socket connection.
     - Database connections between an application pod and a database service.
- Example : A Node.js API server communicating with a PostgreSQL database using a socket connection.
     - Service with ClusterIP or LoadBalancer

#### ReplicationController
- The ReplicationController (RC) ensures that a specified number of pod replicas are running at all times. However, it's deprecated in favor of Deployments and ReplicaSets.
- Key Features
     - Ensures desired state of pod replicas.
     - Recreates failed pods automatically.
     - Scales applications horizontally.
- Use Case:
     - Running multiple instances of a stateless application to improve reliability and availability.
- Example : A frontend service with 3 replicas to handle traffic.
     - kind: ReplicationController (Deprecated)

#### APIVersion
- The apiVersion field in Kubernetes specifies the Kubernetes API group and version that a resource belongs to
- Use Case:
     - Ensuring backward compatibility when upgrading Kubernetes clusters.
     - Using the correct API group for managing Kubernetes objects.
- Example : Migrating from extensions /v1 beta1 to apps/v1 for Deployments.
     - apiVersion: apps/v1

#### Pod.yaml
- A Pod YAML file defines the configuration of a pod, including its containers, resources, and networking details.
- Configuration Files:
     - pod.yaml (Defines a pod)
     - service.yaml (Exposes the pod)
     - deployment.yaml (Manages pod scaling)
- Use Case: Deploying applications inside Kubernetes.
- Example : Running a Python Flask application inside a pod.
     - apiVersion: v1, kind: Pod

#### Kubectl commands
1. Basic Commands
     - kubectl version  ( Show kubectl and API server versions )
     - kubectl cluster-info  	(Show cluster information )
     - kubectl config view  	(View kubeconfig settings)
     - kubectl get nodes	List all nodes in the cluster
     - kubectl describe node <node-name>	Show detailed info about a node
2. Context and Configuration
     - kubectl config get-contexts	List available contexts
     - kubectl config current-context	Show the current context
     - kubectl config use-context <context>	Switch to a different context
     - kubectl config set-context <context>	Set a new context
3. Workloads: Pods, Deployments, and ReplicaSets
     - kubectl get pods	List all pods
     - kubectl get pods -o wide	List pods with extra details
     - kubectl describe pod <pod-name>	Show detailed pod information
     - kubectl logs <pod-name>	View logs of a pod
     - kubectl logs -f <pod-name>	Follow live logs
     - kubectl exec -it <pod-name> -- /bin/sh	Open a shell inside a pod
     - kubectl delete pod <pod-name>	Delete a specific pod
     - kubectl delete pods --all	Delete all pods
     - kubectl rollout status deployment <deployment-name>	Check deployment rollout status
     - kubectl rollout restart deployment <deployment-name>	Restart a deployment
4. Creating and Managing Resources
     - kubectl apply -f <file>.yaml	Apply configuration from a file
     - kubectl create deployment <name> --image=<image>	Create a deployment
     - kubectl delete -f <file>.yaml	Delete resources from a file
     - kubectl scale deployment <deployment-name> --replicas=<number>	Scale a deployment
     - kubectl edit deployment <deployment-name>	Edit a deployment resource
5. Services and Networking
     - kubectl get svc	List all services
     - kubectl describe svc <service-name>	Show detailed service information
     - kubectl expose deployment <deployment-name> --type=NodePort --port=<port>	Expose a deployment as a service
     - kubectl port-forward <pod-name> <local-port>:<pod-port>	Forward a local port to a pod
6. ConfigMaps and Secrets
     - kubectl get configmaps	List all ConfigMaps
     - kubectl describe configmap <configmap-name>	Show ConfigMap details
     - kubectl create configmap <name> --from-literal=key=value	Create a ConfigMap
     - kubectl get secrets	List all Secrets
     - kubectl describe secret <secret-name>	Show secret details
7. Namespaces
     - kubectl get namespaces	List all namespaces
     - kubectl create namespace <namespace>	Create a new namespace
     - kubectl delete namespace <namespace>	Delete a namespace
8. Persistent Volumes and Storage
     - kubectl get pv	List Persistent Volumes (PV)
     - kubectl get pvc	List Persistent Volume Claims (PVC)
9. Debugging and Troubleshooting
     - kubectl describe <resource> <name>	Detailed resource information
     - kubectl logs <pod-name>	View logs
     - kubectl exec -it <pod-name> -- /bin/sh	Open a shell inside a pod
     - kubectl get events	Show cluster events
     - kubectl top pod	Show CPU and memory usage for pods
     - kubectl top node	Show CPU and memory usage for nodes
10. Advanced Commands
     - kubectl apply -k <directory>	Apply a Kustomization
     - kubectl diff -f <file>.yaml	Show differences before applying changes
     - kubectl explain <resource>	Show resource documentation
     - kubectl label pod <pod-name> key=value	Add a label to a pod
     - kubectl annotate pod <pod-name> key=value	Add an annotation to a pod

#### Kubelet
- Kubelet is a critical node-level agent in Kubernetes that ensures containers are running in a Pod. It communicates with the Kubernetes API server, manages container lifecycles, and monitors health.
     - Registers the node with the Kubernetes API server.
     - Starts and stops containers inside Pods.
     - Manages Pod lifecycle and ensures containers run as expected.
     - Monitors Pod health using liveness & readiness probes.
     - Communicates with container runtime (Docker, containerd, CRI-O) to manage containers.

- **Role of Kubelet in Kubernetes**
     - The kubelet is a node agent that manages pod lifecycle, health checks, and container runtime interactions.
     - It ensures that the desired state of Pods matches the actual state

- How does Kubelet interact with the container runtime
     - Kubelet uses the Container Runtime Interface (CRI) to interact with container runtimes like Docker, containerd, CRI-O.
     - It sends commands to start, stop, and monitor containers.

- Kubelet process stops
     - The node will stop reporting to the Kubernetes API server.
     - Pods running on that node may become unreachable.
     - controller manager will schedule new pods on available nodes if configured properly.

#### Pods
- A Pod is the smallest deployable unit in Kubernetes that runs one or more containers with shared networking and storage.
- **Pod Architecture & Components**
     - Containers: Main application + sidecar containers.
     - Network: Every Pod gets a unique IP and can communicate with other Pods.
     - Storage Volumes: Shared storage between containers in a Pod.
- What happens when a Pod fails
     - If managed by a Deployment/ReplicaSet, Kubernetes will reschedule it.
     - If a standalone Pod, it must be recreated manually.
- Pod have multiple containers
     - multiple containers in a Pod can share resources and work together (e.g., an app container + a logging sidecar).
- How do Pods communicate with each other
     - Each Pod gets a unique IP address.
     - Pods communicate within the cluster via Kubernetes networking (CNI plugins like Calico, Flannel, Cilium).
- info
     - Simplifi ed application management
     - Improved scaling and availability
     - Easy deployment and rollback
     - Improved resource utilizatio
     - Increased portability and fl exibility
     - A pod is the smallest deployable unit in Kubernetes that represents a single instance of a running process in a container.

- Conclusion
     - Pods are the smallest deployable unit in Kubernetes.
     - Single-container Pods run simple applications, while multi-container Pods use Sidecar patterns.
     - Pods are managed by controllers like Deployments, StatefulSets, and DaemonSets.
     - Networking, storage, and scheduling are important considerations when deploying Pods.

#### Pod Lifecycle
- Pods go through different phases during their lifecycle:
     - **Phase** -	Description
     - **Pending** - 	The Pod is created but waiting for resources.
     - **Running** -	At least one container is running.
     - **Succeeded** -	All containers have successfully completed execution.
     - **Failed** - 	One or more containers exited with an error.
     - **Unknown** -	The Pod state cannot be determined.

#### Pod Restart Policies
- A Pod's restart policy controls how Kubernetes handles failed containers:
     - **Always** -	Restarts the container if it fails (default).
     - **OnFailure** -	Restarts only if it fails with an error.
     - **Never** -	Does not restart the container.

#### How Pods Are Managed
- Pods are usually managed by Controllers:
     - **Deployment** → Scales and manages ReplicaSets.
     - **StatefulSet** → Manages stateful applications.
     - **DaemonSet** → Runs a Pod on every node.
     - **Job & CronJob** → Runs Pods for batch processing.

#### Pod Networking
- details
     - Each Pod gets a unique IP.
     - Containers within a Pod share the same network namespace.
     - Communication between Pods happens using ClusterIP Services.

**pod pending**
- The node has a taint, which prevents scheduling unless the pod has a matching toleration.
     - we can add a toleration in the pod spec (if pod should run on tainted node)
     - we can remove the taint from the node (if not needed)
     - we can Use nodeSelector or nodeAffinity instead of taints (if controlling placement)
- **taint** is a rule applied to a Kubernetes node that prevents certain pods from being scheduled on it unless the pod has a matching toleration

#### Stateful & Stateless
- **stateful application** maintains data or session state, like databases or message queues. Kubernetes uses resources like StatefulSets and PersistentVolumes to manage stateful applications, ensuring stable network identities and storage persistence across pod restarts or rescheduling
     - Stateful apps retain data and require persistent storage. They use StatefulSets, each pod has a stable identity and its own storage. Example: databases like MySQL.

- **stateless application** does not store any data or client session information between requests. It can be easily scaled horizontally because all replicas behave the same — examples include web servers or front-end services
     - Stateless apps don’t store data between requests. They use Deployments, are easy to scale, and all pods are identical. Example: web servers.

#### StatefulSet
- StatefulSet is a Kubernetes controller used to manage stateful applications that require:
     - Stable network identities (e.g., pod-0, pod-1, pod-2).
     - Persistent storage that remains even if the Pod is restarted.
     - Ordered and controlled scaling (both up and down).
     - Common Use Cases:
          - Databases (MySQL, PostgreSQL, MongoDB).
          - Message Queues (Kafka, RabbitMQ).
          - Distributed Applications (Elasticsearch, Zookeeper)

#### ClusterIP
- it is internal communication between pods within the cluster. It assigns an internal IP and is only accessible inside the cluster.
- It's used for backend services, databases, and internal APIs. For external access, LoadBalancer, NodePort, or Ingress is used.

#### ExternalName
- it maps a service name to an external DNS instead of routing traffic to internal pods.
- It’s used to access external services like third-party APIs or databases using Kubernetes DNS, without exposing cluster resources.

#### Deployment
- A Deployment in Kubernetes manages the rollout and scaling of Pods. It ensures:
     - Declarative updates to applications.
     - Automated rollbacks if failures occur.
     - Scaling of replicas based on demand.
     - Rolling updates & zero downtime deployments.
     - Use Cases:
          - Deploying stateless applications (e.g., Nginx, API services).
          - Managing multiple replicas for high availability.
          - Performing rolling updates & rollbacks safely.
- Deployment Works
     - Define a Deployment (specifies the desired state).
     - Kubernetes creates and manages ReplicaSets, ensuring the correct number of Pods run.
     - Handles updates & rollbacks automatically to prevent downtime.
     - Uses labels and selectors to group Pods efficiently.

#### Stateful
- they are applications store and maintain data between requests. Unlike stateless applications, they require persistent storage and cannot be easily replaced without losing data.
- Key Characteristics of Stateful Applications:
     - Data is stored and maintained across sessions.
     - Requires persistent storage (databases, logs, etc.).
     - Scaling is complex due to data consistency needs.
     - Failure of a node may require data recovery or replication.
- Examples of Stateful Applications:
     - Databases: MySQL, PostgreSQL, MongoDB.
     - Message Queues: Kafka, RabbitMQ.
     - Storage Systems: ElasticSearch, Cassandra.
     - Applications with user sessions: Banking apps, e-commerce carts.

#### Stateless
- Stateless applications do not store data or session state on the server. Each request is independent and does not rely on previous requests.
- Key Characteristics:
     - No persistent storage needed.
     - Each request is handled separately.
     - Can be easily scaled horizontally (add more replicas).
     - Failure of a single instance does not affect the overall system.
- Examples of Stateless Applications:
     - Web servers (Nginx, Apache).
     - RESTful APIs.
     - Microservices.
     - Containers running ephemeral workloads.

#### Microservices
- its an architectural style where an application is built as a collection of small, independent services that communicate over a network (usually via APIs).
- Each microservice is responsible for a specific business functionality and can be developed, deployed, and scaled independently
     - **Large-Scale Applications**: If your application is complex and needs independent scaling.
     - **Frequent Deployments**: When you need CI/CD pipelines and faster releases.
     - **Multiple Development Teams**: Ideal when teams work on separate services.
     - **Cloud & Container-Based Deployments**: Best suited for Kubernetes, Docker, AWS Lambda.
     - **Business Domain Separation**: When different parts of your system have distinct business logic.

##### Nodeport
- it exposes a service on a static port on all nodes in the cluster. This allows external traffic to access the service using NodeIP:NodePort.
- NodePort Works
     - A NodePort service selects a pod using labels.
     - Kubernetes assigns a static port (default range: 30000–32767).
     - The service becomes accessible via http://<NodeIP>:<NodePort>.
     - Any request to this port is forwarded to the underlying pods

#### Node
- its the fundamental compute unit in a Kubernetes cluster where containers run. Nodes are worker machines (can be physical or virtual) that execute Pods.
- Each node contains essential services to run containers, such as:
     - Kubelet – Communicates with the control plane and manages pods.
     - Container Runtime – Runs containerized applications (Docker, containerd, CRI-O).
     - Kube Proxy – Handles networking and service communication.

#### Service Mesh
- its an infrastructure layer that manages service-to-service communication in microservices, enhancing security, observability, reliability without changing application code.
     - It abstracts networking complexities and ensures seamless service interactions.
     - its a dedicated networking layer that helps manage communication between microservices in a Kubernetes cluster.
     - It provides traffic control, security, observability, and reliability without modifying the application code
- 𝗔𝗿𝗰𝗵𝗶𝘁𝗲𝗰𝘁𝘂𝗿𝗲
     - **Data Plane** – Handles the actual traffic flow between services, typically using sidecar proxies like Envoy.
     - **Control Plane** – Manages policies, security, service discovery, and observability for better control and governance.
- 𝗞𝗲𝘆 𝗕𝗲𝗻𝗲𝗳𝗶𝘁𝘀
     - Traffic Control – Load balancing, failover, retries
     - Security – Mutual TLS (mTLS), authentication, authorization, zero-trust security
     - Observability – Distributed tracing, logging, metrics for debugging and monitoring
     - Resilience – Circuit breaking, rate limiting, retries, fault injection
     - Multi-cluster & Multi-cloud Support – Connects services across Kubernetes clusters and hybrid cloud environments

- 𝗪𝗵𝗲𝗻 𝘁𝗼 𝗨𝘀𝗲 𝗮 𝗦𝗲𝗿𝘃𝗶𝗰𝗲 𝗠𝗲𝘀𝗵?
     - Large-scale microservices architectures with complex service-to-service communication
     - Need for end-to-end security with zero-trust networking (mTLS, authorization)
     - Deep observability for debugging, monitoring, and tracing service interactions
     - Managing multi-cluster or hybrid cloud deployments
     - Requirement for advanced traffic policies like canary deployments, blue-green deployments

- 𝗣𝗼𝗽𝘂𝗹𝗮𝗿 𝗦𝗲𝗿𝘃𝗶𝗰𝗲 𝗠𝗲𝘀𝗵𝗲𝘀
     - Istio – Feature-rich, Kubernetes-native, integrates well with K8s RBAC and policies
     - Linkerd – Lightweight, easy to use, lower resource footprint
     - HashiCorp Consul – Multi-platform, supports VM and Kubernetes environments, strong service discovery capabilities
     - Kuma – Built on Envoy, supports both Kubernetes and VMs, good for multi-cloud setups
     - AWS App Mesh – Fully managed, deeply integrated with AWS services

#### Stateless vs. Stateful

|Feature|Stateless|Stateful|
|- |-|-|
|Data Storage|No persistent storage|Uses persistent storage|
|Session Handling|Each request is independent|Requires session tracking|
|Scaling|Easily scalable|More complex to scale|
|Resilience|Any instance can handle requests|Requires coordination between instances|
|Use Case|Web servers, APIs, microservices|Databases, Kafka, Zookeeper|

#### PV - Persistent Volumes
- those are storage resource that provides persistent storage to applications, ensuring that data remains available even if a Pod restarts or is rescheduled to another node.
- those are storage resources that exist independently of Pods. They provide a way to retain data even if a Pod is deleted, rescheduled, or crashes.
     - **Decouples storage from Pods** – Data persists beyond Pod lifecycles.
     - **Supports dynamic provisioning** – Automatically creates storage on demand.
     - **Multiple storage backends** – AWS EBS, Azure Disk.
     - **Access modes define how PVs are used** – ReadWriteOnce, ReadOnlyMany, ReadWriteMany.

#### PVC - Persistent Volume Claims
- They are requests for storage by pods
- they are like request for storage by a Pod in Kubernetes. It allows applications to use persistent storage that remains intact even if a Pod is restarted or rescheduled.
     - A PVC requests storage from a Persistent Volume (PV).
     - Ensures data persistence across Pod restarts and rescheduling.
     - Works with different storage backends ( EBS, NFS, Azure Disk ).
     - Uses Storage Classes for dynamic volume provisioning.

#### Operator
- An operator is an automated way to manage complex applications using custom resources.
- It combines two key components:
     - **Controller**: A controller continuously monitors the state of resources and ensures the actual state matches the desired configuration. When something changes or fails, the controller works to bring things back into alignment.
     - **Custom Resources (CRs)**: These are user-defined extensions to the Kubernetes API, allowing new functionality not provided by default. Custom Resource Definitions (CRDs) let you define new resources to represent your app's needs.
![image](https://github.com/user-attachments/assets/732ca853-8ffb-4d94-9c08-42cd56991e6c)


#### YAML – Deployment with Environment Variable - Kubernetes

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          env:
            - name: ENVIRONMENT
              value: "production"
```

- Explanation:
     - Deployment: Ensures specified number of pods are running.
     - replicas: 3: Run 3 pods.
     - matchLabels: Matches pods to this deployment.
     - template: Pod definition inside deployment.
     - env: Sets an environment variable inside the container.
- Contents 
     - **apiVersion** and kind define the resource type
     - **metadata** specify the details of deployment such as the name of the deployment, labels.
     - **spec** defines the desired state of the Deployment.
     - **replicas** specify the desired number of pods to maintain.
     - **selector** - specifies on which pod the replica configuration should be applied with the help of the label of the pod.
     - **template** describes the pod template and how deployment will use it to create new pods.
     - **containers** will list the containers to run within the pod.
     - **name** specifies the name of the container
     - **image** specifies the name that will be used to run the container. The image will be a Docker Image.
     - **containerport** specifies the port at which the container will listen for incoming traffic.
Use Case: **PostgreSQL Database Operator**
- Imagine setting up a highly available PostgreSQL database. With an Operator, you can automate scaling, backup, and failover with just a few configuration settings, without manually configuring these tasks every time.

```
apiVersion: postgresql.dev/v1
kind: PostgresCluster
metadata:
name: techops-database
spec:
instances: 3
backups:
enable: true
schedule: "0 3 * * *" # Daily backups at 3 am
storage:
size: 100Gi
```

#### Kubernetes errors and fix
#### CrashLoopBackOff in a Production Pod
-  A critical application pod was stuck in a CrashLoopBackOff state. Logs showed an OutOfMemoryError.
- The container repeatedly crashes after starting.
     - Application misconfiguration
     - Missing dependencies
     - Insufficient resources (CPU/Memory)
     - Improper readiness/liveness probes
     - The container exits immediately after execution
          - kubectl logs <pod-name> -n <namespace>
          - kubectl describe pod <pod-name> -n <namespace>
          - Increase CPU/memory limits in the pod spec..
          - Ensure application handles errors properly.
     - Optimized JVM heap size (for Java app).
     - Deployed with a resource monitoring policy.

#### **ImagePullBackOff / ErrImagePull** 
- Issue: New deployment failed with ImagePullBackOff due to missing authentication credentials for a private container registry.
- when Kubernetes cannot pull the container image from the registry.
     - Incorrect image name or tag
     - Authentication issues with private registry
     - Image does not exist in the registry
          - Verify the image name and tag
          - If using a private registry, ensure correct credentials:
          - Restart pod after fixing the issue.
     - Verified registry access (kubectl get secrets).
     - Recreated the imagePullSecret and added it to the service account.
     - Updated deployment YAML to reference the correct secret.
     - Used kubectl describe pod to confirm successful pull.
       
#### OOMKilled (Out of Memory)
- The container exceeds its allocated memory limit.
     -  Insufficient memory allocated in the pod spec.
     -  A memory leak in the application.
          - Increase memory limits in the pod YAML
          - kubectl top pod <pod-name> -n <namespace>

#### NodeNotReady
- The node is not in a ready state.
     - Node running out of resources
     - Network connectivity issues
     - Kubernetes components (kubelet, etc.) not running properly
          - kubectl get nodes
          - kubectl describe node <node-name>
          - systemctl restart kubelet
          - journalctl -u kubelet -f
            
#### PodPending
- The pod is stuck in a pending state and is not getting scheduled.
     - Insufficient resources in the cluster
     - NodeSelector, Taints, or Affinity rules preventing scheduling
     - PVC (Persistent Volume Claim) is not bound
          -  kubectl describe pod <pod-name>
          -  kubectl describe nodes
          -  kubectl get pvc
            
#### CreateContainerConfigError
- The container cannot be created due to incorrect configurations.
     - Incorrect environment variables
     - Secret or ConfigMap missing
     - Volume mount issues
          -  kubectl describe pod <pod-name>
          - kubectl get configmap
          - kubectl get secret
            
#### CreateContainerError
- Kubernetes fails to start the container.
     - Problems with the container runtime
     - Insufficient privileges
     - Host path issues
          - journalctl -u containerd -f

#### DeadlineExceeded
- A job or request takes too long and gets terminated.
     - Timeout in readiness/liveness probes
     - Network issues preventing completion
          - kubectl get svc

#### BackOff
- Kubernetes retries starting a container but fails repeatedly.
     - Failed dependencies
     - Readiness probe failures
          - kubectl get events --sort-by=.metadata.creationTimestamp
            
#### Unauthorized / Forbidden Errors
- Permissions issue with Kubernetes RBAC. 
     - ServiceAccount lacks required roles
     - Missing ClusterRole or Role bindings
          - kubectl auth can-i <action> <resource>
          - kubectl get rolebinding -A

#### PersistentVolumeClaim (PVC) Stuck in Pending
- Issue: An application required persistent storage, but the PVC was stuck in Pending.
     - Checked storage class (kubectl get storageclass).
     - Found that no available PersistentVolumes matched the PVC.
     - Created a new PersistentVolume and bound it manually.
     - Automated PVC creation via Helm to avoid misconfigurations.

#### High Latency in Services Due to Improper HPA
- Issue: A microservice was experiencing high response times under load, even though Horizontal Pod Autoscaler (HPA) was enabled.
     - Investigated using kubectl top pods and kubectl describe hpa.
     - Found that CPU-based scaling was configured, but the application was memory-intensive.
     - Changed HPA to scale based on memory and custom application metrics.
     - Improved readinessProbe to remove unresponsive pods from load balancer rotation.

#### Kubernetes Cluster Node Failure (Production Incident)
- Issue: One of the worker nodes in the cluster went NotReady, causing some workloads to become unavailable.
     - Used kubectl get nodes and kubectl describe node to check node status.
     - Found an issue with disk space (Filesystem full).
     - Cleaned up unused Docker images and logs.
     - Restarted the kubelet service (systemctl restart kubelet).
     - Applied auto-recovery automation via node draining and re-provisioning.

#### Network Latency Between Services (Intermittent 5XX Errors)
- Issue: Microservices communication was unstable, leading to intermittent 502 and 504 errors.
     - Used kubectl exec to test connectivity and kubectl logs for debugging.
     - Found that istio sidecar injection was missing for some pods.
     - Re-deployed affected services with Istio sidecar enabled.
     - Implemented a retry policy at the Istio virtual service level.

#### Kubernetes Ingress Controller Misconfiguration
- Issue: A new service was not accessible externally, and Ingress showed 404 Not Found.
     - Used kubectl describe ingress to check misconfiguration.
     - Found missing host field in ingress rules.
     - Updated the Ingress YAML with the correct path and host.
     - Restarted the Nginx Ingress Controller pod.

#### Secrets Management Issue (Application Failing to Start)
- Issue: An application failed to start due to missing environment variables that were sourced from Kubernetes Secrets.
     - Verified secrets using kubectl get secrets and kubectl describe secrets.
     - Found incorrect reference in deployment YAML.
     - Updated the deployment to use envFrom instead of manually specifying each variable.
     - Enabled automatic secret rotation via an external secrets manager.

#### HPA (Horizontal Pod Autoscaler)  & VPA ( Vertical Pod Autoscaler )
- **HPA (Horizontal Pod Autoscaler)** : when we set it automatically scale the number of pods in a deployment, replica set, or stateful set based on observed CPU/memory utilization or custom metrics.
- **VPA ( Vertical Pod Autoscaler )** : its a component that monitors resource consumption and automatically adjusts pod CPU and memory requests.
- Unlike Horizontal Pod Autoscaler (HPA), which scales the number of pods, VPA scales up or down the resources assigned to each pod

**HPA in detail :**
1. **Benifits:**
     - **Handles Dynamic Traffic** – Automatically adjusts pod count based on real-time demand.
     - **Improves Application Availability** – Prevents performance degradation during high loads.
     - **Optimizes Resource Costs** – Scales down pods when demand is low to save resources.
     - **Works with Custom Metrics** – Can scale pods based on application-specific metrics (e.g., request latency).
2. **How Does HPA Work**
     - HPA continuously monitors metrics and adjusts the number of pods accordingly:
     - **Metrics Collection** – HPA uses the Kubernetes Metrics Server or external/custom metrics to evaluate CPU, memory, or other parameters.
     - **Scaling Decision** – If resource usage exceeds the defined threshold, HPA adds more pods; if usage drops, it removes pods.
     - **Pod Scaling** – Kubernetes adjusts the number of running pods in the deployment.
3. **Best Practices for HPA**
     - Use with VPA – Combine HPA for pod scaling and VPA for resource tuning.
     - Define Proper Min/Max Limits – Prevent excessive scaling that may affect performance or cost.
     - Monitor Scaling Behavior – Use monitoring tools like Prometheus + Grafana.
     - Use Custom Metrics – Scale based on real-world application load (e.g., request count).
4. **When to Use HPA**
     - Web applications with fluctuating traffic.
     - Microservices that need dynamic scaling.
     - Event-driven applications where load varies based on external triggers.

**VPA in detail :**
1. **Benifits:**
     - **Optimized Resource Usage:** Ensures applications get only the necessary resources, reducing underutilization.
     - **Improved Performance:** Prevents CPU throttling and Out of Memory (OOM) issues by assigning appropriate resources.
     - **Reduced Manual Effort:** Automates resource allocation, reducing the need for manual tuning.
     - **Better Cost Management:** Prevents over-provisioning and minimizes cloud resource costs.
2. **How Does VPA Work**
- VPA operates in three main modes:
     - **Off Mode** – Only provides recommendations without making actual changes.
     - **Auto Mode** – Automatically updates pod resource requests.
     - **Initial Mode** – Sets resource requests only at pod startup.
3. **VPA Components**
     - **VPA Recommender**: Analyzes pod resource usage and suggests CPU/memory adjustments.
     - **VPA Updater:** Reschedules pods with updated resource limits.
     - **VPA Admission Controller:** Adjusts resource requests for newly created pods.
4. **Best Practices for Using VPA**
     - **Use VPA with HPA:** Combine VPA for resource tuning and HPA for scaling pods.
     - **Enable VPA in "Off" Mode First:** Start with recommendations before enabling auto mode.
     - **Monitor Pod Restart Impact:** Since VPA may restart pods, be cautious with stateful applications.
     - **Use Custom Metrics:** Fine-tune scaling policies based on real-world application behavior.
5. **When to Use VPA**
     - For stateful applications (databases, caching services) that need controlled resource scaling.
     - When optimizing CPU and memory allocation for cost efficiency.
     - In microservices environments, where resource usage varies across services.

#### K8s States and Components (kubectl get all) kubernetes
- When you run kubectl get all, you get an overview of various Kubernetes resources in the current namespace, including:
- **Core Components Displayed by kubectl get all**

| Component	| Description |
| ---------- |----------------------------------------- |
|Pod	| The smallest deployable unit in Kubernetes that runs a container or multiple containers. |
|Deployment	| Manages a set of identical pods, ensuring high availability and rolling updates. |
|ReplicaSet	| Ensures a specified number of pod replicas are running at all times. |
|StatefulSet |	Manages stateful applications, providing stable network IDs and persistent storage. |
|DaemonSet	| Ensures a copy of a pod runs on each (or some) nodes (e.g., logging or monitoring agents). |
|Service	| Exposes a set of pods as a network service. |
|Ingress	| Manages external access to services, usually via HTTP/HTTPS. |
|Job	| Runs a task to completion, ensuring it runs a specified number of times. |
|CronJob |	Runs scheduled jobs at specified intervals. |

- **Key Fields in kubectl get all Output** - General Resource Fields

|Field	| Description |
| ---------- |----------------------------------------- |
|Name	| The name of the resource (Pod, Deployment, Service, etc.).|
|Type	| The type of resource (e.g., ClusterIP, NodePort, LoadBalancer for services).|
|Cluster-IP |	Internal IP of a service within the cluster (if None, it means it's a headless service). |
|External-IP	| IP accessible from outside the cluster (only for LoadBalancer or NodePort services).|
|Ports |	Ports exposed by the service or pod (e.g., 9093, 9094, 8080, etc.).|
|Age	| The duration since the resource was created. |

 - **Deployment & ReplicaSet Specific Fields**

|Field	| Description |
| ---------- |----------------------------------------- |
|Desired |	Number of replicas expected in a Deployment or ReplicaSet.|
|Current	| Number of replicas currently scheduled.|
|Ready	| Number of replicas that are successfully running and ready to serve traffic. |
|Up-to-Date |	Number of replicas updated to the latest version.|
|Available |	Number of replicas available to serve traffic.|

#### Ingress
- Ingress Architecture
     - Ingress Controller: Handles external traffic (e.g., Nginx, Traefik)
     - Ingress Resource: Defines routing rules (ingress.yaml)
     - Service: Exposes your application inside Kubernetes
     - Pods: Run your application
- **Ingress Controller **
- its a component in Kubernetes that manages external access to services inside a cluster, typically via HTTP/HTTPS. It routes requests based on hostnames, paths, or other rules
     - Single Entry Point – Manages all external traffic
     - Path-Based Routing – Routes /api to one service, /web to another
     - SSL Termination – Manages HTTPS for all services
     - Load Balancing – Distributes traffic between pods
     - Authentication & Security – Enables OAuth, JWT, and IP whitelisting

#### Step-by-Step Process for Kubernetes Cluster Deployment

- We need to 
     - Automate everything using Terraform, Ansible, Helm, ArgoCD.
     - Ensure security with IAM, RBAC, Secrets Management, Network Policies.
     - Optimize observability with Prometheus, Grafana, ELK, Datadog.
     - Improve resilience with Chaos Engineering, Backup Strategies, Disaster Recovery.

1. Planning & Architecture Design
     - Assessed business requirements (e.g., high availability, security, scaling needs).
     - Chose Cloud Provider (AWS EKS, Azure AKS, or on-premises with Kubernetes).
     - Defined Cluster Topology:
          - Master & Worker Nodes placement.
          - Node Pools for different workloads (e.g., General-purpose, GPU, Spot instances).
          - Network Design (VPC, Subnets, CIDR blocks, Security Groups)
      
2. Infrastructure Provisioning (IaC)
     - Used Terraform & Ansible to automate infrastructure deployment:
          - Terraform for provisioning cloud resources (VPC, Subnets, EKS/AKS Cluster, IAM roles, Storage).
          - Ansible for configuring nodes, setting up monitoring agents, and applying security hardening.
- Example Terraform Code for AWS EKS Deployment
```
module "eks" {
  source          = "terraform-aws-modules/eks/aws"
  cluster_name    = "prod-cluster"
  cluster_version = "1.26"
  subnets         = module.vpc.private_subnets
  vpc_id          = module.vpc.vpc_id

  node_groups = {
    worker_nodes = {
      desired_capacity = 3
      max_capacity     = 5
      min_capacity     = 2
      instance_types   = ["t3.medium"]
    }
  }
}
```

3. Cluster Bootstrapping & Configuration
     - Deploying Kubernetes Control Plane & Worker Nodes:
          - Used eksctl for AWS or az aks create for Azure.
          - Set up RBAC & IAM roles for secure access control.
          - Configured Kubeconfig to connect the cluster.
          - Enabled Cluster Autoscaler & Horizontal Pod Autoscaler (HPA) for auto-scaling.
     - Example Command for EKS Cluster Creation:
          - eksctl create cluster --name prod-cluster --region us-east-1 --nodegroup-name worker-nodes --nodes 3
          - az aks create --resource-group myResourceGroup --name myAKSCluster --node-count 3 --enable-managed-identity

4. Networking & Security Setup
     - Configured Ingress Controller (NGINX/Istio) for external traffic routing.
     - Set up Network Policies to restrict pod-to-pod communication.
     - Integrated Cloud Security Best Practices:
          - IAM Roles for Service Accounts (IRSA) for pod security.
          - AWS Security Groups/NACLs for network-level protection.
          - Secrets Management with AWS Secrets Manager/HashiCorp Vault.
- Example Network Policy to Restrict Traffic
```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: restrict-ingress
spec:
  podSelector:
    matchLabels:
      app: my-app
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: allowed-app
```

5. Deploying Applications & Services
     - Used Helm Charts for standardized app deployment.
     - ArgoCD/GitOps for continuous delivery.
     - Canary & Blue-Green Deployment for zero-downtime releases.
- Helm Command to Deploy an Application:
```
# Helm Command to Deploy an Application
helm install my-app ./charts/my-app
----
# ArgoCD Deployment Manifest Example
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: my-app
spec:
  source:
    repoURL: 'https://github.com/my-org/my-app.git'
    path: charts/my-app
    targetRevision: HEAD
  destination:
    server: https://kubernetes.default.svc
    namespace: my-namespace
```

6. Observability & Monitoring Setup
     - Installed Prometheus & Grafana for metrics.
     - Set up ELK/Splunk for log aggregation.
     - Configured Datadog/New Relic for APM (Application Performance Monitoring).
     - Integrated Alerting via Slack/Teams using Alertmanager.
- Example Prometheus Alert Rule for High CPU Usage:
```
groups:
- name: High CPU Usage Alert
  rules:
  - alert: HighCPUUsage
    expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "High CPU usage on instance {{ $labels.instance }}"
```

7. Scaling, Security & Disaster Recovery
     - Configured Cluster Autoscaler & VPA (Vertical Pod Autoscaler).
     - Set up Pod Disruption Budgets (PDBs) to ensure high availability.
     - Configured AWS Backup & EFS snapshots for disaster recovery.
     - Conducted Chaos Engineering (using LitmusChaos) to test system resilience.
- Example Pod Disruption Budget Configuration:
```
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-app-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: my-app
```

8. Incident Management & Continuous Improvements
     - Integrated SLO/SLI/SLA tracking.
     - Set up blameless postmortems and RCAs for production issues.
     - Conducted weekly optimizations & security audits to enhance performance.
- Example SLI/SLO Definition in Prometheus:
```
- record: slo:availability
  expr: sum(rate(http_requests_total{status=~"2.."}[5m])) / sum(rate(http_requests_total[5m])) * 100
```

- Final Outcome & Impact
     - Deployment time reduced from 2 hours to 15 minutes using Terraform & CI/CD.
     - MTTR (Mean Time to Recovery) reduced by 40% with proactive monitoring.
     - Cloud cost optimized by 30% using auto-scaling & Spot instances.
     - System availability improved to 99.99% with robust DR & HA strategies.

#### SLO, SLA, SLI & Error Budget
1. **SLA (Service Level Agreement)** - its a business-level commitments based on SLOs, often with penalties. **External commitment (contract between provider & customer).**
   - Example: "99.95% uptime per month, or a 5% refund."
   - Use: Ensures accountability; failing an SLA can have financial/legal consequences.
2. **SLO (Service Level Objective)** - A target value for an SLI that defines acceptable performance.  **Internal goal (target reliability, e.g., 99.99% uptime).**
   - Example: "99.9% uptime over 30 days" or "p99 latency < 500ms".
   - Use: Sets a goal for reliability and user experience.
3. **SLI (Service Level Indicator)** - A quantitative measure of a system's performance. - the raw metrics - **Measured metric (actual 99.97% uptime recorded).**
   - Example: Response time, uptime percentage, error rate.
   - Use: Helps track how well a service is performing.
4. **Error Budget** - The allowable margin of failure within an SLO.
   - Example: If the SLO is 99.9% uptime, the error budget is 0.1% downtime (about 43.8 minutes per month).
   - Use: Helps balance innovation and reliability.
   - Error budget defines the acceptable level of failure before SLO violations.
![image](https://github.com/user-attachments/assets/7b93fc84-a951-417f-8190-2863db298798)

![image](https://github.com/user-attachments/assets/44042a63-ef17-43e9-96d7-877fdd1d7b7c)

**Metrics :**
1. Uptime/Availability (e.g., 99.9% availability)
2. Response Time (e.g., incidents responded to within 15 minutes)
3. Resolution Time (e.g., critical issues fixed within 4 hours)
4. Performance Metrics (e.g., API response time < 200ms)
5. Error Rates (e.g., < 0.01% failure rate)
6. Support & Escalation Process

#### API
An API (**Application Programming Interface**) is a set of rules and protocols that allows different software applications to communicate with each other. It defines how requests and responses should be structured, enabling seamless interaction between systems, applications, or services.
1. **General Explanation**
     - Facilitates communication between different software systems.
     - Enables modular and scalable development.
     - Promotes reusability of functionalities.
2. **For Developers (Backend, Frontend, Full Stack, Mobile, etc.)**
     - Backend Dev: Helps expose business logic securely to frontend applications.
     - Frontend Dev: Fetches and displays data dynamically using APIs.
     - Mobile Dev: Allows mobile apps to interact with cloud-based services.
3. **For System Architects & DevOps**
     - Helps integrate microservices for scalable applications.
     - Ensures interoperability between different platforms (e.g., cloud services).
4. **For Data Engineers & AI Engineers**
     - Enables fetching and sending large-scale data efficiently.
     - Provides access to external AI models and services.
- **Stages in API Gateway Deployment**
     - When creating an API Gateway (AWS API Gateway, Kong, Apigee, or any other API management tool), multiple stages allow for controlled deployment, testing, and versioning of APIs.
- **Common Stages in API Gateway**
     - **dev**	Development stage (used by developers, may have debugging enabled)
     - **test**	Used for API testing (integration and unit tests)
     - **staging (pre-prod)**	Pre-production stage, mirrors production for final validation
     - **prod**	Live production API, used by real customers
     - **beta**	Used for limited user testing before full production release
- **How SRE & DevOps Use API Gateway Stages**
     - **Traffic Routing** → Different environments for controlled API releases
     - **Rate Limiting & Throttling** → More relaxed in dev, strict in prod
     - **Logging & Monitoring** → More detailed logs in test/dev, essential metrics in prod
     - **Feature Flags** → Enable/disable API features per stage
     - **Rollback Mechanisms** → Deploy a stable previous version in case of issues
- **Canary Deployment & Blue-Green Deployment**
     - **Canary Release** → Gradually roll out new API versions to a small % of users before full deployment.
     - **Blue-Green Deployment** → Switch traffic between old (blue) and new (green) versions instantly.

#### REST API
**Representational State Transfer Application Programming Interface**
- its a web service which follows RESTful principles, it enables communication between client and server over HTTP.
     - **Stateless:** Each request is independent and does not store client session data.
     - **Client-Server Architecture:** Separation of concerns between UI (client) and backend (server).
     - **Cacheable:** Responses can be cached to improve performance.
     - **Uniform Interface:** Uses standard HTTP methods (GET, POST, PUT, DELETE).
     - **Resource-Based:** Data is represented as resources (e.g., /users, /orders).

#### RESTful API (Representational State Transfer API) 
- its a web service architecture style that allows systems to communicate over HTTP using simple, stateless operations. It's widely used in modern applications, especially in microservices and web development.
- Used in DevOps/SRE
     - REST APIs for cloud platforms (AWS, Azure)
     - Monitoring tools like Prometheus or Grafana expose APIs
     - CI/CD tools (like Jenkins, GitLab) expose APIs for triggering jobs
     - Terraform Cloud and ArgoCD also expose RESTful APIs for automation

#### API Key
- its an unique identifier used to authenticate requests made to an API. It helps control access and track API usage.
     - Client sends a request with an API key in the header, URL, or request body.
     - API server validates the key against a database or authentication system.
     - If valid, the request is processed; otherwise, it's rejected.
- use cases
     - **Authentication & Access Control** – Verify who is making API requests.
     - Rate Limiting & Throttling – Prevent abuse by restricting request frequency.
     - Monitoring & Logging – Track API usage for security and billing.
     - Integrations & Automation – Used in CI/CD pipelines, monitoring tools, and cloud services

#### API access
- details
     - it refers to how services, users, and applications interact with APIs securely and efficiently.
     - we need to manage API authentication, authorization, security, and monitoring.
     - secure API access in a DevOps environment
          - Authentication – Use OAuth 2.0, JWT, API keys, AWS IAM Roles to verify identity.
          - Authorization – Implement RBAC (Role-Based Access Control) or ABAC (Attribute-Based Access Control).
          - Rate Limiting & Throttling – Prevent abuse using tools like AWS API Gateway, Kong, or Nginx.
          - Encryption – Use TLS/SSL for securing data in transit.
          - Logging & Monitoring – Track API requests via CloudWatch, Prometheus, ELK Stack, or Datadog.

#### API Gateway
- info
     - its a central entry point for managing, securing, and routing API requests between clients and backend services.
     - It acts as a reverse proxy that controls traffic, performs authentication, and enforces security policies.

#### TLS (Transport Layer Security) and SSL (Secure Sockets Layer) 
- Content
     - TLS (Transport Layer Security) is a cryptographic protocol that secures data transmission between systems over the internet.
     - It ensures confidentiality, integrity, and authentication of data, preventing eavesdropping, tampering, and impersonation.
- TLS ensures:
     - Confidentiality (Data is encrypted and cannot be read by unauthorized parties).
     - Integrity (Data is not altered during transmission).
     - Authentication (Validates the identity of the communicating parties).
- SSL (Secure Sockets Layer) is a cryptographic protocol designed to secure communication over the internet by encrypting data between a client (e.g., browser, API, microservice) and a server
- It helps protect sensitive information from hackers, ensuring privacy, integrity, and authentication.

- SSL/TLS Used (Real-World Use Cases)
     - Websites & APIs → HTTPS ensures secure web traffic (NGINX, Apache, AWS Load Balancer, API Gateway).
     - Kubernetes & Microservices → Mutual TLS (mTLS) secures communication (Istio, Linkerd).
     - Cloud Security → AWS, Azure, and GCP enforce TLS for data encryption.
     - DevOps Pipelines → TLS secures GitHub, Jenkins, CI/CD data transfers.
     - Database Encryption → TLS protects connections in MySQL, PostgreSQL, MongoDB.

#### API Server (kube-apiserver)
- The API Server is the central control plane component in Kubernetes, acting as the gateway for all interactions with the cluster. It validates, processes, and serves API requests from users, controllers, and other components.
- **Key Responsibilities of API Server**
- Handles All API Requests
     - The API server exposes the Kubernetes API over HTTP(S) and serves as the frontend for the control plane.
     - All interactions with the cluster (e.g., kubectl, UI, automation) go through the API server.
- Authentication & Authorization
     - It enforces authentication (Token, OAuth, Client Certificates, etc.).
     - Implements authorization mechanisms like RBAC, ABAC, and Webhook authorizers.
- Validates & Processes Requests
     - Ensures API requests are well-formed and conform to Kubernetes schema (using OpenAPI).
     - Performs request validation before persisting data.
- Acts as the Gatekeeper to etcd
     - Persists and retrieves cluster state from etcd, ensuring strong consistency.
     - The API server is the only component that directly communicates with etcd.
- Admission Control & Policy Enforcement
     - Uses admission controllers to enforce security, resource limits, and best practices.
     - Examples: Namespace limits, Pod Security Admission, ResourceQuota.
- Handles API Aggregation & Extensions
     - Supports Custom Resource Definitions (CRDs) and API Aggregation Layer to extend Kubernetes APIs

#### Application Gateway

- its one of the most powerful Layer 7 load balancers in Azure, especially when combined with AKS for secure ingress traffic.
- It provides SSL termination, URL-based routing, WAF, and host-based routing, making it a go-to choice for secure, scalable applications.

#### 7 Layers - protocols tcp UDP

|Layer|Name|Example Protocols & Devices|
|-|-|-|
|L7|Application Layer|HTTP, HTTPS, FTP, DNS, SMTP|
|L6|Presentation Layer|SSL/TLS, Encryption, ASCII|
|L5|Session Layer|RPC, NetBIOS, PPTP, SMB|
|L4|Transport Layer|TCP, UDP, QUIC|
|L3|Network Layer|IP, ICMP, BGP, OSPF, Routers|
|L2|Data Link Layer|Ethernet, MAC, VLAN, ARP|
|L1|Physical Layer|Cables, Hubs, Fiber Optics|

- Common Network Protocols and Their Use Cases
1. Transport Layer Protocols (Layer 4 - OSI Model)
     - TCP (**Transmission Control Protocol**)
          - Connection-oriented protocol ensuring reliable, ordered, and error-checked delivery.
          - Used in applications like HTTP(S), SSH, FTP, and email (SMTP, IMAP, POP3).
          - Provides mechanisms like three-way handshake, retransmissions, and flow control.
     - UDP (**User Datagram Protocol**)
          - Connectionless, faster than TCP but does not guarantee delivery.
          - Used for real-time applications like DNS, VoIP, video streaming, and gaming.
          - No congestion control or flow control, reducing overhead.
2. Network Layer Protocols (Layer 3 - OSI Model)
     - IP (Internet Protocol)
          - Responsible for routing packets across networks.
          - IPv4 (32-bit) and IPv6 (128-bit) addressing.
          - Works with ICMP for error reporting.
     - ICMP (Internet Control Message Protocol)
          - Used for diagnostic and error messages (e.g., ping, traceroute).
          - Helps in network troubleshooting by reporting unreachable hosts, TTL expiration, etc.
3. Application Layer Protocols (Layer 7 - OSI Model)
     - HTTP/HTTPS (Hypertext Transfer Protocol)
          - Used for web communication.
          - HTTPS is the secure version using SSL/TLS encryption.
     - DNS (Domain Name System)
          - Resolves domain names (e.g., example.com) to IP addresses.
          - Works with UDP (primary, port 53) and TCP (for large queries like zone transfers).
     - FTP/SFTP (File Transfer Protocol)
          - FTP (uses TCP, ports 20 & 21) for file transfers.
          - SFTP (uses SSH) for secure file transfers.
     - SMTP, IMAP, POP3 (Email Protocols)
          - SMTP (port 25/587) for sending emails.
          - IMAP (port 143/993) & POP3 (port 110/995) for retrieving emails.
4. Security Protocols
     - TLS/SSL (Transport Layer Security / Secure Sockets Layer)
          - Encrypts data for secure transmission (e.g., HTTPS).
          - TLS 1.3 is the latest and most secure version.
     - SSH (Secure Shell)
          - Secure remote access protocol.
          - Uses port 22 for encrypted communication.
5. Other Important Protocols
     - BGP (Border Gateway Protocol)
          - Used for internet routing between ISPs.
          - Plays a crucial role in high availability and network failover.
     - NTP (Network Time Protocol)
          - Synchronizes time across distributed systems.
     - SNMP (Simple Network Management Protocol)
          - Used for monitoring network devices like routers, switches, and servers.

- Activity of SRE
     - Troubleshooting: Diagnosing packet loss, latency, and connectivity issues.
     - Performance Optimization: Understanding TCP tuning, load balancing, and congestion control.
     - Security: Preventing attacks like DNS spoofing, TCP SYN flood, and man-in-the-middle attacks.
     - Reliability Engineering: Designing fault-tolerant, highly available, and scalable network architectures.

#### Critical Protocols for SRE

|Layer|Protocol|Reason to Use|
|-|-|-|
|Transport|TCP - Transmission Control Protocol|Reliable data transmission|
|Transport|UDP - User Datagram Protocol|Fast, low-latency communication|
|Network|IP (IPv4/IPv6)|Packet routing and addressing|
|Network|ICMP - Internet Control Message Protocol|Network troubleshooting (ping, traceroute)|
|Application|HTTP/HTTPS - Hypertext Transfer Protocol|Web traffic, API communication|
|Application|DNS - Domain Name System|Resolves domain names to IPs|
|Application|SSH - Secure Shell|Secure remote system access|
|Security|TLS/SSL - Transport Layer Security / Secure Sockets Layer|Encrypts web and API traffic|
|Security|OAuth 2.0/OIDC|Authentication & authorization|
|Monitoring|SNMP - Simple Network Management Protocol|Network and device monitoring|
|Monitoring|NTP - Network Time Protocol|Synchronizes time across servers|
|Routing|BGP - Border Gateway Protocol|Internet and cloud routing|
|Redundancy|VRRP - Virtual Router Redundancy Protocol|High availability of network gateways|

#### Shellscripting Components:
- Components
     - Shebang (#!/bin/bash)
     - Variables (e.g., name="John")
     - Comments (e.g., # This is a comment)
     - Commands (e.g., echo, ls)
     - Input/Output (read, echo)
     - Conditionals (if, else, elif)
     - Loops (for, while, until)
     - Functions (Reusable code blocks)
     - File Handling (reading, writing, redirecting)
     - Exit Status (Success/Failure indicators)
     - Pipes and Redirection (|, >, >>)
     - Arrays (Storing multiple values)
     - String Manipulation (e.g., substrings, replacements)
     - Exit Command (Ends script execution)
     - Trap (Handling signals)

#### Sample Shell Scripting

```
#!/bin/bash
SERVICE="nginx"
if systemctl is-active --quiet $SERVICE
then
    echo "$SERVICE is running."
else
    echo "$SERVICE is not running. Restarting..."
    systemctl restart $SERVICE
    if systemctl is-active --quiet $SERVICE
    then
        echo "$SERVICE restarted successfully."
    else
        echo "Failed to restart $SERVICE."
    fi
fi
```

#### shell script to execute command output

```
#!/bin/bash
# Execute a command and store its output
OUTPUT=$(ls -l)
# Print the output
echo "Command Output:"
echo "$OUTPUT"
```

#### shell script to get cpu utilization

```
#!/bin/bash
echo "Current CPU Usage:"
top -b -n1 | grep "Cpu(s)"
```

#### shell Script to Check if a Service is Running

```
#!/bin/bash
SERVICE="nginx"
if systemctl is-active --quiet $SERVICE; then
    echo "$SERVICE is running"
else
    echo "$SERVICE is not running"
fi
```

#### Shell Script to Delete Old Log Files

```
#!/bin/bash
LOG_DIR="/var/log"
find "$LOG_DIR" -type f -name "*.log" -mtime +7 -exec rm -f {} \;
echo "Old log files deleted."
```

#### Sample Bash scripting

**Backup Script**
```
#!/bin/bash
SOURCE="/home/user/Documents"
DEST="/home/user/backup"
tar -czvf "$DEST/backup_$(date +%Y%m%d).tar.gz" "$SOURCE"
echo "Backup completed."
```
|String | Summary |
|-|-|
|Shebang (#!)|Defines the script interpreter (e.g., #!/bin/bash).|
|Comments (#)|Used to add explanations.|
|Variables (VAR=value)|Store data for later use.|
|User Input (read)|Accepts input from users.|
|Conditional Statements (if, elif, else)|Controls flow based on conditions.|
|Loops (for, while, until)|Executes code multiple times.|
|Functions (function_name() {})|Groups reusable code.|
|Command Substitution ($(command) or `command`)|Stores command output in variables.|
|Arithmetic Operations ($((expression)))|Performs mathematical calculations.|
|String Manipulation|Extract, replace, or modify strings.|
|File Handling (cat, echo, touch, rm, mv, cp)|Reads, writes, and manages files.|
|Redirection (>, >>, <, 2>)|Directs input/output streams.|
|Pipes |)|Passes output of one command as input to another.|
|Background Execution (&)|Runs processes in the background.|
|Process Control (jobs, kill, wait)|Manages running processes.|
|Error Handling (, &&, set -e)|Handles script failures.|
|Logging (tee, logger)|Logs script outputs.|
|Cron Jobs (crontab -e)|Automates script execution.|
|Trap (trap 'command' SIGNAL)|Captures signals (e.g., SIGINT, EXIT).|
|Exit Status ($?)|Captures the return status of the last executed command.|

#### Unix commands

#### Linux / RHEL Boot Process
- BIOS / UEFI Initialization
     - BIOS/UEFI runs POST (Power-On Self Test).
     - Identifies bootable devices (like hard disks).
     - Loads the bootloader from the MBR (Master Boot Record) or EFI partition.
- Bootloader (GRUB2)
     - GRUB (GRand Unified Bootloader) is the default in RHEL.
     - Loads the Linux kernel and initial RAM disk (initramfs).
     - Presents a boot menu (if configured), allows kernel selection.
     - Passes boot parameters to the kernel.
- Kernel Initialization
     - Initializes hardware via drivers.
     - Mounts the initramfs (temporary root filesystem).
     - Looks for and mounts the real root filesystem.
- initramfs
     - Temporary root filesystem loaded into memory.
     - Contains essential drivers and scripts needed to mount the real root FS (e.g., LVM, RAID, iSCSI).
     - Switches control to the actual root filesystem after mounting.
- Systemd Initialization (PID 1)
     - Replaces traditional init (/sbin/init) in modern RHEL.
     - Reads unit files to initialize:
          - Targets (runlevels)
          - Services (networking, SSH, etc.)
     - Starts all configured services based on the default target (e.g., multi-user.target or graphical.target).
- Login Prompt
     - Once all services are started:
     - CLI (tty login) or GUI (GDM for graphical mode) is presented.
     - System is ready for use.

#### Init Levels (Runlevels) – Legacy SysVinit
|Runlevel	|Purpose|
|-|-|
|0|	Halt (Shutdown)|
|1|	Single User Mode (Maintenance)|
|2|	Multi-user without NFS (rare)|
|3|	Full Multi-user (CLI only)|
|4|	Unused / Custom|
|5|	Multi-user with GUI (X11)|
|6|	Reboot|

#### PID
- What the ID number 1 in unix
     - PID 1 is the first process started by the kernel after booting.

#### What is an Inode in Unix
- its a data structure used by Unix-like file systems to store metadata about a file or directory. Each file or directory has a unique inode number, which the system uses to manage files independently of their names.
- example :  ls -i filename

#### when a user corrupts the fstab file , how do you fix it
     - Boot into Recovery Mode manually
     - Remount the Root Filesystem in Read-Write Mode
     - mount -o remount,rw /
     - Edit the Corrupted /etc/fstab File
     - mount -a 
     - reboot

#### Soft and Hard Links in Unix/Linux
     - hard link is an additional directory entry (filename) that points to the same inode as the original file.
     - soft link (symlink) is a separate file that acts as a pointer (shortcut) to another file or directory.

#### Tier applications
- **Single-Tier(Monolithic)**- 1-Tier: Everything in one place (simpler, not scalable).
- **Two-Tier Application**   - 2-Tier: Client-server model (separation of UI and data logic, better scalability).
- **Three-Tier Application** - 3-Tier: Presentation, business logic, and data layers (best for scalability and maintainability).
- **N-Tier Application**: --- Multiple layers for specialized tasks (maximum flexibility and modularity).

#### Three-Tier Architecture - 3 tier
- its a software design pattern that separates an application into three layers: the presentation tier (UI), the application tier (logic), and the database tier (data storage).
- This separation improves scalability, security, and maintainability.
- Components Three-Tier Architecture

|Tier	|Components|	Example Services|
|-|-|-|
|Presentation Layer (UI/Frontend)	|User interface, Web pages|	React.js, Angular, Vue.js, AWS S3 (static hosting), CloudFront|
|Application Layer (Business Logic)|	APIs, Microservices, Backend processing|	AWS EC2, AWS Lambda, Kubernetes, AWS ECS, Spring Boot, Node.js|
|Database Layer (Data Storage)	|Databases, Data Caching	| AWS RDS, DynamoDB, PostgreSQL, MySQL, Redis, MongoDB|

- Managing Three-Tier Architecture
     - **Use Load Balancers:** AWS ALB, Nginx, HAProxy for distributing traffic efficiently.
     - **Enable Auto Scaling:** Auto Scaling Groups for EC2, Kubernetes HPA for scaling microservices.
     - **Monitor & Alert:** CloudWatch, Prometheus, Datadog, ELK for proactive monitoring.
     - **Use CI/CD Pipelines:** Automated deployments with Jenkins, GitHub Actions, ArgoCD.
     - **Implement Caching & CDN:** Redis, CloudFront, and Memcached to reduce latency.
     - **Ensure Security & Compliance:** IAM, Security Groups, WAF, S3 Bucket Policies for access control.
     - **Optimize Database Performance:** Indexing, read replicas, partitioning for database efficiency.

- handle failures in a Three-Tier Architecture
     - **Application Layer Failures** → Auto-scaling, blue-green deployments, circuit breaker patterns.
     - **Database Failures** → Multi-AZ RDS, automated backups, read replicas, failover strategies.
     - **Load Balancer Failures** → Deploy in multiple availability zones, set up health checks.
     - **Network Issues** → Route 53 failover routing, VPN/Direct Connect for hybrid setups.
     - **Security Incidents** → Use AWS GuardDuty, WAF, and CloudTrail for threat detection.

#### Helm Use Cases in a 3-Tier Architecture with Kubernetes ( Frontend, Backend, Database )
- A 3-tier architecture typically consists of:
     - Presentation Layer (Frontend - UI, Web Server)
     - Application Layer (Backend - APIs, Business Logic)
     - Database Layer (Database - MySQL, PostgreSQL, MongoDB)
- **Deploying a Scalable E-Commerce Application**
1. A company is deploying an e-commerce website using a 3-tier architecture on AWS EKS (Kubernetes). The goal is to automate deployments, manage configurations, and enable rollbacks efficiently.
- Architecture Breakdown
     - Frontend (React + Nginx) – Runs in a Kubernetes Deployment & exposed via an Ingress Controller.
     - Backend (Node.js API) – Microservices-based API handling business logic.
     - Database (PostgreSQL) – Stores user data, transactions, and product info.
2. Helm Chart for a 3-Tier Application
- Using Helm, we create a modular chart where each layer (frontend, backend, database) is a separate subchart, making it reusable and configurable.
- Helm Chart Structure
```ecommerce-helm-chart/
│── charts/                 # Subcharts for different tiers
│    ├── frontend/          # Frontend (React + Nginx)
│    ├── backend/           # Backend (Node.js API)
│    ├── database/          # Database (PostgreSQL)
│── templates/              # Parent chart templates
│── values.yaml             # Global configuration
│── Chart.yaml              # Metadata
```

1. Deploy Frontend (Presentation Layer)
     - The frontend runs in a Kubernetes Deployment with an Nginx web server and is exposed using an Ingress Controller.
     - Helm values file for frontend (frontend/values.yaml):
2. Deploy Backend (Application Layer)
     - The backend API (Node.js) interacts with the database and is exposed as a Kubernetes service for the frontend to communicate.
     - Helm values file for backend (backend/values.yaml):
3. Deploy Database (Data Layer)
     - The PostgreSQL database is deployed using a pre-built Helm chart (Bitnami’s PostgreSQL chart).
     - Helm values file for database (database/values.yaml):
4. Helm Chart Customization for Different Environments
     - Using Helm values overrides, we can deploy the same application in different environments with minimal changes.
     - Deploying to Staging vs. Production - staging-values.yaml
6. Helm Upgrade and Rollback for Zero-Downtime Deployments
     - If a new version of the backend needs to be deployed, Helm ensures a safe upgrade.
7. GitOps: Automating 3-Tier Deployment with ArgoCD
     - In a GitOps workflow, Helm can be integrated with ArgoCD for declarative deployment management.
8. Monitoring & Logging for the 3-Tier Application
     - Helm deploys Prometheus & Grafana for monitoring and ELK Stack (Elasticsearch, Logstash, Kibana) for logging.

#### How SRE Fixes Latency in a 3 Tier Application
- Identifying Where the Latency is Occurring ,  finding the bottleneck by analyzing:
     - **Frontend Latency** → Slow UI response, long loading times
     - **Backend Latency** → Slow API calls, timeouts
     - **Database Latency** → Slow queries, connection pool issues
- Tools Used for Monitoring & Debugging:
     - **APM (Application Performance Monitoring)** – Datadog, New Relic, Dynatrace
     - **Distributed Tracing** – Jaeger, AWS X-Ray, OpenTelemetry
     - **Logging & Metrics** – Prometheus, Grafana, ELK Stack, CloudWatch
- Analyze the Root Cause & Fix - Frontend Latency Fixes
     - Optimize Static Assets & Reduce Client-Side Processing
     - Use CDN (CloudFront, Akamai, Cloudflare) for faster delivery
     - Minify JavaScript, CSS, Images
     - Enable Browser Caching
     - Reduce API Calls from Frontend
- Backend Latency Fixes -  Improve API Response Time
     - Horizontal Scaling – Add more backend instances (Auto Scaling in AWS)
     - Optimize API Queries (Reduce redundant calls)
     - Enable Connection Pooling (Keepalive, gRPC)
     - Use Asynchronous Processing (Queue systems like Kafka, SQS)
     - Optimize Code & Framework (Lazy loading, caching)
-  Database Latency Fixes - Optimize Queries & Scaling
     - Indexing & Query Optimization (Use EXPLAIN ANALYZE to find slow queries)
     - Read Replicas & Caching (Redis, Memcached)
     - Database Connection Pooling (pgbouncer for PostgreSQL)
     - Partitioning & Sharding for large datasets

-  Implement Monitoring & Auto-Healing , Once fixed, we need ensure the problem does not reoccur by:
     - Setting up SLI/SLO Alerts (e.g., if API latency > 500ms, trigger alert)
     - Implementing Auto-Scaling (AWS EC2/Kubernetes HPA to add/remove instances)
     - Automating Failover & Circuit Breakers (e.g., Hystrix for degraded services)

#### SSL Certificate
- An SSL (Secure Sockets Layer) certificate is a digital certificate that authenticates a website's identity and enables encrypted communication between the server and the client (typically a web browser). It ensures data integrity, confidentiality, and trust by encrypting HTTP traffic using HTTPS (Hypertext Transfer Protocol Secure).

- How is an SSL Certificate Related to a Domain or Website
- contents
     - **Encryption & Security:** It ensures secure communication by encrypting data exchanged between users and the website.
     - **Authentication:** Confirms the legitimacy of a website, preventing phishing attacks.
     - **Trust & SEO Benefits:** Browsers mark sites without SSL as "Not Secure," and search engines prioritize HTTPS websites in rankings.
     - **HTTPS Implementation:** Websites need an SSL certificate to enable HTTPS and secure transactions.

- SSL Certificate in Azure (Using Azure Application Gateway or Azure Front Door)
**Steps to Add an SSL Certificate in Azure Application Gateway**:
1. Purchase or Obtain an SSL Certificate
     - Buy from a trusted Certificate Authority (CA) or generate a self-signed certificate (for internal use).
     - Convert the SSL certificate to PFX format (Azure requires this).
2. Upload the SSL Certificate to Azure Key Vault (Optional for Security)
     - Store it in Azure Key Vault for secure management.
3. Configure Azure Application Gateway with HTTPS
     - Go to Azure Portal → Application Gateway → Listeners.
     - Create an HTTPS Listener and upload the SSL certificate.
4. Update Backend and Rules
     - Configure routing rules to ensure secure traffic flow.
5. Test and Verify
     - Ensure HTTPS is working and redirect HTTP traffic to HTTPS.

- **SSL Certificate in AWS** (Using AWS Certificate Manager - ACM)
1. Request a Public SSL Certificate
     - Go to AWS Certificate Manager (ACM).
     - Request a new certificate for your domain (e.g., yourdomain.com).
     - Choose DNS validation (recommended) or Email validation.
2. Validate the Domain Ownership
     - For DNS validation: Add a CNAME record in Route 53 (or your DNS provider).
     - For Email validation: Approve the email from AWS.
3. Attach the SSL Certificate to AWS Services
     - Use the validated SSL certificate in:
     - AWS Load Balancer (ALB/NLB) → For HTTPS traffic.
     - CloudFront CDN → For secure content delivery.
     - API Gateway → For securing API endpoints.
4. Redirect HTTP to HTTPS (If Required)
     - Configure the Load Balancer or CloudFront to enforce HTTPS redirection.
5. Test and Verify
     - Open your website using HTTPS and check for a valid SSL certificate.

### Application Gateway vs Azure Load Balancers vs Front door

|Feature|	Azure Application Gateway	|Azure Load Balancer|	Azure Front Door|
|-|-|-|-|
|Layer|	Layer 7 (HTTP/S)|	Layer 4 (TCP/UDP)|	Layer 7 (HTTP/S)|
|Traffic Routing|	URL-based & host-based	|IP-based|	Global routing|
|Web Application Firewall (WAF)	|✅ Yes|	❌ No	|✅ Yes|
|SSL Termination	|✅ Yes|	❌ No|	✅ Yes|
|Multi-Region Traffic Distribution	|❌ No	|❌ No	|✅ Yes|
|Best Use Case|	Secure app-level routing & WAF protection|Load balancing for internal services|	Global traffic routing|

#### NPM
NPM (Node Package Manager) is a package manager for JavaScript, used to install, manage, and share JavaScript libraries and tools. It is the default package manager for Node.js and is widely used for JavaScript-based applications.
- When to Use NPM
     - Managing JavaScript-based infrastructure tools (e.g., AWS CDK, Terraform CDK)
     - Automating frontend CI/CD builds (e.g., React, Angular, Vue.js)
     - Deploying Node.js-based microservices (Docker, Kubernetes, Serverless)
     - Security scanning & compliance (npm audit)
     - Monitoring Node.js applications (New Relic, Datadog)

#### Memory Limit
Memory limit refers to the maximum amount of RAM allocated to a process, container, virtual machine, or function. memory limits help prevent excessive memory usage, improve stability, and optimize resource allocation.

- **Memory Limits in Different Environments & Use Cases**
     - **AWS Lambda**	128MB - 10GB # Prevents excessive memory use in serverless functions.
     - **Docker Containers**	Configurable via --memory # Restricts container memory to avoid host system overload.
     - **Kubernetes Pods**	Defined in resource limits (limits.memory) # Prevents memory leaks in pods and ensures fair resource sharing.
     - **Virtual Machines (VMs)**	Configurable per VM # Ensures proper memory allocation for multiple VMs.
     - **CI/CD Pipelines**	Limits for build agents # Prevents high-memory builds from affecting shared CI runners.
     - **Web Servers (Node.js, Java, Python, etc.)** Configurable in runtime settings # Prevents out-of-memory (OOM) errors in web applications.
-
     - Monitor actual memory usage before setting limits.
     - Use auto-scaling in cloud environments (AWS Lambda, Kubernetes HPA).

#### Port Mapping
- Port mapping is the process of redirecting traffic from a host machine's port to a container, VM, or application running inside it. It is commonly used in Docker, Kubernetes, and networking to expose services running in isolated environments.
     - A system has many ports (0-65535) for communication.
     - Some services run on specific ports (e.g., HTTP → 80, HTTPS → 443).
     - Port mapping bridges the gap between the host and the application inside a container/VM.
- Why Use Port Mapping? 
     - Expose containerized applications to external users
     - Allow multiple services to run on different ports
     - Securely control access to applications
     - Avoid port conflicts in multi-container setups

**Docker Port Mapping** - Expose a containerized Flask API running on port 5000:
```
docker run -p 8080:5000 myflaskapp
```
**Kubernetes Port Mapping** - Define a Kubernetes Service for a containerized app:
```apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: NodePort
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80         # Service Port
      targetPort: 3000 # Container Port
      nodePort: 30007  # Exposed NodePort
```

##### Environment Variables
- Environment variables are key-value pairs used to store configuration settings for applications. They allow you to keep sensitive or environment-specific data outside your code and are widely used
- **Why Use Environment Variables**
     - Configuration Management – Store database URLs, API keys, etc.
     - Security – Avoid hardcoding sensitive credentials in source code.
     - Portability – Define variables dynamically for different environments (dev, staging, prod).
     - CI/CD Pipelines – Pass environment-specific values to build and deploy stages.
     - Containerization – Pass variables into Docker, Kubernetes, and ECS containers.
- **Best Practices for Using Environment Variables**
     - Store secrets securely – Use AWS Secrets Manager, Azure Key Vault, or Kubernetes Secrets.
     - Use ConfigMaps for non-sensitive data in Kubernetes.
     - Avoid hardcoding variables in Dockerfiles – use .env files or docker run -e.
     - Standardize variable names across environments (APP_ENV, DB_USER, etc.).

#### Resource Limits
- Resource limits are constraints set on CPU and memory usage for containers, pods, virtual machines (VMs), and cloud functions to prevent excessive resource consumption.
     - Preventing resource starvation – Ensuring fair resource distribution.
     - Avoiding Out-of-Memory (OOM) kills – Preventing crashes due to excessive memory usage.
     - Cost optimization – Avoiding unnecessary over-provisioning.
     - Ensuring system stability – Preventing one process from overloading the entire system.

- Best Practices for Resource Limits
     - Use resource requests and limits in Kubernetes to ensure fair allocation.
     - Monitor actual usage using Prometheus, Grafana, or CloudWatch.
     - Optimize CI/CD pipelines to avoid high-memory builds.
     - Use Horizontal Pod Autoscaling (HPA) in Kubernetes to adjust resources dynamically.
     - Avoid setting limits too low, which may cause performance degradation.

#### Vulnerabilities scanning
- software, configurations, or infrastructure that can be exploited by attackers to **compromise security, leading to data breaches, unauthorized access, or service disruption**s.
1. **OS and Package Vulnerabilities** - Outdated or unpatched dependencies in Linux/Windows OS.
2. **Container Image Vulnerabilities** - Security flaws in Docker images.
3. **Code Vulnerabilities** - Flaws in source code (e.g., SQL injection, XSS).
4. **Infrastructure as Code (IaC)** - Issues in Terraform, CloudFormation, or Ansible configurations.
5. **Secrets and Credential Exposure** - Hardcoded secrets in repositories.\
6. **Runtime Vulnerabilities** - Security gaps in running containers, Kubernetes pods, or cloud workloads.
7. **Network Misconfigurations** - Open ports, weak firewall rules.
8. **CI/CD Pipeline Risks** - Insecure Jenkins jobs, exposed API keys.

|Stage|	Component|	Scanning Tool|	Purpose|
|----|---|---|---|
|Code Repository	|Source Code (GitHub, GitLab)	|SonarQube, Snyk, Semgrep	|Static code analysis (SAST)|
|CI/CD Pipeline	|Build Dependencies (Maven, npm, pip)	|Snyk, OWASP Dependency Check|	Dependency vulnerability scanning|
|Containerization	|Docker Images|	Trivy, Clair, Grype	|Image scanning|
|Kubernetes	|Running Pods, Configurations |	Kubescape, Kube-bench, Aqua Trivy |	Kubernetes security validation|
|Cloud Infra |	Terraform, AWS, Azure, GCP	|Checkov, tfsec, Prowler|	IaC security scanning|
|Runtime |Security	| Running Workloads	|Falco, Sysdig Secure	|Real-time anomaly detection|

**Files, Images, and Components Scanned**
1. **Source Code** - .java, .py, .js, .go, etc.
2. **Dependency Files** - package.json, requirements.txt, pom.xml, Gemfile
3. **Dockerfiles** - Dockerfile, .dockerignore
4. **Container Images** - nginx:latest, ubuntu:20.04
5. **Kubernetes Manifests** - deployment.yaml, service.yaml
6. **Terraform/CloudFormation** - main.tf, variables.tf
7. **Secrets & Config Files** - .env, config.json, kubeconfig

#### Fix Vulnerabilities
1. **Patch Management** - Update OS, dependencies, and base images.
2. **Use Secure Base Images** - Use minimal images like distroless, alpine.
3. **Remove Unused Packages** - Minimize attack surface.
4. **Implement Least Privilege** - Use proper IAM roles, RBAC policies.
5. **Secrets Management** - Use HashiCorp Vault, AWS Secrets Manager.
6. **Runtime Protection** - Implement Falco rules for security monitoring.
7. **CI/CD Hardening** - Secure build pipelines, scan artifacts before deployment.
8. **Regular Audits** - Periodic security assessments and penetration testing.

- in-detail
1. **Compute & Event-Driven Automation**
- **AWS Lambda / Azure Functions**
     - Used for automating CI/CD processes, log processing, auto-scaling, and incident response automation.
     - Example: Triggering a function to restart a failed pod in Kubernetes.
- **AWS Fargate / Azure Container Apps** 
     - Serverless container management, reducing the overhead of managing EC2/VMs.
     - Example: Running lightweight workloads that require auto-scaling without managing nodes.
2. **CI/CD Pipeline Automation**
- **GitHub Actions / GitLab CI/CD / AWS CodePipeline / Azure DevOps Pipelines**
     - Serverless CI/CD automation without managing Jenkins or dedicated servers.
     - Example: Deploying a serverless app using GitHub Actions with AWS Lambda.
3. Monitoring & Incident Response
- **AWS Step Functions / Azure Logic Apps**
     - Used for orchestrating workflows in an event-driven manner.
     - Example: Automating an incident response workflow for Kubernetes pod failures.
- **AWS EventBridge / Azure Event Grid**
     - Serverless event-driven messaging for microservices and logging.
     - Example: Forwarding logs from CloudWatch to an SRE dashboard.
- **Amazon CloudWatch / Azure Monitor**
     - Serverless monitoring and logging.
     - Example: Triggering Lambda/Functions based on log events for anomaly detection.
- **PagerDuty / AWS SNS / Twilio**
     - Serverless alerting and notifications.
     - Example: Automatically paging an SRE on-call when a service exceeds an error threshold

#### Serverless 
- Serverless application is an application that runs on cloud-managed services where developers don’t have to worry about provisioning, scaling, or maintaining infrastructure. The cloud provider automatically manages compute resources, scaling, and execution based on demand.
- Key Characteristics
     - No Infrastructure Management - No need to provision or maintain servers (handled by the cloud provider).
     - Auto-Scaling - Automatically scales based on traffic, executing only when needed.
     - Event-Driven Execution - Functions are triggered by events like HTTP requests, file uploads, or database updates.
     - Cost-Effective (Pay-as-You-Go) - Charges are based on actual execution time and resource usage instead of idle capacity.
     - High Availability and Fault Tolerance - Cloud providers ensure redundancy and failover mechanisms.

#### Serverless Application Architecture
- Serverless Web Application (AWS)
     - Components Used:
          - **Frontend**: Static website hosted on Amazon S3.
          - **Backend API**: Exposed using Amazon API Gateway.
          - **Business Logic**: Implemented as AWS Lambda functions.
          - **Database**: Uses Amazon DynamoDB (serverless NoSQL DB).
          - **Authentication**: Uses Amazon Cognito for user authentication.
          - **Event-Driven Processing**: Uses Amazon EventBridge or SNS for notifications.
     - Workflow Example:
          - User visits a website hosted on S3.
          - API requests are sent to API Gateway, which triggers an AWS Lambda function.
          - Lambda processes the request and stores/retrieves data from DynamoDB.
          - If an event occurs (e.g., a new user registers), an SNS notification is triggered.
          - The response is sent back to the user, and everything scales automatically.

- Serverless Application Use Cases
     - Web Applications → Frontend (S3) + API Gateway + Lambda + DynamoDB.
     - IoT Applications → Event processing with Lambda, S3 for data storage.
     - Automated Infrastructure Management → Serverless automation for DevOps (e.g., auto-scaling, monitoring alerts).
     - Event-Driven Processing → Log analysis, security scanning, data transformation.
     - CI/CD Automation → Serverless functions triggering deployments, running tests, or validating code.


1. AWS Lambda
     - Use Case: Automating infrastructure tasks, processing S3 file uploads, real-time data transformation in AWS Kinesis.
     - Where Used: Event-driven automation for CI/CD, log processing, auto-healing infrastructure.
2. Amazon API Gateway
     - Use Case: Exposing RESTful APIs without managing servers.
     - Where Used: Secure API management for microservices, integrating Lambda functions with external applications.
3. AWS Step Functions
     - Use Case: Orchestrating serverless workflows with Lambda, ECS, and DynamoDB.
     - Where Used: Automating deployment pipelines, handling retry logic for fault tolerance.
4. Amazon DynamoDB (Serverless NoSQL DB)
     - Use Case: Storing application state, handling high-scale transactions.
     - Where Used: Storing telemetry/log data for SRE monitoring dashboards.
5. Amazon S3 (Event-Driven Serverless Storage)
     - Use Case: Hosting static websites, triggering events when objects are uploaded.
     - Where Used: Log storage, auto-triggering data pipelines with S3 events.
6. Amazon EventBridge & SNS/SQS
     - Use Case: Asynchronous event-driven applications, event-based automation.
     - Where Used: Alerting DevOps teams based on cloud events (e.g., security policy changes).
7. AWS Fargate (Serverless Containers)
     - Use Case: Running containers without managing EC2 instances.
     - Where Used: Deploying stateless microservices in EKS or ECS with auto-scaling.

1. Azure Functions
     - Use Case: Event-driven automation, scheduled tasks, real-time processing.
     - Where Used: Automating DevOps workflows (e.g., scaling Azure resources on demand).
2. Azure Logic Apps
     - Use Case: Low-code serverless workflow automation.
     - Where Used: Automating security compliance checks across Azure resources.
3. Azure API Management (APIM)
     - Use Case: Managing APIs without infrastructure.
     - Where Used: Gateway for microservices, exposing internal services securely.
4. Azure Event Grid & Service Bus
     - Use Case: Event-driven messaging and integration between applications.
     - Where Used: Automating infrastructure monitoring and alerting in SRE.
5. Azure Container Apps
     - Use Case: Running microservices and event-driven apps without managing Kubernetes.
     - Where Used: Deploying lightweight serverless APIs for internal tools.
6. Azure Cosmos DB (Serverless Mode)
     - Use Case: Storing low-latency, globally distributed data.
     - Where Used: Real-time event storage for monitoring logs and alerts.
7. Azure Durable Functions
     - Use Case: Running long-running workflows and stateful functions.
     - Where Used: Coordinating CI/CD workflows with event-driven triggers.

#### Serverless - Use Cases

1. **CI/CD Pipelines**: Use AWS Lambda/Azure Functions to trigger deployments automatically on code commits.
2. **Auto-Healing & Incident Response**: Serverless functions can restart failing services, detect outages, and notify on-call teams.
3. **Log Processing & Monitoring**: Use AWS Lambda/Azure Functions to transform and analyze logs from CloudWatch or Log Analytics.
4. **Event-Driven Infrastructure Automation**: EventBridge/Event Grid can trigger auto-scaling or compliance enforcement.
5. **Serverless Microservices**: APIs deployed on API Gateway & Fargate/Azure Container Apps for scalable, cost-efficient apps.

#### SonarQube
- it is an open-source static code analysis tool that helps identify bugs, vulnerabilities, code smells, security issues, technical debt in code. It integrates with CI/CD pipelines to enforce code quality standards before merging or deploying code.
- How SonarQube is Used in a CI/CD Pipeline
     - Prevention of Code Quality Issues → Before merging code, SonarQube scans the repository and enforces coding standards.
     - Automated Code Review → Finds bugs, vulnerabilities, and security hotspots early in development.
     - Security Compliance → Helps ensure compliance with OWASP, SANS, and other security standards.
     - Improves Maintainability → Identifies duplicated code, unused methods, and bad coding practices.

#### SonarQube CI/CD Integration Workflow** 
- A CI/CD pipeline with SonarQube typically follows this workflow:
     - Developer pushes code to GitHub/GitLab/Azure Repos.
     - CI pipeline triggers a SonarQube scan after compilation (e.g., in Jenkins, GitHub Actions, GitLab CI/CD, or Azure DevOps).
     - SonarQube analyzes the code for bugs, vulnerabilities, and code smells.
     - Quality Gate determines pass/fail criteria based on predefined rules.
     - If code fails quality checks, the build fails, and developers must fix issues before merging.
     - If the code passes, it moves to the next deployment stage.
     - Integrating SonarQube in Jenkins CI/CD Pipeline
          - SonarQube Server: Configured in Jenkins under "Manage Jenkins" → "Configure System" → SonarQube Servers.
          - Sonar Scanner: Installed on the Jenkins agent to perform the analysis.
          - Maven Plugin: Executes sonar:sonar during the build process 

#### SonarQube Fixing Issues Found
     - Security Vulnerabilities → Review OWASP/SANS recommendations and refactor the code accordingly.
     - Code Smells → Refactor complex or duplicated code to improve maintainability.
     - Bugs & Errors → Fix logic errors based on SonarQube reports.
     - Quality Gate Failures → Update rules or exclude false positives where necessary.
     - Technical Debt → Optimize code for better efficiency and performance.

#### SonarQube Use Cases
     - CI/CD Code Quality Enforcement → Blocks bad code from getting deployed.
     - Security & Compliance Scanning → Enforces OWASP Top 10 and SANS 25 security rules.
     - Infrastructure as Code (IaC) Scanning → Scans Terraform, Ansible, Kubernetes YAML, and Dockerfiles.
     - Microservices Code Quality Analysis → Ensures consistency across multiple repositories.
     - Technical Debt Tracking → Identifies areas that need refactoring before they cause issues.

#### SonarQube in a Cloud-Based CI/CD Pipeline
- Developer team is developing a Java-based microservices application deployed on Azure Kubernetes Service (AKS). we want to ensure that every code commit is scanned for bugs, vulnerabilities, and security issues before deployment.
- Tools Used:
     - Version Control: GitHub
     - CI/CD Pipeline: Azure DevOps
     - Code Quality Analysis: SonarQube
     - Build Tool: Maven
     - Deployment: Azure Kubernetes Service (AKS)
- SonarQube Use Cases
     - Prevents Deployment of Bad Code – Blocks PRs with security issues.
     - Ensures Compliance – Enforces security policies (OWASP, PCI-DSS).
     - Automated Infrastructure Scanning – Scans Terraform, Ansible, and Kubernetes YAML files.
     - Code Security Monitoring – Detects and alerts on insecure code before production.
     - Reduces Technical Debt – Helps teams refactor inefficient or outdated code.

#### SonarQube → Code Quality & Static Code Security
- Best for:
     - Detecting code smells, bugs, and vulnerabilities in source code
     - Measuring code quality and maintainability
     - Enforcing coding standards and best practices
- Use Case Example:
     - You are writing a Java Spring Boot application and want to ensure the code follows best practices and is free from security flaws like SQL injection, XSS, etc.
     - Integrated into CI/CD (Azure DevOps, Jenkins, GitHub Actions) to fail builds if code issues exist.

#### Trivy 
- it is an open-source security scanner detects vulnerabilities, misconfigurations, secrets, we do this security scanning checks belore we go for an deployment
     - Container Image Scanning before pushing to Docker Hub, ECR, ACR
     - Infrastructure as Code (IaC) Security scanning for Terraform and Kubernetes YAML files
     - Runtime Kubernetes Security to detect vulnerabilities in running clusters
     - Secret Scanning for hardcoded credentials in Git repositories
- example :
     - Vulnerability: Outdated OpenSSL in Docker Image
     - Vulnerability: Outdated Log4j in a Java Application

#### Trivy → Container & Infrastructure Security
- Best for:
     - Scanning Docker images for CVEs before pushing to a registry
     - Checking Terraform and Kubernetes manifests for security misconfigurations
     - Kubernetes runtime security (scanning running workloads for vulnerabilities)
- Use Case Example:
     - You are deploying an NGINX container to Kubernetes, and you want to check for outdated libraries in the image before deployment.
     - You are scanning Terraform files for security misconfigurations (e.g., public S3 buckets, weak IAM policies).

#### Snyk 
- it is a developer-first security tool used to identify and fix vulnerabilities in:
     - Open-source dependencies (NPM, Maven, PyPI, etc.)
     - Container images (Docker, Kubernetes)
     - Infrastructure as Code (IaC) (Terraform, Kubernetes YAML)
     - Code security (SAST - Static Application Security Testing)
- Snyk integrates with CI/CD pipelines, Git repositories, and Kubernetes clusters to provide real-time vulnerability scanning and automated remediation.

- We can integrate Snyk into our CI/CD pipelines, Kubernetes security, and infrastructure security workflows.
- Key Use Cases:
     - **Dependency Scanning** - Detect vulnerabilities in Maven, NPM, Python, etc.
     - **Container Image Security** - Scan Docker images before deployment
     - **Infrastructure as Code (IaC) Scanning** - Detect misconfigurations in Terraform and Kubernetes
     - **Runtime Security** - Scan running workloads in EKS, AKS, GKE, or Kubernetes clusters
- example :
     - Vulnerability: Outdated OpenSSL in Docker Image
     - Vulnerability: Outdated Log4j in a Java Application

#### Snyk → Dependency, Container & Infrastructure Security
- Best for:
     - Scanning open-source dependencies (Maven, NPM, Python, etc.)
     - Fixing vulnerabilities by suggesting patches & updates
     - Scanning Docker images, Kubernetes YAMLs, and Terraform files
     - Runtime security monitoring for Kubernetes clusters
- Use Case Example:
     - Your application uses Node.js dependencies from NPM, and you want to check for vulnerabilities in third-party libraries.
     - You need automatic PRs with dependency updates (e.g., upgrading lodash due to a CVE).
     - You want to scan Docker images for vulnerabilities before deployment.

#### Vault HashiCorp
its a secrets management and data protection tool designed to securely store, manage, and control access to secrets (such as API keys, passwords, certificates, and encryption keys). It helps organizations enforce least privilege access and audit trails for secret usage.
- **Key Features**
     - **Secrets Management** – Store and retrieve secrets securely.
     - **Dynamic Secrets** – Generate temporary credentials for databases and cloud platforms.
     - **Access Control (RBAC & Policies)** – Fine-grained access control using policies.
     - **Encryption as a Service** – Encrypt/decrypt data without exposing encryption keys.
     - **Audit Logging** – Logs all actions to track access history.
     - **Secret Revocation** – Auto-expiry and revocation of secrets.
- **Key Takeaways**
     - HashiCorp Vault is a secure secrets management tool.
     - It provides static & dynamic secrets, encryption, and access control.
     - It is available as open-source (free), enterprise (paid), and HCP Vault (managed service).
     - It integrates with Terraform, Kubernetes, AWS, Azure, and more.
     - Reduces security risks by managing secret rotation and limiting access.   

#### EC2 - Elastic Compute Cloud
- Amazon EC2, AWS’s flagship compute service. Learn how to provision virtual servers, choose the right instance types, and optimize performance for your applications.
- EC2 instances are categorized based on their use cases:
     - General Purpose: Balanced compute, memory, and networking (e.g., t3, m5). - **Use case:** Web servers, microservices, small databases.
     - Compute Optimized: High-performance CPUs (c5, c6g). **Use case:** High-performance computing, batch processing, gaming servers.
     - Memory Optimized: Large RAM (r5, x1e). **Use case:** In-memory databases (Redis, SAP HANA), caching.
     - Storage Optimized: High disk throughput (i3, d2). **Use case:** Big data, NoSQL databases.
     - Accelerated Computing: GPUs and FPGAs (p4, g5).  **Use case:** Machine learning, AI, video encoding.
- To choose the right instance:
     - Analyze CPU, memory, and network requirements.
     - Use AWS Compute Optimizer to recommend instance types.
     - Use Spot Instances for cost savings on fault-tolerant workloads
       
#### EC2 Instance Requirements:
1. AWS Account with IAM permissions.
2. we need to choose AMI (Amazon Machine Image).
3. need to Select EC2 Instance Type based on workload needs.
4. Configure Instance Details, such as VPC, Subnet, and IAM Role (if required).
5. Add Storage, such as an EBS volume.
6. Configure Security Groups to allow necessary traffic (e.g., SSH, RDP).
7. Key Pair for accessing the instance via SSH (Linux) or RDP (Windows).
8. Launch the instance and access it via SSH or RDP.

#### AMI - Amazon Machine Image (AMI) 
its is a pre-configured template that contains all the necessary information to launch an Amazon EC2 instance. It includes the operating system, application software, configuration settings, and required permissions.
- Key Components of an AMI
     1. Root Volume
          - The primary disk of the instance (e.g., Amazon EBS or Instance Store).
          - Contains the operating system and system files.
     2. Launch Permissions
          - Defines who can use the AMI (private, shared, or public).
     3. Block Device Mappings
          - Specifies additional storage volumes attached to the instance.
     4. Metadata and Permissions
          - IAM roles, kernel ID, and virtualization type (HVM or PV).

#### EC2 Security
- use cases
     - Use IAM Roles Instead of Root Credentials: - Assign least privilege permissions using IAM roles.
     - Enable Security Groups & NACLs: - Restrict SSH (only allow specific IPs) & Use least-privilege access for inbound/outbound rules.
     - Encrypt Data at Rest & In Transit: - Use EBS encryption and TLS/SSL for secure communication.
     - Enable Multi-Factor Authentication (MFA) for SSH: - Use AWS Systems Manager Session Manager instead of SSH for access.
     - Monitor & Audit Instances: - Use AWS CloudTrail and GuardDuty for security monitoring.
     - Regularly Patch & Update Instances: Use Amazon Systems Manager Patch Manager for automatic updates.

#### RBAC - role base access control 
- its a security model that restricts system access based on a user's role within an organization.
- It ensures least privilege access, users only have the permissions necessary to perform their job functions.
     - Security: Prevents unauthorized access to critical infrastructure.
     - Compliance: Helps meet regulatory requirements (SOC2, ISO 27001, HIPAA).
     - Operational Efficiency: Reduces manual permission management and human errors.
     - Audit & Monitoring: Tracks who accessed what resources and when.
- Components of RBAC
     - Roles: Define a set of permissions (e.g., Developer, Admin, Read-Only).
     - Permissions: Actions allowed for a role (e.g., read, write, delete).
     - Users/Groups: Users are assigned roles based on their responsibilities.
     - Resources: The services and infrastructure components that need access control.
- Use IAM when managing cloud users, federated access, and API security.
- Use RBAC when defining access levels inside an application, Kubernetes cluster, or CI/CD pipeline.

#### Simple Storage Service (S3)
- S3 as a huge online storage locker where you can store and retrieve files anytime, from anywhere
     - Buckets = Folders 📂 → You create a bucket to store your files.
     - Objects = Files 📝 → Each file (image, video, document, backup) is an object inside a bucket.
     - Keys = File Names 🔑 → Every object has a unique name (key) in the bucket.

#### IAM - Identity and Access Management
- IAM as a security guard for your AWS account. It controls who can access what in AWS.
     - **Users** – Individual people (e.g., DevOps engineers, SREs).
     - **Groups** – A set of users with the same permissions (e.g., Admins, Developers).
     - **Roles** – Temporary access for AWS services (e.g., EC2 accessing S3).
     - **Policies** – Rules that define what actions are allowed (e.g., read-only access to S3).
- usecases	
     - **Secure Access** – Only authorized people/services can use AWS.
     - **Least Privilege** – Users get only the permissions they need, nothing extra.
     - **Multi-Factor Authentication (MFA)** – Adds an extra layer of security.
     - **Audit & Monitoring** – Tracks who did what in AWS.

#### IAM Role
Common Use Cases:
- An IAM (Identity and Access Management) Role is a set of permissions assigned to AWS services or users without requiring explicit credentials (like API keys). It follows the least privilege principle.
     - **EC2 Role** : Allows EC2 instances to access S3, DynamoDB, etc.
     - **Lambda Role** : Grants Lambda permission to interact with other AWS services.
     - **EKS Role** : Manages Kubernetes cluster permissions.
     - **Cross-Account Role** : Enables access between AWS accounts.

#### IAM vs RBAC – Key Differences

|Feature|IAM|RBAC|
|- |-|-|
|Scope|System-wide access (users/services/resources)|Resource or namespace-level access|
|Entity|User, group, service|Role|
|Assignment|Policies are attached directly to identities|Roles assigned to users or groups|
|Granularity|Fine-grained, supports conditionals|Role-based, coarser by default|
|Example Systems|AWS IAM, Azure IAM, GCP IAM|Kubernetes, Linux ACLs, Postgres roles|
|Used For|Cloud resource access control|Internal access to specific components|

#### Virtual Private Cloud (VPC)
- its a logically isolated network in AWS, where you deploy your resources like EC2, RDS, and Lambda. It controls networking such as IP addressing, subnets, routing, and security.
     - **Custom IP Range**: Uses CIDR blocks (e.g., 10.0.0.0/16).
     - **Subnets**: Divide the VPC into smaller sections.
     - **Internet Gateway** (IGW): Enables internet access.
     - **NAT Gateway**: Allows outbound internet access for private instances.
     - **Security Controls**: Uses Security Groups and Network ACLs for traffic filtering
     - **Route Tables** – Control traffic routing within the VPC.
     - **Security Groups** – Act as virtual firewalls for instances.
     - **Network ACLs** – Control traffic at the subnet level.
     - **VPC Peering** – Connects multiple VPCs for secure communication.
     - **VPN & Direct Connect** – Secure on-premises connectivity.
- Common Use Cases:
     - Hosting secure workloads (databases, applications).
     - Multi-tier architecture (public/private subnet separation).
     - Hybrid cloud setups (linking on-premises networks).
     - Isolated development and testing environments.

#### VPC Peering
its a networking connection between two Amazon Virtual Private Clouds (VPCs) that allows them to communicate privately as if they were on the same network.
- Key Features:
     - Direct communication between VPCs using private IPs
     - No bandwidth limitations (unlike VPNs)
     - Low latency since traffic stays within AWS' network
     - Secure (No internet exposure, works over AWS backbone)
     - Cross-Region & Cross-Account Peering Supported

#### VPN (Virtual Private Network)
- its a secure connection that encrypts data and allows users or systems to communicate privately over public or shared networks
- Types of VPNs:
     - **Site-to-Site VPN** – Connects entire networks (e.g., on-premises to cloud).
     - **Remote Access VPN** – Allows individual users to securely connect to a private network.
     - **Client VPN** – Secure access for employees working remotely.
- Key Features:
     - **Encryption** – Secures data transmission over the internet.
     - **Authentication** – Ensures only authorized users access the network.
     - **Tunneling Protocols** – Common ones include:
          - **IPSec** – Used for Site-to-Site VPNs.
          - **OpenVPN** – Secure, open-source VPN solution.
          - **WireGuard** – Fast and modern VPN protocol.
          - **SSL/TLS VPN** – Common for remote access VPNs.
               - No IP Leaks – Prevents exposure of private IP addresses.
               - Kill Switch – Disconnects the internet if the VPN fails to prevent data leaks.
- VPN in Cloud (AWS, Azure)
     - AWS VPN: Uses AWS Site-to-Site VPN and Client VPN.
     - Azure VPN Gateway: Connects on-premises to Azure securely.

#### Route Table
- AWS controls how traffic is directed within a VPC and to external networks (internet, another VPC, on-premises networks).
- Each subnet in a VPC must be associated with a route table, and a single route table can be shared by multiple subnets.
- it consists of Routes that define destination CIDR blocks and their corresponding targets

#### Subnet
- its a smaller section of a VPC that divides the network into manageable parts. Each subnet has specific Availability Zone (AZ) and helps control how resources communicate inside AWS.
     - **Public Subnet** – it has internet access (used for web servers).
     - **Private Subnet** – it does not have direct internet access (used for databases, backend services).

- A Subnet (Sub-network) is a smaller network segment inside a VPC, used to separate resources and control traffic flow.
- Types of Subnets:
     - **Public Subnet**: Connected to the internet via an Internet Gateway (IGW).
     - **Private Subnet**: No direct internet access; typically uses a NAT Gateway.
     - **Isolated Subnet**: No internet access, used for security-sensitive workloads

#### Security Groups
- its firewall for EC2 instances and other AWS resources. It controls what traffic is allowed in and out based on rules for inbound and outbound traffic.
     - **Inbound Rules** – Define what traffic can enter (HTTP (80), HTTPS (443), SSH (22)
     - **Outbound Rules** – Define what traffic can leave (by default, all traffic is allowed).

#### Internet Gateway
- it allows resources in a public subnet to connect to the internet and receive incoming traffic. It acts as a bridge between the VPC and the public internet.
     - IGW is attached to a VPC to enable internet communication.
     - Public subnets must have a route in the route table pointing to the IGW.
     - EC2 instances in a public subnet need a public IP or Elastic IP to use the IGW

#### NAT gateway
- allows private subnets to access the internet securely without exposing them to incoming traffic.
     - The private subnet sends traffic to the NAT Gateway.
     - The NAT Gateway forwards the request to the internet via an Internet Gateway (IGW).
     - The response returns through the NAT Gateway to the private subnet.

 #### Security Controls
 - it protects cloud resources from unauthorized access, data breaches, and cyber threats.
      - IAM (Identity & Access Management): Controls who can access what.
      - Security Groups & NACLs: Firewalls that allow/deny traffic.
      - Encryption (S3, EBS, RDS): Protects data from unauthorized access.

#### NACL - Network Access Control List
- Its a firewall for controlling inbound and outbound traffic at the subnet level in AWS. It allows or denies traffic based on defined rules
     -  Works at the subnet level (unlike Security Groups, which work at the instance level).
     -  Stateless – Each request (inbound and outbound) is evaluated separately.
     -  Rules are checked in order, starting from the lowest rule number (e.g., Rule #100 is checked before Rule #200).
     -  The default NACL allows all traffic, but a custom NACL denies all traffic by default.

#### CIDR Block
- **Classless Inter-Domain Routing** - its a method for allocating IP addresses and managing subnets efficiently. It represents an IP range using a prefix notation

```<IP Address>/<Subnet Mask Bits>
192.168.1.0/24 → Includes 256 IPs (192.168.1.0 - 192.168.1.255)
10.0.0.0/16 → Includes 65,536 IPs (10.0.0.0 - 10.0.255.255)
```
It is used in AWS VPCs, Kubernetes clusters, VPNs, and firewalls.

- **CIDR work in AWS VPCs**
     - AWS VPCs require CIDR blocks to define private networks.
     - Default AWS VPC CIDR: 172.31.0.0/16 (65,536 IPs).
     - Custom VPC example:
```
10.0.0.0/16 for the whole VPC
10.0.1.0/24 for public subnet
10.0.2.0/24 for private subnet
```
- IPs are available in a subnet?
     - AWS reserves 5 IPs per subnet (first 4 + last).
     - Formula: 2ⁿ - 5 (where n is the number of host bits).
|Subnet|	Total IPs|	Usable IPs in AWS|
|- | -| -|
|/16	|65,536|	65,531|
|/24	|256	|251|
|/28	|16	|11|
- you cannot change the CIDR of an existing VPC
     - AWS does not allow changing the primary VPC CIDR.
     - You can add a secondary CIDR block but cannot shrink or modify an existing one.

**(Classless Inter-Domain Routing)** is a method for allocating IP addresses and routing IP packets. A CIDR block represents a range of IP addresses and is used to define the size of subnets within a network.
 - **Basic Understanding of CIDR Notation:**
   - its differs from traditional class-based IP addressing (Class A, B, C).
   - Understand how the subnet prefix (/24, /16, etc.) defines the number of IP addresses available in the network and how it impacts routing.
 - **Subnetting and Network Addressing:**
   - Ability to break down and subnet IP ranges based on given requirements.
   - Knowing how to determine usable IP ranges, network address, and broadcast address from a given CIDR block. 
 - **CIDR in Cloud & Containers:**
   -  Understanding how CIDR blocks are used for defining VPCs, subnets, and network isolation in cloud environments (AWS, GCP, Azure).
   - Knowing how CIDR is applied in Kubernetes, Docker, or ECS for pod/container IP assignment.
 - **IP Addressing Efficiency:**
   - Being able to explain the trade-off between larger and smaller CIDR blocks in terms of IP efficiency and wasted IPs (e.g., when to use /24 vs. /30).
 - **Routing with CIDR:**
   - Understanding how CIDR affects routing tables and how a router uses CIDR blocks to forward packets to the correct subnet.
   - Be able to explain the concept of supernetting or aggregating multiple smaller CIDR blocks into one larger block.

#### Elastic Load Balancing - ELB
- its an AWS service that automatically distributes incoming traffic across multiple servers, which can ensure high availability and reliability of applications.
     - Distributes traffic to prevent server overload.
     - Improves availability by redirecting traffic if an instance fails.
     - Enhances security by integrating with AWS services like SSL/TLS encryption.

#### Auto Scaling
- its is the process of dynamically adjusting the number of compute resources (such as virtual machines, containers, or instances) based on real-time demand.
- It ensures to see that applications remain highly available, cost-efficient, and performant by automatically scaling out (adding instances) when load increases and scaling in (removing instances) when demand drops.
- **Use cases Auto Scaling**: Combine with AWS Auto Scaling Groups to handle variable loads.
     - **Monitor Load Balancer Health:** Use Prometheus, Grafana, AWS CloudWatch, or Datadog.
     - **Enable SSL Termination:** Offload SSL/TLS processing to reduce backend load.
     - **Optimize for Latency:** Use geo-load balancing (e.g., AWS Route 53, GCP Traffic Director).
     - **Implement Failover Strategies:** Ensure traffic is redirected in case of failures.
- **Auto Scaling Group**
     - ASGs automatically scale in (remove instances) or scale out (add instances) based on predefined policies, ensuring that workloads handle traffic efficiently without manual intervention.
- **Auto Scaling in Different DevOps Environments**
1. Auto Scaling in AWS
     - **EC2 Auto Scaling** – Automatically scales EC2 instances using Auto Scaling Groups (ASG).
     - **AWS Fargate Auto Scaling** – Manages containerized workloads with ECS and Kubernetes (EKS).
     - **DynamoDB Auto Scaling** – Adjusts read/write capacity based on usage.
     - **Lambda Auto Scaling** – Scales function execution based on event triggers.
2. Auto Scaling in Kubernetes (K8s)
     - **Horizontal Pod Autoscaler** (HPA) – Adjusts the number of running pods based on CPU/memory usage.
     - **Vertical Pod Autoscaler** (VPA) – Adjusts resource requests for existing pods.
     - **Cluster Autoscaler** – Dynamically adjusts node count in a Kubernetes cluster.
3. Auto Scaling in Azure
     - **Azure Virtual Machine Scale Sets** (VMSS) – Manages scaling of VM instances.
     - **Azure Kubernetes Service** (AKS) Autoscaler – Adjusts cluster size dynamically.
     - **Azure App Service Auto Scaling** – Scales web applications based on demand.
- Rules
- In AWS, I use EC2 Auto Scaling Groups (ASG) with CloudWatch alarms to dynamically add or remove instances based on CPU and request metrics. I configure scheduled scaling for predictable traffic spikes and enable predictive scaling for ML-based forecasting.
     - Scale Out (Add VMs) - If CPU usage > 75% for 5 minutes, increase instances by 1
     - Scale In (Remove VMs) - If CPU usage < 30% for 10 minutes, remove 1 instance.
     - Scheduled Scaling - Increase instances to 5 at 8 AM (business hours) and reduce to 2 at 8 PM 
- In Azure, I configure Virtual Machine Scale Sets (VMSS) or Kubernetes-based Horizontal Pod Autoscaling (HPA) to manage workloads. I use Azure Monitor to define scaling rules based on CPU, memory, and request thresholds, ensuring high availability and cost efficiency.
     - Scale Out (Add VMs) - If CPU usage > 75% for 5 minutes, increase instances by 1
     - Scale In (Remove VMs) - If CPU usage < 25% for 10 minutes, remove 1 instance.
     - Scheduled Scaling - Scale up to 10 instances at 9 AM and scale down to 3 at 6 PM 

#### Route 53 
- its a highly available and scalable DNS (Domain Name System) service that helps route traffic to websites, applications, and AWS resources.
     - **Domain Registration** – Register and manage domain names.
     - **DNS Resolution** – Translates domain names (e.g., example.com) into IP addresses.
     - **Traffic Routing** – Routes users to AWS services like EC2, S3, and CloudFront.
     - **Health Checks** – Monitors the health of resources and reroutes traffic if needed.

#### DNS - Domain Name System
- domain names (e.g., www example com) into IP addresses (e.g., 192.168.1.1), which computers use to communicate.
     - User types www example com in a browser.
     - The browser asks the DNS server for the website’s IP address.
     - The DNS server finds the IP (192.168.1.1) and returns it.
     - The browser connects to the website using the IP address.

#### CNAME - Canonical Name record
-  its related to DNS system, which is used to map one domain name to another. It helps in redirecting traffic from an alias domain to the actual domain
     - Instead of pointing directly to an IP address, a CNAME points one domain to another domain name.
     - When users visit the alias domain, they are automatically directed to the target domain.

#### Lambda Functions
- its a serverless computing service that lets you run code without managing servers. It automatically scales, and you only pay for the execution time.
     - Upload your code (supports Python, Node.js, Java, etc.).
     - Set a trigger (e.g., an S3 upload, API Gateway request, or CloudWatch event).
     - Lambda executes the function automatically.
     - **Key Benefits**
          - **No servers to manage** – Fully managed by AWS.
          - **Auto-scaling** – Runs only when triggered.
          - **Cost-efficient** – You pay only for execution time, not idle time.
- use case
     - **You need to automatically start or stop EC2 instances based on usage**.
          - Triggered by CloudWatch Scheduled Events.
          - Checks instance utilization using AWS SDK (Boto3 in Python).
          - Starts or stops instances dynamically to optimize cost.
     - **You want to build a Slack chatbot that responds to messages**.
          - API Gateway receives Slack messages.
          - Lambda processes the input and generates a response.
          - Sends back the response to Slack using SNS or another API.

#### Elastic Beanstalk
- it helps in deploy and manage web applications easily without worring of infrastructure. It automatically handles deployment, scaling, and monitoring.
     - Upload Your Code (supports Java, Python, Node.js, .NET, etc.).
     - Elastic Beanstalk automatically sets up servers, networking, and databases.
     - It scales the application up or down based on demand.
     - It monitors performance and provides health metrics.
- use case
     - If a company wants to deploy a web application quickly without managing EC2, load balancers, or databases, Elastic Beanstalk automatically sets everything up, including scaling and monitoring. 

#### DynamoDB
- its a fully managed NoSQL database service. It is designed for high performance, scalability, and low-latency workloads.
- DynamoDB automatically scales based on demand and supports key-value and document data models.
- Key Features:
     - **Serverless & Fully Managed** – No need to manage servers, backups, or scaling.
     - **High Availability** – Multi-AZ replication for resilience.
     - **Automatic Scaling** – Scales up/down based on workload.
     - **Low Latency** – Single-digit millisecond response times.
     - **Encryption & Security** – Integrated with AWS IAM, KMS, and VPC for secure access.
     - **Streams & Event-Driven** – Supports DynamoDB Streams for real-time processing.

#### CloudFormation
- CloudFormation is AWS’s Infrastructure as Code service. I’ve used it extensively to provision and manage environments, including VPCs, EC2 instances, RDS databases, IAM roles, and more. I’ve written modular templates using nested stacks and parameters to promote reusability. We integrated it into our CI/CD pipeline to automate environment provisioning for dev, staging, and production
- Focus Areas: Modularity, maintainability, and automation.
     - Use nested stacks to break up large templates.
     - Store templates in version control (Git).
     - Use parameters, conditions, and mappings for flexibility.
     - Maintain naming conventions and documentation.
     - Automate validation with cfn-lint or tools like Taskcat.
- **UpdateStack** - it applies the changes directly to the stack, which can be risky without preview.
- **CreateChangeSet** - it allows me to preview what changes will occur, such as resource replacement or deletion, before executing. In production, I always use Change Sets to avoid unintended downtime or destructive changes.
- **drift detection** - it a feature in CloudFormation to detect if resources were modified outside the stack. For critical stacks, we run drift detection regularly via scheduled jobs, and alert the team on any detected changes. This ensures infrastructure stays declarative and in sync with code.
- **Secrets** : I avoid hardcoding secrets in templates. Instead, I use AWS Systems Manager Parameter Store (with SecureString) or AWS Secrets Manager, and reference them securely in CloudFormation using dynamic references. I also control access using IAM policies.
- **roll back** : CloudFormation automatically attempts rollback on failure unless DisableRollback is set to true. I always test stacks in lower environments and use Change Sets to reduce failure risk. For critical environments, I take snapshots or backups before updates to ensure full recovery options.
- **integrate with CI/CD** : We use a pipeline (e.g., GitHub Actions, CodePipeline, Jenkins) to lint, validate, and deploy CloudFormation templates. We run cfn-lint, test in dev with ChangeSets, and require approval before production deploys. We use parameter overrides and tagging to manage environments and ownership.
- **Versioning** : All templates are stored in Git. We use tagged releases or branches for versioning. If something fails in production, we can easily redeploy a previous template version. Combined with Change Sets and backups, this provides a reliable rollback strategy.
- its an Infrastructure as Code (IaC) service that lets you define and deploy AWS resources using templates. It automates resource creation, ensuring consistency and repeatability.
     - **Write a Template** – Define AWS resources (EC2, S3, IAM, etc.) in YAML or JSON.
     - **Deploy as a Stack** – CloudFormation creates and manages the resources.
     - **Automate Updates** – Modify the template to update infrastructure safely.
- Instead of manually creating EC2 instances and S3 buckets, you write a CloudFormation template. It provisions everything automatically in a repeatable way.
- **use cases**
     - **when team wants to implement CI/CD but avoid manually setting up Jenkins and deployment pipelines.**
          - Defines CodeCommit, CodeBuild, CodeDeploy, and CodePipeline in a single template.
          - Enables automated updates to infrastructure with version control.
          - Reduces risk by ensuring infrastructure matches approved templates
     - **when application gets unpredictable traffic spikes, and you want auto-scaling**
          - Automatically deploys Auto Scaling Groups and Load Balancers.
          - Ensures traffic is distributed efficiently with an Application Load Balancer (ALB).
          - Uses Scaling Policies to handle demand without manual intervention.
     - **when need to deploy a highly available web application with an EC2 instance, RDS database, and S3 bucket for storage**
          - Automates the setup of EC2, Load Balancer, RDS, and S3.\
          - Ensures resources are deployed consistently across environments (Dev, QA, Prod).
          - Uses Parameters to customize configurations dynamically.
- Reuse - I use parameterized templates with environment-specific values passed during deployment. Mappings and Conditions help keep the template dynamic, and nested stacks ensure modularity. The pipeline uses different parameter files or SSM values per stage.

#### CloudFront
its is a Content Delivery Network (CDN) service that securely delivers data, videos, applications, and APIs to users globally with low latency and high transfer speeds.
- Key Features of CloudFront
     - **Global Content Delivery** – Uses AWS’s worldwide network of edge locations for fast content distribution.
     - **Caching** – Stores content at edge locations to reduce load on origin servers.
     - **Security & DDoS Protection** – Integrates with AWS Shield, AWS WAF, and SSL/TLS for secure content delivery.
     - **Origin Options** – Can pull content from S3, EC2, Load Balancer, API Gateway, or custom origins.
     - **Lambda@Edge & CloudFront Functions** – Runs lightweight code at the edge for request/response modifications.
     - **Real-time Logging & Analytics** – Monitors performance with CloudWatch and S3 logging.
- How CloudFront Works
     - **User Requests Content** → User requests a file via a URL (e.g., https://cdnexamplecom/image.jpg).
     - **CloudFront Edge Location** → Checks if the file is cached in the nearest edge location.
          - If Cached → Serves content instantly.
          - If Not Cached → Retrieves it from the origin (S3, EC2, or another server).
          - Caches & Delivers → Stores the content for future requests and sends it to the user.
- Use Cases
     - **Static Website Hosting** – Faster delivery of HTML, CSS, JS, and images.
     - **Streaming Media** – Efficient video streaming with HLS/DASH support.
     - **API Acceleration** – Improves performance for RESTful APIs.
     - **Software Distribution** – Reduces latency for downloads and updates.
     - **DDoS Mitigation** – Works with AWS Shield for security.

#### CodeCommit
- A fully managed source control service that hosts private Git repositories. Learn how to securely store and version your code, collaborate with team members, and integrate with other AWS services for continuous integration and deployment.
- its a fully managed Git-based source control service that helps teams store and manage their code securely in AWS, just like GitHub or Bitbucket
     - Developers push code to CodeCommit repositories, just like in Git.
     - Code is securely stored in AWS, with high availability and encryption.
     - CodeCommit integrates with CI/CD pipelines (CodeBuild, CodePipeline) for automation.

#### CodePipeline
- its a fully managed CI/CD service that automates software release processes, like building, testing, and deploying applications.
     - Automates deployments – No manual work needed.
     - Faster releases – Code changes go live quickly.
     - Integrates with AWS & third-party tools – Works with GitHub, Jenkins, CloudFormation, etc.
     - CI/CD Process in Steps
          - Source Stage → Gets code from GitHub, CodeCommit, or S3.
          - Build Stage → Compiles code using CodeBuild or Jenkins.
          - Test Stage → Runs automated tests.
          - Deploy Stage → Deploys to EC2, ECS, Lambda, or Kubernetes

#### CodeBuild
- its a fully managed CI/CD service that compiles source code, runs tests, and produces build artifacts without needing to manage servers
     - Pulls source code from GitHub, CodeCommit, or S3.
     - Runs build commands (e.g., compiling Java, Python, or Node.js code).
     - Executes tests to ensure the code is working.
     - Generates artifacts (like a deployable package).
     - Sends results to CodeDeploy, S3, or ECR for deployment.
     - **buildspec.yml**
- Create a CodeBuild Project -  **before we need to create IAM role and we need to add few policies**
     - Open AWS CodeBuild → Click Create Build Project
     - Project Name: MyFirstBuildProject
     - Source: Choose CodeCommit, GitHub or S3
     - Environment:
          - Runtime: Choose Amazon Linux or a custom Docker image
          - Privileged Mode: Enable (if building Docker images)
               - Buildspec File: Use **buildspec.yml** from the repository or enter manually
               - Artifacts: Store build output in S3
               - Service Role: Choose the **IAM role** created earlier
               - Click Create Build Project

#### CodeDeploy
- its a deployment service that automates software updates to EC2, on-premises servers, Lambda functions, and ECS. It helps ensure zero-downtime and rollback capabilities.
     - Automates deployments – No manual intervention needed.
     - Supports rolling updates – No downtime while deploying.
     - Rollback on failure – Automatically reverts to the last successful version.
     - Works with EC2, Lambda, and ECS – Flexible across different environments
- Deployment Types:
     - **In-Place Deployment** (for EC2 & on-premises) – Stops the app, updates it, and restarts.
     - **Blue/Green Deployment** ( EC2, ECS, Lambda) – Creates a new environment, tests, then switches
- if wants to deploy a new version of their web app to EC2 instances without downtime. CodeDeploy automates the process, rolling back if an issue is detected.
- CodeDeploy uses CloudWatch Logs and SNS notifications for monitoring

#### use cases : CodeCommit, CodePipeline, CodeBuild, CodeDeploy
- use case - **if developer are developing some application where it requires deployment across multiple environments before reaching production**.
     - How AWS Services Are Used:
          - CodeCommit – Developers push code updates.
          - CodePipeline – Triggers CI/CD pipeline for each environment.
          - CodeBuild – Builds the application, runs tests, and creates Docker images.
          - CodeDeploy – Deploys the microservice to different ECS clusters (Dev, Staging, Production).
     - Workflow:
          - Developer commits code to CodeCommit (main branch).
          - CodePipeline detects the change and triggers the pipeline.
          - CodeBuild compiles the code and creates a Docker image, pushing it to Amazon ECR.
          - CodeDeploy deploys the image:
               - Step 1: Deploys to Dev ECS Cluster → Runs automated tests.
               - Step 2: Deploys to Staging ECS Cluster → Runs performance tests.
               - Step 3: Deploys to Production ECS Cluster using Blue/Green Deployment.

- Use Case: if a company is hosting a Node.js web application on EC2 instances behind an Application Load Balancer (ALB). They want to:
          - Automate the deployment process to avoid manual errors.
          - Ensure zero downtime using Blue/Green deployment.
          - Run unit tests before deploying.
     - How AWS Services Are Used:
          - CodeCommit – Developers push new code to a Git repository in CodeCommit.
          - CodePipeline – Detects changes in CodeCommit and triggers the pipeline.
          - CodeBuild – Compiles the code, installs dependencies, and runs tests.
          - CodeDeploy – Deploys the application to EC2 instances using Blue/Green deployment.
     - Workflow:
          - Developer commits code to CodeCommit.
          - CodePipeline detects the change and starts the pipeline.
          - CodeBuild runs unit tests and builds an artifact (e.g., ZIP package).
          - CodeDeploy deploys the new version to a new environment (Blue/Green strategy).
          - If deployment is successful, traffic shifts to the new instances. If not, CodeDeploy rolls back automatically.

#### CloudWatch
- its a monitoring and observability service in AWS that helps track system performance, logs, and events. It allows you to set alarms, collect metrics, and monitor applications and infrastructure in real time.
- CloudWatch Key Components
     - **CloudWatch Metrics** – Collects real-time performance data (e.g., CPU, Memory, Disk I/O).
          - services like EC2, RDS, and Lambda send metrics to CloudWatch automatically.
          - To view them:
               - Go to AWS Console > CloudWatch > Metrics
               - Select the relevant AWS service namespace (e.g., AWS/EC2, AWS/Lambda).
               - Choose the metrics you want to monitor.
     - **CloudWatch Logs** – Stores logs from applications, servers, and AWS services.
          - Go to CloudWatch Logs.
          - Create a Metric Filter from logs to track specific patterns.
          - Create alarms based on log-based metrics.
     - **CloudWatch Alarms** – Triggers actions based on metric thresholds (e.g., restart EC2 if CPU > 90%).
          - Go to CloudWatch Console > Alarms.
          - Click Create Alarm.
          - Select a metric and define:
               - Threshold (e.g., CPU utilization > 80%)
               - Evaluation Period (e.g., for 5 minutes)
               - Actions (e.g., send an SNS notification, trigger Auto Scaling)
          - Configure SNS (Simple Notification Service) for email/SMS notifications.
     - **CloudWatch Dashboards** – Visual representation of metrics in graphs.
          - Go to CloudWatch Console → Dashboards.
          - Click Create dashboard and add widgets for the required metrics.
          - Choose visualization types like graphs, number widgets, or text.
     - **CloudWatch Events (EventBridge)** – Automates responses to AWS events (e.g., start/stop EC2 at specific times).
          - Go to AWS Console → CloudWatch → Events → Rules
          - Click "Create Rule"
          - Define an Event Pattern (e.g., EC2 instance state change)
          - Select a Target (Lambda, SNS, SQS, etc.)
          - Create the rule
- CloudWatch stores logs in CloudWatch Log Groups - **/var/log/syslog**

#### SNS
- Simple Notification Service (SNS) is a fully managed pub/sub messaging service used for sending alerts, notifications, and messages across distributed systems. It integrates with Amazon CloudWatch to notify users about system events like high CPU utilization, failed deployments, or security breaches.
- **SNS for Alerts**
     - AWS SNS Console → Topics → Create topic
     - Choose Standard type (FIFO is for ordered messaging).
     - Enter a name (e.g., CloudWatchAlerts).
     - Click Create topic.
- **CloudWatch Alarm to Trigger SNS**
     - CloudWatch Console → Alarms → Create Alarm.
     - Select a metric (e.g., EC2 CPUUtilization).
     - Set a threshold (e.g., trigger when CPU > 80% for 5 minutes).
     - Choose Actions → Send notification to SNS topic.
     - Select CloudWatchAlerts as the SNS topic.
     - Click Create Alarm. 

#### CloudTrail
- its a logging and monitoring service that tracks API calls and account activity across AWS services. It helps in:
     - Security Auditing – Track changes to AWS resources.
     - Compliance Monitoring – Meet compliance requirements (SOC2, HIPAA, GDPR, etc.).
     - Operational Troubleshooting – Investigate changes and incidents.
     - Threat Detection – Identify suspicious API calls and unauthorized access

#### Elastic Container Registry (ECR)
- A fully managed Docker container registry. Learn how to securely store, manage, and deploy container images using ECR, integrate with other AWS services like ECS and EKS, and implement best practices for container security.

#### Elastic Container Service (ECS)
- its managed container orchestration service.
- If a container becomes unhealthy, ECS can restart it, replace it, or reroute traffic to healthy instances.
- **How ECS Uses Health Checks**
     - Container-level health check fails → ECS restarts the container.
     - Target group health check fails → ECS deregisters the task from the load balancer.
     - Task health check fails repeatedly → ECS replaces the task with a new instance.
- **Best Practices for ECS Health Checks**
     - Use both container and ALB health checks for redundancy.
     - Set startPeriod in ECS task definition to avoid false failures on slow startups.
     - Ensure the app exposes a /health endpoint for accurate checks.
     - Use IAM permissions to allow ECS to manage task replacements.

#### Elastic Kubernetes Service (EKS)
- A fully managed Kubernetes service. Learn how to deploy, manage, and scale Kubernetes clusters on AWS, leverage features like managed node groups and automatic updates, and run containerized applications with ease.

#### S3 Glacier
- We explore Amazon S3 Glacier, a low-cost storage service designed for long-term data archiving and backup. Learn how to archive data securely, manage retrieval times, and implement lifecycle policies to automate data retention.

#### Block Storage
- Which provides scalable and high-performance storage volumes for EC2 instances. Learn how to create and manage block storage volumes, attach them to EC2 instances, and optimize performance for your applications.

#### Amazon Kinesis
- A platform for streaming data on AWS. Learn how to ingest, process, and analyze real-time data streams at scale, and build applications for use cases such as log and event data analysis, IoT data processing, and more.

#### Amazon Interactive Video Service
- A fully managed live and on-demand video processing service. Learn how to build interactive and immersive video experiences, integrate with other AWS services, and deliver high-quality video content to your viewers.

#### AWS Budgets
- Join us for the final day of our journey as we explore AWS Budgets, a service that helps you set custom cost and usage budgets for your AWS resources. Learn how to monitor your spending, receive alerts when you exceed budget thresholds, and optimize costs for your AWS environment.

#### Multi zone
multi-AZ deployment for high availability
- Deploy EC2 instances in multiple AZs
     - Use an Auto Scaling Group to distribute instances across AZs.
     - Use an Elastic Load Balancer (ELB) to distribute traffic
     - Enable Multi-AZ for databases (e.g., RDS, DynamoDB, EFS)
     - use S3 Cross-Region Replication for disaster recovery

- Deploy a Kubernetes cluster across multiple AZs
     - Deploy an Amazon EKS cluster across multiple AZs
     - Use Multi-AZ Auto Scaling Groups for worker nodes.
     - Deploy Persistent Volumes using Amazon EFS (since EBS is AZ-specific).
     - Configure Kubernetes Load Balancer (ALB/NLB) for multi-AZ traffic routing.

#### SNS
- Simple Notification Service (SNS) is a fully managed pub/sub messaging service used for sending alerts, notifications, and messages across distributed systems. It integrates with Amazon CloudWatch to notify users about system events like high CPU utilization, failed deployments, or security breaches.
- **SNS for Alerts**
     - AWS SNS Console → Topics → Create topic
     - Choose Standard type (FIFO is for ordered messaging).
     - Enter a name (e.g., CloudWatchAlerts).
     - Click Create topic.
- Architecture:
     - Topic: Logical access point and communication channel.
     - Publisher: Sends messages to a topic.
     - Subscriber: Receives messages (can be AWS Lambda, SQS, HTTP/S endpoint, email, SMS, etc.)
- **CloudWatch Alarm to Trigger SNS**
     - CloudWatch Console → Alarms → Create Alarm.
     - Select a metric (e.g., EC2 CPUUtilization).
     - Set a threshold (e.g., trigger when CPU > 80% for 5 minutes).
     - Choose Actions → Send notification to SNS topic.
     - Select CloudWatchAlerts as the SNS topic.
     - Click Create Alarm.

#### SQS (Simple Queue Service)
- Amazon SQS is a fully managed message queuing service that enables decoupling and scaling of microservices, distributed systems, and serverless applications. It allows you to send, store, and receive messages between components in a reliable and asynchronous manner.
- Key Features
     - **Decoupling** -	Separates producers and consumers of messages.
     - **Scalability** -	Scales automatically to handle high throughput.
     - **Message durability** -	Stores messages redundantly across multiple AZs.
     - **Visibility Timeout** -	Prevents multiple consumers from processing the same message.
     - **Dead Letter Queues (DLQ)** -	Isolates messages that can't be processed successfully.
     - **Long polling** -	Reduces cost by waiting for messages to arrive instead of continuous polling.
     - **Message retention** -	Messages can be retained from 1 minute to 14 days.

#### Availability Zone goes down
- If an AZ failure occurs, I will first check the impact using CloudWatch, Azure Monitor, Prometheus to identify affected resources and I ensure traffic is rerouted using ALB, Traffic Manager, DNS failover.
          - Check monitoring tools like CloudWatch (AWS), Azure Monitor, Prometheus, Datadog, or Splunk.
          - Verify affected services (EC2, RDS, EKS, ALB, VMSS, Storage) and dependencies.
          - Use CLI or API to query service status (AWS CLI, Azure CLI).
          - Check cloud provider status page (AWS Health Dashboard, Azure Service Health).
- For stateless applications, I will verify that Auto Scaling launches new instances in another AZ. For stateful services like RDS, EBS, storage, I will check if failover is working and restore from Multi-AZ replicas or snapshots if needed.
     - To prevent future failures, I will implement a multi-AZ architecture, automated failover, and disaster recovery policies to ensure high availability.

#### HAProxy
- HAProxy (High Availability Proxy) is a fast, reliable, and open-source load balancer and proxy server used to distribute network traffic across multiple backend servers.
- It supports Layer 4 (TCP) and Layer 7 (HTTP) load balancing and is widely used for high availability, performance optimization, and security in cloud and on-prem environments.
- Key Features:
     - **Load Balancing** – Distributes traffic across multiple servers.
     - **High Availability** – Ensures failover and redundancy.
     - **SSL Termination** – Handles SSL encryption/decryption.
     - **Reverse Proxy** – Protects backend servers.
     - **Health Checks** – Monitors server status and removes unhealthy nodes.

#### NGINX proxy
- its a high-performance web server, reverse proxy, load balancer, and API gateway. Originally developed for handling high-concurrency applications,
- it is widely used for serving static content, proxying requests, load balancing, caching, and security filtering.
- Key Features of NGINX:
     - High-Performance Web Server – Handles millions of requests with minimal resource usage.
     - Load Balancing – Supports Layer 4 (TCP) and Layer 7 (HTTP) load balancing.
     - Reverse Proxy – Acts as an intermediary between clients and backend servers.
     - Security & Access Control – Supports SSL/TLS termination, rate limiting, and DDoS protection.
     - Caching & Compression – Optimizes response times by caching static content.
     - Microservices Support – Works with Kubernetes, Docker, and cloud environments.

#### Landing Zones in AWS
- its AWS is a pre-configured, secure, and scalable environment that organizations use to set up and govern their AWS workloads. 
     - It follows best practices for security, compliance, networking, and governance, providing a standardized foundation for multi-account AWS environments.
1. Multi-Account Structure
     - Uses AWS Organizations and Control Tower to manage multiple accounts.
     - Common account structure:
          - Management Account – Centralized governance and billing.
          - Security Account – Logging, monitoring, and security services (AWS GuardDuty, Security Hub).
          - Shared Services Account – Centralized services like IAM, DNS, and networking.
          - Workload Accounts – Separate accounts for development, staging, and production environments.
2. Security and Compliance
     - IAM and RBAC: Least privilege access with AWS IAM roles, groups, and policies.
     - Guardrails: Preventive and detective guardrails using AWS Control Tower.
     - Logging & Monitoring: Centralized logging with AWS CloudTrail, AWS Config, and AWS Security Hub.
     - Encryption: Enforce encryption for data at rest (AWS KMS) and in transit (TLS).
3. Networking and Connectivity
     - VPC Design: Multi-account VPC peering, Transit Gateway, or AWS PrivateLink.
     - Firewall & Security Groups: Strict Network ACLs, Security Groups, AWS WAF.
     - Hybrid Connectivity: AWS Direct Connect or VPN for on-prem integration.
4. Automation and CI/CD
     - Infrastructure as Code (IaC): Terraform, AWS CloudFormation for Landing Zone deployment.
     - GitOps: Managed through Git repositories with automated pipelines.
     - Configuration Management: AWS Systems Manager for patching and automation.
5. Monitoring & Incident Management
     - Observability Stack: AWS CloudWatch, Prometheus, Grafana, and Datadog for monitoring.
     - Incident Response: AWS Lambda automation for self-healing.
     - SLI/SLO Tracking: Use of AWS Service Catalog to manage and monitor reliability.
6. Cost Management & Optimization
     - Billing and Cost Insights: AWS Cost Explorer, AWS Budgets.
     - Rightsizing & Reserved Instances: AWS Compute Optimizer and Savings Plans.
     - Tagging Strategy: Enforced via AWS Tag Policies.

#### Landing Zone in Azure
- Azure Landing Zones are a set of guidelines, ARM/Bicep/Terraform templates, and configurations to set up secure, scalable Azure environments.
- Models:
     - CAF (Cloud Adoption Framework) Landing Zones
     - Azure Landing Zone Accelerator (Terraform/Bicep-based automation)
- Components:
     - Management groups
     - Azure Policy, RBAC, Blueprints
     - Log Analytics, Azure Monitor, Security Center
     - Shared services: hub-spoke networking, identity (Azure AD)
- Use Cases:
     - Enterprise-scale architecture
     - Role-based access with RBAC
     - Compliance enforcement with Azure Policies
     - Hybrid networking setup

#### Landing Zone (Oracle Cloud)
- Oracle Cloud Landing Zones provide automated Terraform-based solutions for setting up secure, multi-compartment OCI environments.
- Components:
     - Tenancy configuration
     - Compartments (similar to projects/folders)
     - IAM (users, groups, dynamic groups)
     - Policies for access control
     - Networking (VCNs, DRG, subnets)
     - Audit/logging setup
- Use Cases:
     - Automate compartment setup for teams/departments
     - Apply security best practices
     - Enable centralized identity and audit control

#### Log Analytics 
- itis the process of collecting, aggregating, parsing, indexing, searching, analyzing, and visualizing logs from various systems (e.g., servers, containers, applications, cloud services) to:
     - Detect incidents and anomalies
     - Troubleshoot performance issues
     - Perform root cause analysis (RCA)
     - Ensure compliance and auditing
     - Improve observability and reliability
- Design a centralized log analytics solution for a multi-cloud environment
     - Use Fluent Bit or Logstash on each host/container to collect logs.
     - Route logs to a centralized storage like Amazon OpenSearch, Azure Log Analytics, or Elasticsearch cluster.
     - Use Log Enrichment (add labels like region, service, env).
     - Store logs in S3/Blob for archival and use ILM (Index Lifecycle Management) to control storage costs.
     - Enable role-based access (RBAC) for querying via Kibana/Grafana.
     - Integrate alerting systems (e.g., Prometheus Alertmanager, Grafana Alerts, Splunk On-Call) for anomalies.

#### EBS and EFS difference

|Feature|EBS|EFS|
|-|-|-|
|Storage Type|Block|File|
|Access|Single EC2|Multiple EC2s|
|Performance|High IOPS, low latency|Scales with use, higher latency|
|Mount Method|/dev/xvdf (block device)|NFS mount|
|Best For|Databases, boot volumes|Shared storage, web servers, containers|
|Availability Zone Scope|Tied to one AZ|Regional (multi-AZ)|
|OS Support|Linux & Windows|Linux only|

#### Elastic File System - EFS
Elastic File System is a fully managed, scalable, and shared file storage service for use with AWS cloud and on-premises resources. It provides a highly available, elastic file system that automatically scales based on storage needs.
- Key Features:
     - Fully Managed & Serverless – No need to provision storage.
     - Scalability – Automatically scales up/down with demand.
     - Multi-AZ & High Availability – Replicated across multiple Availability Zones.
     - NFS-Compatible – Supports Network File System (NFSv4).
     - Concurrent Access – Multiple EC2 instances can access the same file system.
     - Lifecycle Management – Moves infrequently used data to lower-cost storage.

- **EFS is primarily used to share storage across multiple instances or containers.**
     - Create an EFS File System in the AWS Console or CLI.
     - Attach EFS to EC2 instances using NFS mount.
     - Use File Storage – Read and write files as a shared filesystem.
     - Configure Access & Permissions using AWS IAM and Security Groups.
     - Enable Backup & Lifecycle Policies to optimize costs.

#### Elastic Block Store - EBS
Amazon Elastic Block Store (EBS) is a high-performance block storage service designed for use with EC2 instances. It provides persistent storage that remains intact even when an instance is stopped or terminated.
- Key Features:
     - Persistent Storage – Data is retained even after instance termination.
     - High Performance – SSD-based volumes offer low latency and high IOPS.
     - Scalability – Storage can be resized dynamically.
     - Snapshots & Backups – Supports automated backups and replication.
     - Encryption & Security – Supports AWS KMS encryption.
     - Multi-Attach (For io2 Volumes) – Attach a volume to multiple instances.
- EBS volumes are used as virtual hard drives for EC2 instances.
     - Create an EBS Volume from AWS Console or CLI.
     - Attach the Volume to an EC2 instance.
     - Format & Mount the Volume to make it usable.
     - Use for Storage (Databases, Applications, etc.).
     - Take Snapshots & Backups for data protection.

#### AWS services majorly used
- **Compute Services**
     - **EC2 (Elastic Compute Cloud)** - Used for hosting applications, databases, and workloads.
     - **EKS (Elastic Kubernetes Service)** - Managed Kubernetes cluster for deploying containerized applications.
     - **Lambda (Serverless Compute)**      - Runs functions without provisioning servers.
- **Networking & Security**
     - **VPC (Virtual Private Cloud)**      - Isolates workloads using subnets, NAT, Internet Gateway, etc.
     - **Elastic Load Balancer (ALB/NLB)**      - Distributes traffic across instances or containers.
     - **Route 53**     - DNS service to route traffic globally.
     - **AWS WAF & Shield**      - Protects applications from DDoS and OWASP attacks.
- **Storage & Databases**
     - **S3 (Simple Storage Service)**      - Object storage for static assets, backups, and data lakes.
     - **EBS (Elastic Block Store)**      - Block storage for EC2 instances.
     - **RDS (Relational Database Service)** - Managed databases (MySQL, PostgreSQL, Aurora, etc.).
     - **DynamoDB** - NoSQL database for high-speed applications.
     - **ElastiCache (Redis/Memcached)** - In-memory caching for fast database queries.
- **CI/CD & DevOps**
     - **CodePipeline & CodeBuild** - Automates CI/CD for deploying apps.
     - **Terraform & CloudFormation** - Infrastructure as Code (IaC).
     - **AWS Systems Manager (SSM)** - Manages EC2 instances remotely.
- **Monitoring & Logging**
     - **CloudWatch** - Logs, metrics, and alerts.
     - AWS X-Ray - Traces application performance.
     - **CloudTrail** - Logs all AWS API activities.
     - **Prometheus & Grafana** (on AWS Managed Services) - Monitors Kubernetes & AWS services.
- **Security & IAM**
     - **IAM (Identity and Access Management)** - Manages users, roles, and permissions.
     - **Secrets Manager** - Securely stores API keys, database passwords.
     - **AWS KMS** (Key Management Service) - Encrypts S3 data, EBS volumes, RDS instances.
- **Containers & Serverless**
     - **ECS (Elastic Container Service)** - Manages Docker containers without Kubernetes.
     - **Fargate** - Serverless compute for containers
     - **EKS** → Kubernetes, highly customizable.
       
#### multi-region HA application in AWS
Use Route 53 for geo-routing, deploy workloads across multiple VPCs using Transit Gateway, and replicate data via Aurora Global Database or DynamoDB Global Tables

#### Fargate
- its a **serverless container** with minimal management overhead.
- It’s best for microservices, CI/CD pipelines, and event-driven applications.
- However, for cost-sensitive or high-performance workloads, EC2 might be a better choice

- **Deploy a Container on AWS Fargate with ECS**:
     - Create an ECS Cluster with Fargate as the launch type.
     - Define a Task Definition (container settings, CPU, memory, IAM roles).
     - Create a Service to manage and scale the container.
     - Attach a Load Balancer (ALB or NLB) if required.
     - Deploy and monitor with CloudWatch Logs.
- **Deploy a Kubernetes Pod on Fargate with EKS**:
     - Create an EKS Cluster with Fargate enabled.
     - Define a Fargate Profile (namespace selection).
     - Deploy workloads using kubectl.

#### Azure DevOps components
Azure DevOps Services  --- A suite of cloud-based DevOps tools that support collaboration, CI/CD, and application lifecycle management.
1. **Azure Repos**                # A Git-based version control system for managing source code.
2. **Azure Pipelines**            # A CI/CD service that automates builds, testing, and deployments.
3. **Azure Boards**               # Agile project management for tracking work items, sprints, and releases.
4. **Azure Test Plans**           # A testing suite for manual and automated testing.
5. **Azure Artifacts**            # A package management service for storing and sharing dependencies.

- in detail
1. **Azure Repos**
     - Supports both Git and TFVC (Team Foundation Version Control).
     - Provides branching strategies, pull requests, and code review processes.
     - Integration with Azure Pipelines for automated builds.
2. **Azure Pipelines**
     - Supports CI/CD automation for faster delivery.
     - Compatible with multiple languages (Java, .NET, Python, Node.js, etc.).
     - Multi-cloud deployment (Azure, AWS, GCP, Kubernetes).
     - Uses YAML or classic UI-based pipelines.
3. **Azure Boards**
     - Helps teams manage work items, backlogs, and sprints.
     - Supports Kanban and Scrum methodologies.
     - Provides dashboards and reports for tracking progress.
4. **Azure Test Plans**
     - Allows manual, exploratory, and automated testing.
     - Supports integration with Selenium, JMeter, and other test frameworks.
     - Provides bug tracking and reporting.
5. **Azure Artifact**s
     - Manages Maven, npm, NuGet, and Python packages.
     - Provides artifact versioning and retention policies.
     - Ensures secure package sharing across teams.
- **Additional Feature**s
     - Service Connections – Securely integrate external tools and cloud services.
     - Agent Pools – Manage self-hosted and Microsoft-hosted build agents.
     - Security & Compliance – Implement RBAC (Role-Based Access Control) and audit logs
     - Integration with GitHub – Enables seamless CI/CD for GitHub repositories.

#### Azure DevOps key activities
1. **CI/CD Pipeline Implementation & Optimization**
     - Design, develop, and manage Azure Pipelines for continuous integration and deployment.
     - Implement multi-stage YAML pipelines for automated builds, testing, and releases.
     - Ensure blue-green and canary deployments for minimal downtime.
2. **Source Code Management & Version Control**
     - Manage repositories using Azure Repos (Git) or integrate with GitHub.
     - Implement branching strategies (GitFlow, trunk-based development).
     - Set up pull request policies and code reviews.
3. **Infrastructure as Code (IaC)**
     - Use Terraform, ARM Templates, or Bicep to provision cloud resources.
     - Maintain version-controlled infrastructure in Azure Repos.
     - Automate environment setup with Azure DevOps Pipelines.
4. **Security & Compliance**
     - Implement RBAC (Role-Based Access Control) for Azure DevOps services.
     - Use Azure Key Vault for secrets management.
     - Perform vulnerability scanning in CI/CD pipelines.
5. **Monitoring & Logging**
     - Set up Azure Monitor, Application Insights, and Log Analytics.
     - Implement Prometheus/Grafana or ELK for logs and metrics.
     - Create automated alerts for failures and performance issues.
6. **Performance & Cost Optimization**
     - Optimize CI/CD pipeline efficiency to reduce build times.
     - Use self-hosted agents to minimize costs.
     - Implement auto-scaling and fault tolerance in cloud environments.
7. **Containerization & Kubernetes**
     - Build and manage Docker images for applications.
     - Deploy and manage workloads on Azure Kubernetes Service (AKS).
     - Use Helm charts for application packaging and deployment.
8. **Release Management & Blue-Green Deployments**
     - Set up multi-environment deployments (Dev, QA, Prod).
     - Use feature flags and deployment rings for controlled releases.
     - Automate rollback strategies for failed deployments.
9. **Incident Management & RCA**
     - Respond to incidents, failures, and service degradation.
     - Perform Root Cause Analysis (RCA) and implement corrective actions.
     - Improve system reliability using SLOs, SLIs, and SLAs.
10. **Collaboration & Documentation**
     - Work closely with developers, testers, security teams, and cloud architects.
     - Maintain documentation for pipelines, infrastructure, and troubleshooting.
     - Conduct DevOps best practices training for teams.

#### App Services in Azure

- **Compute Services**:
     - **Azure Virtual Machines (VMs)** – Provides on-demand, scalable computing resources for running Windows and Linux-based workloads.
     - **Azure Virtual Machine Scale Sets (VMSS)** – Enables auto-scaling of virtual machines to handle varying workloads.
     - **Azure Functions** – A serverless compute service that allows running event-driven code without managing infrastructure.
     - **Azure Kubernetes Service (AKS)** – Fully managed Kubernetes cluster for containerized applications.
     - **Azure Container Apps** – Serverless containers with automatic scaling and built-in Dapr integration for microservices.
     - **Azure App Service** – A fully managed platform for hosting web apps, APIs, and mobile backends.
     - **Azure Batch** – A job scheduling and compute management service for running large-scale parallel and batch computing workloads.
     - **Azure Service Fabric** – A distributed systems platform for building scalable, reliable microservices applications.
- **Networking Services**:
     - **Azure Application Gateway** – A Layer 7 load balancer with web application firewall (WAF) support.
     - **Azure Front Door** – A global application delivery network providing load balancing, caching, and security.
     - **Azure Traffic Manager** – DNS-based traffic load balancing across global regions.
     - **Azure VPN Gateway** – Securely connects on-premises networks to Azure over VPN tunnels.
     - **Azure ExpressRoute** – Private, high-bandwidth connectivity between on-premises networks and Azure.
- **Storage & Database Services**:
     - **Azure Blob Storage** – Massively scalable object storage for unstructured data.
     - **Azure Files** – Managed file shares accessible via SMB and NFS.
     - **Azure SQL Database** – Fully managed relational database service based on SQL Server.
     - **Azure Cosmos DB** – Globally distributed, multi-model NoSQL database service.
     - **Azure Cache for Redis** – In-memory data caching for fast performance.
     - **Azure Data Lake Storage** – Scalable storage for big data analytics.
- **Security & Identity Services**:
     - **Azure Active Directory (Azure AD)** – Identity and access management (IAM) service for secure authentication.
     - **Azure Key Vault** – Securely stores and manages secrets, keys, and certificates.
     - **Azure Security Center** – Provides threat protection and security recommendations for Azure resources.
     - **Azure Sentinel** – Cloud-native SIEM (Security Information and Event Management) for threat detection and response.
- **Integration & Messaging Services**:
     - **Azure Logic Apps** – Serverless workflow automation for integrating apps and services.
     - **Azure Service Bus** – Cloud messaging service for decoupling applications and services.
     - **Azure Event Grid** – Event routing service for real-time event-driven architectures.
     - **Azure API Management** – Manages and secures APIs for internal and external use
- **Monitoring & Management Services**:
     - **Azure Monitor** – Collects, analyzes, and acts on telemetry data from Azure resources.
     - **Azure Application Insights** – Performance monitoring for web applications.
     - **Azure Automation** – Automates cloud tasks and configuration management.

1. Azure App Service (Web Apps)
     - Used for deploying containerized and non-containerized applications.
     - Integrated with Azure DevOps Pipelines for automatic deployment.
     - Enabled slot swapping for blue-green deployments.
     - Configured custom domains, SSL certificates, and authentication using Azure AD.

2. Azure Functions
     - Used for event-driven automation (e.g., triggering CI/CD workflows).
     - Integrated with Service Bus, Event Grid, and Storage Queues.
     - Automated tasks like scaling resources, handling webhook events, and monitoring deployments.

3. Azure Kubernetes Service (AKS)
     - Managed Kubernetes cluster for containerized applications.
     - Deployed using Helm charts, Terraform, and Azure DevOps CI/CD pipelines.
     - Integrated with Azure Monitor, Prometheus, and Grafana for observability.

4. Azure Container Apps
     - Used for deploying serverless containers without managing Kubernetes.
     - Integrated with Dapr for microservices communication.
     - Used for scaling lightweight applications and event-driven processing.

5. Azure API Management (APIM)
     - Managed API versioning, security, and analytics.
     - Integrated with Azure DevOps CI/CD for automated API deployments.
     - Applied JWT validation, OAuth authentication, and rate-limiting policies.

6. Azure Logic Apps
     - Used for workflow automation (e.g., integrating DevOps with monitoring tools).
     - Connected with GitHub, Jira, Slack, and Azure Monitor for alerting and incident management.

7. Azure SQL Managed Instance / Azure Cosmos DB
     - Used Azure DevOps Pipelines for schema migrations and database updates.
     - Implemented Data Masking, Geo-Replication, and Backup Policies.

8. Azure Key Vault
     - Stored and managed secrets, certificates, and keys securely.
     - Integrated with Azure DevOps Pipeline Variables for secret management.

9. Azure Virtual Machines (VMs) & VM Scale Sets
     - Automated VM provisioning using Terraform.
     - Configured Azure DevOps agents on VMs for running build jobs.
     - Implemented auto-scaling and integrated with Azure Load Balancer.

#### Azure Functions
- I have used to automate tasks, trigger CI/CD workflows, manage cloud resources, handle event-driven processes.
- I have used Azure Functions for various automation, security, and DevOps processes, including:
     - CI/CD pipeline automation
     - Auto-scaling Kubernetes & VMs
     - Security compliance & auditing
     - Self-healing mechanisms
     - Secrets rotation in Azure DevOps
     - Log file processing & alerting

|Feature|Benefit|
|-|-|
|Serverless Execution|No need to manage infrastructure|
|Event-driven Model|Executes only when needed|
|Cost-Effective|Pay only for execution time|
|Seamless Integration|Works with Azure Monitor, DevOps, Key Vault, Kubernetes, etc.|
|Scalable|Handles high loads efficiently|

1. Automating CI/CD Workflows
- Trigger Azure DevOps pipeline execution automatically based on external events like GitHub webhooks, Azure Storage changes, or Service Bus messages.
     - Created an Azure Function (HTTP Trigger) to listen for incoming webhook requests from GitHub.
     - The function validated the payload and triggered an Azure DevOps pipeline via REST API.
     - Used Azure Key Vault to securely manage DevOps PAT (Personal Access Token).

2. Auto-Scaling AKS Nodes Based on Workload
- Dynamically scale Azure Kubernetes Service (AKS) nodes based on CPU & memory utilization instead of relying only on Kubernetes HPA (Horizontal Pod Autoscaler).
     - Deployed an Azure Function (Timer Trigger) to run every 5 minutes.
     - It fetched CPU & Memory metrics from Azure Monitor.
     - If thresholds were exceeded, the function automatically increased node count using Azure CLI commands via REST API.
     - Sent an alert to Teams & Slack using Azure Logic Apps.

3. Automated Security & Compliance Checks
- Perform security audits on Azure resources (e.g., Storage, Key Vault, AKS, VMs) and ensure compliance with company security policies.
     - Developed an Azure Function (Timer Trigger) that ran every 24 hours.
     - Checked for:
          - Open security groups (NSGs)
          - Expired SSL/TLS certificates
          - Unused Azure resources
          - Missing encryption on Azure Storage & SQL
     - If any issue was found, the function sent alerts to Azure Monitor and triggered a remediation pipeline in Azure DevOps.

4. Self-Healing Mechanism for VM & Container Failures
- If an Azure VM or a Kubernetes Pod crashes, it should be automatically restarted and logged for investigation.
     - Created an Azure Function (Event Grid Trigger) to listen for VM shutdown events.
     - If a VM unexpectedly stopped, the function restarted it using Azure PowerShell or CLI commands.
     - Logged the incident in Azure Log Analytics.
     - For Kubernetes, it monitored pod status in AKS and restarted failed pods if necessary

5. Azure DevOps Pipeline Secrets Rotation
- Automatically rotate secrets used in Azure DevOps pipelines (e.g., Database passwords, API keys, Service Principal credentials) to enhance security.
     - Created an Azure Function (Timer Trigger) to run every 30 days.
     - Generated a new secret for databases & APIs using Azure Key Vault.
     - Updated the Azure DevOps Library variable group with the new secret via REST API.
     - Notified DevOps teams via email & Microsoft Teams.

6. Handling Large Log Files & Sending Alerts
- Process large application logs stored in Azure Blob Storage, filter critical errors, and send alerts to Ops teams via Slack & PagerDuty.
     - Created an Azure Function (Blob Trigger) that gets triggered whenever a new log file is uploaded.
     - Parsed the log file to identify critical errors (e.g., HTTP 500, OutOfMemory, DB failures).
     - Sent alerts to Teams, Slack, PagerDuty if a critical error was found.

#### Azure App Service
Azure App Service is a fully managed platform-as-a-service (PaaS) offering from Microsoft Azure that enables the deployment, hosting, and scaling of web applications, RESTful APIs, and mobile backends. It supports multiple programming languages, including .NET, Java, Python, PHP, Node.js, and Ruby.

- Azure App Service is used because it simplifies application hosting and provides built-in scalability, security, and DevOps integration.

- Key Reasons for Using Azure App Service
     - Reduces Operational Overhead – No need to manage servers.
     - Security & Compliance – Supports AAD authentication, firewall rules, private endpoints.
     - High Availability – Supports auto-scaling & multi-region deployments.
     - CI/CD Automation – Seamless integration with Azure DevOps & GitHub.
     - Monitoring & Troubleshooting – Logs, metrics, alerts via Azure Monitor.
     - Cost-Effective – Pay-as-you-go pricing, no upfront infrastructure costs.

- Key Features of Azure App Service
     - Supports Multiple Runtimes – .NET, Java, Node.js, PHP, Python, etc.
     - Containerized Deployments – Supports Docker and Azure Kubernetes Service (AKS) integration.
     - CI/CD Integration – Works with Azure DevOps, GitHub Actions, Jenkins, and Bitbucket.
     - Scaling Options – Supports auto-scaling based on traffic load.
     - Custom Domains & SSL – Secure application using HTTPS/TLS certificates.
     - Authentication & Security – Integrates with Azure Active Directory (AAD), OAuth, and OpenID.
     - Monitoring & Logging – Supports Azure Monitor, Application Insights, and Log Analytics.
     - Deployment Slots – Supports blue-green deployment via staging slots.

- Use Case 1: Hosting Java-Based Microservices
     - We deployed a Java Spring Boot application on Azure App Service (Linux).
     - Integrated it with Azure SQL Database as the backend.
     - Used CI/CD pipelines in Azure DevOps to automate deployments.
     - Enabled auto-scaling to handle traffic spikes efficiently.
     - Configured Azure Application Gateway for secure access.

- Use Case 2: Hosting a React/Angular Frontend
     - Hosted a React.js single-page application (SPA) on Azure App Service.
     - Configured Azure CDN for caching and faster content delivery.
     - Enabled custom domain & SSL certificates for secure access.
     - Integrated Azure API Management (APIM) for backend API calls.

- Use Case 3: Running a RESTful API Backend
     - Developed a Node.js API and deployed it to Azure App Service.
     - Integrated with Azure Key Vault for managing API secrets securely.
     - Used Azure Monitor & Application Insights for performance monitoring.
     - Implemented role-based access control (RBAC) using Azure AD authentication.

- Use Case 4: Blue-Green Deployment with Deployment Slots
     - Created Staging and Production slots in Azure App Service.
     - Deployed changes to the Staging slot, ran tests, and performed validation.
     - Swapped Staging → Production for zero-downtime deployment.
     - Enabled automatic rollback on failure using Azure DevOps.

#### AKS - Azure Kubernetes Service
Azure Kubernetes Service (AKS) is a managed Kubernetes service in Microsoft Azure that simplifies the deployment, management, and scaling of containerized applications. It eliminates the complexity of Kubernetes cluster management by handling infrastructure provisioning, scaling, updates, and security on behalf of the user.

- Key Features of AKS:
     - Managed Kubernetes Control Plane (Azure manages the Kubernetes master nodes for free).
     - Integrated with Azure DevOps for CI/CD pipelines.
     - Auto-scaling of worker nodes using Cluster Autoscaler and KEDA.
     - Built-in monitoring and logging via Azure Monitor and Prometheus/Grafana.
     - RBAC and Azure Active Directory (AAD) integration for secure authentication.
     - Built-in networking support (Azure CNI, Kubenet, and BYO networking).
     - Supports Windows and Linux containers.

- Implementing AKS in a DevOps Workflow
     - Provisioned AKS Cluster using Terraform
     - Built and Pushed Docker Images to Azure Container Registry (ACR)
     - Deployed Application on AKS using Helm
     - Integrated Azure DevOps Pipeline for automated CI/CD.
     - Implementing Prometheus, Grafana, and Log Analytics for monitoring.
     - Applied security best practices (RBAC, network policies, secret management).
 
- AKS for several use cases
1. Scalable Microservices Deployment
- Requirement: Deploying a Java-based microservices application that needed high availability, auto-scaling, and zero-downtime deployments.
     - Solution: AKS enabled auto-scaling of pods based on CPU/memory utilization and integrated with Azure Load Balancer for traffic distribution.
2. CI/CD Automation for Kubernetes
- Requirement: Implement a fully automated CI/CD pipeline using Azure DevOps to deploy containerized applications on Kubernetes.
     - Solution:
          - Built Docker images and stored them in Azure Container Registry (ACR).
          - Used Helm charts for defining Kubernetes configurations.
          - Implemented rolling updates and blue-green deployments using Helm and Kubernetes manifests.
3. High Availability & Disaster Recovery
- Requirement: Ensure zero downtime and a disaster recovery plan for a production application.
     - Solution:
          - Deployed AKS clusters in multiple Azure regions using Terraform.
          - Used Azure Traffic Manager for global load balancing and failover.
          - Enabled multi-zone node pools for high availability.
4. Observability & Monitoring
- Requirement: Improve real-time monitoring, logging, and alerting for AKS workloads.
     - Solution: Integrated:
          - Azure Monitor, Prometheus, and Grafana for performance metrics.
          - ELK (Elasticsearch, Logstash, Kibana) and Fluentd for logging.
          - Azure Application Insights for tracing application requests.
5. Cost Optimization
- Requirement: Reduce cloud costs by optimizing Kubernetes resource usage.
     - Solution:
          - Used Cluster Autoscaler to scale nodes dynamically.
          - Used KEDA (Kubernetes Event-Driven Autoscaler) for scaling workloads based on custom events.
          - Implemented Azure Spot Nodes for running non-critical workloads at a lower cost.

#### Azure Container Apps
- it is a fully managed serverless container service that allows you to deploy containerized applications without managing Kubernetes infrastructure. It is designed for microservices, APIs, event-driven workloads, and background tasks, providing scalability, auto-scaling, and built-in observability

- useful when:
     - You need serverless containerization without managing Kubernetes.
     - You require automatic scaling based on HTTP requests, CPU, or custom events.
     - You want built-in networking, Dapr support, and ingress management.
     - Your workload includes background jobs, microservices, or event-driven processing.
     - You need cost-efficient container hosting without overprovisioning resources.

Key Features of Azure Container Apps
1️. Serverless Compute for Containers
     - Deploy Docker containers without provisioning VMs or Kubernetes.
     - Supports any language or framework (Node.js, Python, Java, .NET, Go, etc.).
     - No need to manage container orchestration.
2. Built-in Auto-Scaling
     - Scales dynamically based on:
     - HTTP Requests
     - CPU/Memory Usage
     - KEDA (Kubernetes Event-Driven Autoscaling) for event-based scaling
     - Can scale down to zero when idle, reducing costs.
3. Ingress and Networking
     - Public or private access via Azure Virtual Network (VNet).
     - Built-in TLS/SSL termination with custom domain support.
     - Service-to-service communication via internal networking.
4️. Dapr (Distributed Application Runtime) Support
     - Helps in building microservices with features like:
     - Service-to-service calls
     - Pub/Sub messaging
     - State management
     - Secrets management
     - No need for complex Kubernetes setups.
5. Azure DevOps & GitHub Actions Integration
     - Deploy ACA using Azure DevOps Pipelines or GitHub Actions.
     - Supports continuous deployment with Azure Container Registry (ACR).
6. Observability & Monitoring
     - Azure Monitor, Log Analytics, Application Insights for real-time logging & metrics.
     - Built-in Jaeger Tracing and OpenTelemetry support.

#### ACR (Azure Container Registry)
- Azure Container Registry is a managed Docker container registry service that allows you to:
     - Store and manage container images (Docker, OCI).
     - Host Helm charts, SBOMs, and OCI artifacts.
     - Integrate with Azure Kubernetes Service (AKS) or Azure DevOps pipelines.
     - Manage image lifecycle, security, and geo-replication for global deployments.
- secure an ACR instance
     - Private Endpoint: Ensures ACR traffic stays in Azure backbone network.
     - RBAC: Assign roles like AcrPull, AcrPush, Owner to identities.
     - Service Principal or Managed Identity: Used in CI/CD pipelines or AKS for secure pulls.
     - Content Trust: Enforces signed images only.
     - Firewall rules: IP allowlisting for restricted access.
     - Image Scanning: Integrate with Microsoft Defender for Containers or 3rd-party tools.

#### Service Connections
- Service Connection is a secure, reusable configuration that allows your CI/CD pipeline or DevOps tool to authenticate and interact with external services such as:
     - Cloud providers (Azure, AWS, GCP)
     - Repos (GitHub, GitLab)
     - Artifact registries (Docker Hub, ACR, ECR)
     - Kubernetes clusters (AKS, EKS, GKE)
     - Terraform Cloud/Enterprise
     - Databases or custom endpoints (via Service Hooks)
- They are essential for enabling secure, permission-scoped automation in a CI/CD or infrastructure pipeline.

#### Azure Key Vault
- Azure Key Vault is a cloud-based secrets management service that securely stores and controls access to secrets, encryption keys, and certificates used by applications and services. It helps protect sensitive data and reduces the risk of accidental exposure.

- Key Features of Azure Key Vault
     - Secret Management – Securely stores API keys, passwords, database connection strings, and other secrets.
     - Key Management – Stores and manages encryption keys for data encryption and decryption.
     - Certificate Management – Stores and manages TLS/SSL certificates.
     - Access Control – Uses Azure RBAC and Access Policies to restrict access.
     - Auditing and Logging – Tracks access and changes via Azure Monitor and Azure Sentinel.
- Key Vault Used
     - Securely Store and Access Secrets – Prevents hardcoding credentials in code or repositories.
     - Centralized Secret Management – Ensures a single place for managing secrets across environments.
     - Integration with Azure Services – Works with Azure DevOps, App Service, AKS, and VMs.
     - Automatic Key Rotation – Reduces security risks by enforcing key rotation policies.
     - Compliance & Governance – Helps meet security standards like ISO, SOC, and PCI-DSS.

Some use cases
- Secure Secrets in Azure DevOps Pipelines
     - Store API keys, database credentials, and access tokens securely instead of storing them in pipeline variables.
- Storing and Retrieving Secrets in AKS (Kubernetes)
     - Securely manage database credentials and API keys in Kubernetes without hardcoding them in config files
- Storing TLS Certificates for Web Applications
     - Store and manage SSL/TLS certificates for secure web application deployment 

#### VM - Azure Virtual Machines
- Azure Virtual Machines (VMs) are on-demand, scalable computing resources in Azure’s Infrastructure-as-a-Service (IaaS) offering. They allow running Windows, Linux, and custom images with full control over the OS, storage, and networking.
- Azure VM Scale Sets (VMSS) allow you to deploy and manage a group of identical, auto-scalable VMs to handle high availability and large workloads. It provides automatic scaling based on CPU, memory, or custom metrics.

#### VM creation - Azure Portal
- Steps:
     - Go to portal.azure.com.
     - Search for “Virtual Machines”.
     - Click “+ Add > Virtual machine”.
     - Fill in:
          - Resource Group
          - VM Name
          - Region
          - Image (e.g., Ubuntu, Windows)
          - Size (e.g., Standard B2s)
          - Authentication (SSH key or password)
     - Configure:
          - Disks (Standard/Premium SSD)
          - Networking (VNet, NSG rules)
          - Monitoring (Enable Log Analytics)
          - Click “Review + create” → Create.

#### VM creation - Azure CLI

```
az login

az group create --name myResourceGroup --location eastus

az vm create \
  --resource-group myResourceGroup \
  --name myVM \
  --image UbuntuLTS \
  --admin-username azureuser \
  --generate-ssh-keys \
  --size Standard_B2s
```

#### VM creation - Azure PowerShell

```
Connect-AzAccount

New-AzResourceGroup -Name myResourceGroup -Location eastus

New-AzVM `
  -ResourceGroupName "myResourceGroup" `
  -Name "myVM" `
  -Location "East US" `
  -VirtualNetworkName "myVnet" `
  -SubnetName "mySubnet" `
  -SecurityGroupName "myNSG" `
  -PublicIpAddressName "myPublicIP" `
  -OpenPorts 22
```

#### VM creation by Terraform

```
provider "azurerm" {
  features {}
}

resource "azurerm_resource_group" "rg" {
  name     = "myResourceGroup"
  location = "East US"
}

resource "azurerm_virtual_network" "vnet" {
  name                = "myVnet"
  address_space       = ["10.0.0.0/16"]
  location            = azurerm_resource_group.rg.location
  resource_group_name = azurerm_resource_group.rg.name
}

resource "azurerm_subnet" "subnet" {
  name                 = "mySubnet"
  resource_group_name  = azurerm_resource_group.rg.name
  virtual_network_name = azurerm_virtual_network.vnet.name
  address_prefixes     = ["10.0.2.0/24"]
}

resource "azurerm_network_interface" "nic" {
  name                = "myNIC"
  location            = azurerm_resource_group.rg.location
  resource_group_name = azurerm_resource_group.rg.name

  ip_configuration {
    name                          = "internal"
    subnet_id                    = azurerm_subnet.subnet.id
    private_ip_address_allocation = "Dynamic"
  }
}

resource "azurerm_linux_virtual_machine" "vm" {
  name                = "myVM"
  resource_group_name = azurerm_resource_group.rg.name
  location            = azurerm_resource_group.rg.location
  size                = "Standard_B2s"
  admin_username      = "azureuser"
  network_interface_ids = [azurerm_network_interface.nic.id]

  admin_ssh_key {
    username   = "azureuser"
    public_key = file("~/.ssh/id_rsa.pub")
  }

  os_disk {
    caching              = "ReadWrite"
    storage_account_type = "Standard_LRS"
  }

  source_image_reference {
    publisher = "Canonical"
    offer     = "UbuntuServer"
    sku       = "18.04-LTS"
    version   = "latest"
  }
}
```

#### VM creation - Bicep/ARM Templates

```
resource vm 'Microsoft.Compute/virtualMachines@2021-07-01' = {
  name: 'myVM'
  location: resourceGroup().location
  properties: {
    hardwareProfile: {
      vmSize: 'Standard_B2s'
    }
    osProfile: {
      computerName: 'myVM'
      adminUsername: 'azureuser'
      linuxConfiguration: {
        disablePasswordAuthentication: true
        ssh: {
          publicKeys: [
            {
              path: '/home/azureuser/.ssh/authorized_keys'
              keyData: '<SSH_PUBLIC_KEY>'
            }
          ]
        }
      }
    }
    ...
  }
}
```

#### Application Hosting on Azure VM
- Hosted a stateful application that required custom OS configurations, persistent storage, and full control over the environment.
     - Implementation Steps:
          - Deployed multiple Azure VMs running application servers.
          - Configured Load Balancer for traffic distribution.
          - Automated VM setup using Ansible/Terraform in CI/CD pipelines.
          - Used Azure Monitor and Log Analytics for monitoring and alerts.
- Key Characteristics of Azure Virtual Machines:
     - Infrastructure-as-a-Service (IaaS): Azure VMs provide compute power to run applications, websites, databases, and more. You have the flexibility to choose the operating system and configure them as needed.
     - Fully Managed Compute Resource: VMs can be used for a variety of tasks such as hosting websites, running workloads, custom software applications, or even running self-hosted agents in a DevOps pipeline.
     - Dynamic Scaling: Azure VMs can be scaled manually or through Azure VM Scale Sets (VMSS) to handle demand spikes.
     - High Availability & Disaster Recovery: Azure provides options for high availability (e.g., Availability Sets, Availability Zones) and disaster recovery.
- When to Use Azure Virtual Machines:
     - When you need full control over the OS and software configurations.
     - When you need to run an application or service that cannot be containerized or requires specific infrastructure setup (e.g., legacy apps).
     - When scaling compute resources up or down dynamically is required.
     - When you need to manage applications that do not fit into PaaS offerings (such as app services) or need complete flexibility.

#### Self-Hosted Azure DevOps Agents on VMs
- A self-hosted agent is a virtual or physical machine (either on-premises or in the cloud) that is configured to run Azure DevOps pipelines. These agents are user-managed and provide the ability to run DevOps builds, releases, and deployments.
- Key Characteristics of Self-Hosted Agents:
     - **User-Managed**: Self-hosted agents are typically installed and maintained by the user on virtual machines, on-prem hardware, or even in containers. They require manual installation and configuration of necessary build tools.
     - **More Control**: You have full control over the software, build tools, network configurations, and security settings on the machine where the agent is installed.
     - **Custom Configurations**: Useful if you have special dependencies or need specific configurations (e.g., custom SDKs, specific versions of software) that Microsoft-hosted agents don’t support.
     - **Persistent Environment**: The environment stays persistent and doesn't get reset after each job, meaning caching and pre-installed dependencies can persist across pipeline runs.
     - **Resource Scaling**: You are responsible for scaling, maintaining, and managing the resources needed for the agent.
- When to Use Self-Hosted Agents:
     - When your build or release process requires a specific software configuration (e.g., version-specific tools).
     - When you need the speed of using already installed dependencies.
     - When you want to maintain private network access or custom infrastructure for builds.
     - When you need to run long-running jobs (e.g., large Docker builds, complex machine learning model training).
- Implementation Steps:
     - Created Azure VMs (Linux/Windows) for self-hosted Azure DevOps agents.
     - Installed the Azure DevOps agent on the VM and connected it to the Azure DevOps organization.
     - Used the self-hosted agents for builds and deployments (e.g., heavy Docker builds, Kubernetes deployments).
     - Automated VM provisioning using Terraform or ARM templates.

#### Auto-Scaling Workloads using VM Scale Sets (VMSS)
- Needed auto-scaling VMs to handle fluctuating workloads without manual intervention.
     - Implementation Steps:
          - Configured VM Scale Sets with autoscaling policies based on CPU, memory, and custom metrics.
          - Integrated Azure DevOps Pipelines to deploy the application on scale sets.
          - Used Azure Load Balancer for traffic distribution across VM instances.

#### Hybrid Cloud & VPN Connectivity with Azure VMs
- Deployed jump servers, bastion hosts, and VPN gateways to connect Azure workloads with on-premises data centers.
     - Implementation Steps:
          - Set up Azure VMs as bastion hosts for secure SSH/RDP access.
          - Configured VPN Gateway to connect Azure VMs to on-prem infrastructure.
          - Managed firewall rules and NSGs to secure traffic.

#### Disaster Recovery & Backup using Azure VMs
- Implemented Azure Site Recovery (ASR) for business continuity in case of failures.
     - Implementation Steps:
          - Created VM backups using Azure Backup.
          - Configured ASR replication for VMs to a secondary region.
          - Automated failover/failback in case of DR scenarios.

#### Deploying to Azure Container Apps Using Azure DevOps
- Build a Docker Image
     - Push it to Azure Container Registry (ACR).
- Create Azure Container App
     - Use Terraform, Azure CLI, or ARM templates.
- CI/CD Pipeline in Azure DevOps
     - Build & deploy using Azure DevOps Pipelines.
- Enable Auto-Scaling
     - Define KEDA-based scaling rules.
- Monitor & Troubleshoot
     - Use Azure Monitor, Log Analytics, and Application Insights.

#### Azure DevOps - implement CI/CD pipelines
- Azure DevOps provides Azure Pipelines to automate CI/CD workflows.
1. Continuous Integration (CI):
     - Code is pushed to Azure Repos, GitHub, or other repositories.
     - Build pipeline (yaml/classic) triggers automatically.
     - Build stages include compilation, unit testing, security scanning, and artifact creation.
     - Artifacts are published to Azure Artifacts, Docker Hub, or other registries.
2. Continuous Deployment (CD):
     - Releases are managed in Azure Release Pipelines or multi-stage YAML pipelines.
     - Deployment can target Azure Kubernetes Service (AKS), Azure App Services, VMs, Functions, etc.
     - Approvals, manual gates, and rollback strategies are incorporated.
     - Infrastructure as Code (IaC) via Terraform, Ansible, or ARM templates is used for environment provisioning.
     - Monitoring & Logging via Application Insights, Log Analytics, or Prometheus/Grafana.

#### DevOps pipelines secure Azure 
1. Identity & Access Management:
     - Use Azure AD authentication and RBAC (Role-Based Access Control).
     - Implement least privilege access for service connections, agents, and secrets.
2. Secrets Management:
     - Store secrets in Azure Key Vault and integrate with pipelines.
     - Avoid hardcoding credentials in YAML files.
3. Pipeline Security:
     - Enable pipeline approvals & checks for sensitive deployments.
     - Use branch policies & PR validation to enforce reviews before merging code.
4. Code & Artifact Security:
     - Enable Azure Defender for DevOps to scan repositories for vulnerabilities.
     - Use code signing and artifact integrity verification.
5. Network & Compliance:
     - Restrict pipelines to trusted agent pools.
     - Use Private Links & Service Endpoints to isolate deployments.

#### Azure DevOps - blue-green or canary deployments 
- Azure DevOps supports progressive delivery strategies like blue-green, canary, and rolling deployments:
1. Blue-Green Deployment:
     - Maintain two identical environments (Blue & Green).
     - Deploy new changes to Green while keeping Blue live.
     - Switch traffic using Azure Traffic Manager, Application Gateway, or Kubernetes Ingress.
2. Canary Deployment:
     - Gradually roll out changes to a small subset of users.
     - Use Azure Front Door, Traffic Manager, or Feature Flags to route traffic dynamically.
     - Monitor telemetry (App Insights, Log Analytics) to detect issues before full rollout.
3. Rolling Deployment:
     - Deploy changes incrementally across multiple instances to avoid downtime.
     - Works well with Azure Kubernetes Service (AKS) and VM Scale Sets.

#### Azure devops pipelines - monitor and troubleshoot
- To ensure reliability in CI/CD pipelines, use:
1. Built-in Azure DevOps Logs:
     - Check real-time logs via the Azure Pipelines UI.
     - Use "Enable system diagnostics" for detailed logs.
2. Monitoring Tools:
     - Azure Monitor & Log Analytics for tracking pipeline runs.
     - Application Insights for end-to-end tracing of deployed applications.
     - Grafana & Prometheus for custom monitoring dashboards.
3. Alerts & Notifications:
     - Use Azure DevOps Service Hooks to trigger alerts in Slack, Teams, or PagerDuty.
     - Set up custom Azure Monitor alerts based on pipeline failures.
4. Troubleshooting Tips:
     - Debug agent logs if self-hosted runners fail.
     - Check YAML syntax errors using az pipelines validate.
     - Use retry strategies for transient failures in cloud environments.

#### Azure Resource Manager (ARM) Templates
- its a deployment and management service for Azure. It provides a unified way to manage, provision, organize Azure resources through declarative templates, APIs, built-in role-based access control (RBAC).
- ARM allows you to deploy, update, and delete resources as a single unit, known as a resource group, ensuring consistency and control over cloud infrastructure.
- They're basically JSON files that help you define and deploy Azure resources in a repeatable and consistent way
- instead of going to the Azure portal and clicking around to create resources one by one, we can just write an ARM template to create any resource.
- mainly for automation and consistency. Like, if you’re working in a DevOps setup and you need to spin up the same environment again and again — maybe for dev, test, and prod
- Key features, haaa...
     - They're declarative, so you just say what you want, not how to build it
     - You can use parameters to make them dynamic
     - There’s also variables, functions, and outputs, umm… to make them flexible and reusable
     - And you can even do nested templates or linked templates if it gets complex

- **Unified Resource Management**
     - ARM provides a single control plane to manage all Azure resources such as VMs, storage, databases, networking, Kubernetes clusters, etc.
     - It supports automation via Azure PowerShell, Azure CLI, REST API, SDKs, and Azure Portal.
- **Resource Groups**
     - A resource group is a logical container for Azure resources that share the same lifecycle.
     - All resources in a group can be deployed, updated, and deleted together.
     - Example: Grouping an Azure VM, Virtual Network, and Storage Account in a single resource group.
- **Declarative Infrastructure as Code** (IaC)
     - ARM enables Infrastructure as Code using ARM Templates (JSON-based files) to define, deploy, and update resources in a repeatable and automated manner.
     - Alternative to Terraform & Bicep.
- **Role-Based Access Control (RBAC)**
     - ARM integrates RBAC to define who can perform what actions on resources.
     - Access is managed via Azure Active Directory (Azure AD).
- **Tagging & Policy Enforcement**
     - Tags help in organizing resources (e.g., Environment: Production, Owner: DevOps Team).
     - Azure Policy ensures compliance with security & governance rules (e.g., enforce encryption, region restrictions).
- **Dependency Management & Consistency**
     - ARM ensures dependencies are resolved before provisioning.
     - Deployments remain idempotent, meaning running the same ARM template multiple times does not create duplicates.

#### ARM Template Json file

```
{
  "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#",
  "contentVersion": "1.0.0.0",
  "resources": [
    {
      "type": "Microsoft.Storage/storageAccounts",
      "apiVersion": "2021-02-01",
      "name": "mystorageacct123",
      "location": "[resourceGroup().location]",
      "sku": {
        "name": "Standard_LRS"
      },
      "kind": "StorageV2",
      "properties": {}
    }
  ]
}
```

#### Bicep
- Bicep is actually the next-gen alternative to ARM templates.
     - It’s kind of a simplified language that still compiles down to ARM templates in the backend.
     - So instead of writing 200 lines of JSON… you can do the same in like, 50 lines of clean and readable Bicep code.
     - And haaa… it's much easier to debug, reuse, and maintain.
- Example

```
  resource myStorage 'Microsoft.Storage/storageAccounts@2021-02-01' = {
  name: 'mystorageacct123'
  location: resourceGroup().location
  sku: {
    name: 'Standard_LRS'
  }
  kind: 'StorageV2'
  }
```

#### Triggers
- Triggers define when a pipeline should run.
     - Continuous Integration (CI) : Runs when code is pushed to a branch
     - Pull Request (PR) Triggers : Runs on PR creation or updates
     - Scheduled Triggers : Runs at a specified time (e.g., nightly builds)
     - Manual Triggers : Requires user approval to start

#### Azure Secure security of secrets
- Security
     - Azure DevOps Secret Variables – Store sensitive data in Pipeline Variables ($(MY_SECRET)).
     - Azure Key Vault – Securely fetch secrets at runtime.
     - Service Connections – Authenticate deployments securely.
- 
     - Enable Access Control (RBAC) – Restrict access to pipelines.
     - Secrets Management – Store credentials in Azure Key Vault.
     - Service Principals – Secure cloud authentication.
     - Enable Pipeline Policies – Require approvals before deployment.
     - Enable CI/CD Auditing – Log and monitor pipeline run

#### Spot Instances
- Spot instances are a type of virtual machine (VM) that lets you leverage unused compute capacity at a massively reduced cost—often up to 90% cheaper than standard pricing.
- However, there’s a trade-off: these instances can be evicted at any time by the cloud provider when the capacity is needed for regular workloads.

- Use Case
     - **Batch Jobs** -	Can resume/retry if interrupted
     - **CI/CD Builds** -	Jobs are stateless and short-lived
     - **Data Processing** -	MapReduce, Spark, Flink—built for retries
     - **Containers/Pods** -	Kubernetes clusters with preemption toleration
     - **Testing Environments** -	Cheap, disposable VMs

#### Logs in kubernetes
- info
     - Check Pod Logs (stdout/stderr) - kubectl logs <pod-name>
     - Multi-Container Pod: - kubectl logs <pod-name> -c <container-name>
     - Check Previous Pod Logs (e.g., after crash): kubectl logs <pod-name> --previous
     - Check Logs Across All Pods in a Namespace - kubectl logs -l app=<label> --all-containers=true
     - loop over pods: for pod in $(kubectl get pods -n <namespace> -o name); do kubectl logs $pod -n <namespace>; done
     - All events in a namespace: kubectl get events -n <namespace> --sort-by='.metadata.creationTimestamp'
     - All cluster events: kubectl get events --all-namespaces
     - Describe Resources (for deeper debugging) - kubectl describe pod <pod-name> -n <namespace> ( Helps identify issues like scheduling failures, container crash loops, readiness/liveness failures, etc. )
     - Using journalctl (on the node): journalctl -u kubelet
     - Container runtime logs (e.g., containerd): journalctl -u containerd
     - Logs from DaemonSet or Sidecars (like Fluentd, Filebeat, etc.)
- These tools collect and forward logs to:
     - Elasticsearch (view via Kibana)
     - Loki (view via Grafana)
     - Cloud-native platforms like CloudWatch, Azure Monitor, or Stackdriver
- Use stern or kubetail for Real-Time Logs (Multi-pod tail)
     - stern <pod-name-pattern> -n <namespace>
     - kubetail <deployment-name> -n <namespace>
- Check Logs in Persistent Volumes (if application writes to disk)
     - kubectl exec <pod-name> -- cat /path/to/logfile.log
     - kubectl cp <pod-name>:/path/to/logfile.log .
- Using Monitoring/Logging Stack (ELK, EFK, Loki/Grafana)
     
## Kubernetes Issues

|Issue|Command to Use|Solution|
|-|-|-|
|Pod CrashLoopBackOff|kubectl describe pod <pod>|Check kubectl logs for errors. Fix application crash.|
|Pod ImagePullBackOff|kubectl get pods -o wide|Verify Docker image name and credentials.|
|Pod Pending|kubectl get events|Check for node capacity issues.|
|Service Not Accessible|kubectl describe svc <service>|Verify labels, selectors, and ports.|
|DNS Resolution Failure|nslookup my-service|Check CoreDNS logs and ensure correct service names.|
|OOMKilled (Out of Memory)|kubectl describe pod <pod>|Increase memory limits or optimize app memory usage.|

#### Loki
- Loki is light weight tool, its a log aggregation system designed by Grafana Labs, optimized for storing and querying logs efficiently
- Loki consists of three main components:
     - Promtail – Collects logs from local files and forwards them to Loki
     - Loki – Stores and indexes log metadata (labels, timestamps)
     - Grafana – Visualizes logs using queries
- Promtail is a log collector that reads logs from local files (like Fluentd or Logstash) and pushes them to Loki.

#### AWS Storage Services

|Service|Type|Use Case|Durability Availability|Access Protocols|
|-|-| -|-|-|
|Amazon S3|Object Storage|Data storage, backups, web hosting|99.99% durability|HTTP/HTTPS|
|Amazon EBS|Block Storage|EC2 instance storage|99.999% availability|Block-level|
|Amazon EFS|File Storage|Shared file storage for multiple instances|99.9% availability|NFS|
|AWS Storage Gateway|Hybrid Cloud Storage|Backup, disaster recovery|Depends on configuration|NFS|
|RDS Storage|Database Storage|Managed relational database storage|99.95% availability|SQL-based|

#### Azure Storage Services

|Storage Type|Best For|Protocol|
|-|-|-|
|Blob Storage|Unstructured data, backups, media files|HTTP(S)|
|File Storage|Shared file storage, application file shares|SMB, NFS|
|Disk Storage|VM storage, persistent disk|Block storage|
|Table Storage|NoSQL key-value data|REST API, SDK|
|Queue Storage|Asynchronous messaging|REST API, SDK|
|Data Lake Storage|Big data analytics|HDFS, REST API|
|NetApp Files|High-performance file storage|NFS, SMB|

#### Routing
its network traffic flows between resources, like within the cloud or to the internet or to on-premises networks.
- **Routing Components in AWS**

|AWS Component	|Description|
| - | - |
|Internet Gateway (IGW)	|Enables internet access for public subnets.|
|NAT Gateway / NAT Instance	|Allows private instances to access the internet without being directly exposed.|
|Elastic Load Balancer (ELB)	|Routes traffic to backend instances for high availability.|
|VPC Peering	|Connects two VPCs for internal routing.|
|AWS Transit Gateway	|Centralized hub for inter-VPC and on-premises routing.|
|AWS Direct Connect	|Provides a dedicated network link between on-prem and AWS.|
|AWS Route 53	|DNS and global traffic management service.|

- **Routing Components in Azure**

|Azure Component	|Description|
|- | - |
|Internet Gateway (Default)	|Automatically allows internet access for public IPs.|
|Network Security Group (NSG)	|Controls inbound/outbound traffic with firewall rules.|
|Azure Firewall	|Provides advanced security policies and threat protection.|
|Load Balancer	|Distributes traffic across backend servers.|
|Azure Virtual Network Peering	|Connects multiple VNets.|
|Azure VPN Gateway	|Routes traffic between on-prem and Azure.|
|Azure ExpressRoute	|Dedicated high-speed link between on-prem and Azure.|

- **AWS vs Azure Routing – Key Differences**

|Feature	|AWS 	|Azure |
| - |- | - |
|Default Routing	|Uses Route Tables in VPC	|Uses System Routes in VNet|
|Custom Routing	|Route Tables + Transit Gateway	|User-Defined Routes (UDR)|
|Inter-VPC/VNet Routing	|VPC Peering or Transit Gateway	|VNet Peering|
|Internet Access	|Internet Gateway (IGW)	|Built-in Internet Gateway|
|Private Internet Access	|NAT Gateway / NAT Instance	|Azure NAT Gateway|
|On-Prem Connectivity	|VPN Gateway / Direct Connect	|VPN Gateway / ExpressRoute|
|Firewall Protection	|AWS Security Groups, AWS Firewall	|NSGs, Azure Firewall|

#### Managed Identities
- details
     - Managed identities simplify resource authentication without handling credentials.
     - System-assigned identities are tied to the lifecycle of the resource, while user-assigned identities are reusable. 
     - Managed identities can be used to authenticate to various Azure services (e.g., Key Vault, SQL, Storage).
     - Permissions are granted using RBAC, ensuring secure and controlled access.

## All ports information

|Port|Protocol|Usage|
| - | - |- |
|20, 21|FTP|File Transfer Protocol (Data & Control)|
|22|SSH|Secure Shell for remote access|
|23|Telnet|Remote login (Insecure, avoid using)|
|25|SMTP|Sending email (Restricted in AWS & Azure for spam control)|
|53|DNS|Domain Name System for name resolution|
|67, 68|DHCP|Dynamic Host Configuration Protocol (IP allocation)|
|80|HTTP|Unencrypted web traffic|
|110|POP3|Email retrieval|
|123|NTP|Network Time Protocol (Time synchronization)|
|143|IMAP|Email retrieval|
|443|HTTPS|Secure web traffic (SSL/TLS)|
|465, 587|SMTP (SSL/TLS)|Secure email sending|
|989, 990|FTPS|Secure FTP|
|3389|RDP|Remote Desktop Protocol (Windows)|
| - | **Cloud** |- |
|1194|OpenVPN|VPN connections|
|500, 4500|IPsec VPN|Used for site-to-site VPN tunnels (AWS VPN, Azure VPN Gateway)|
|1723|PPTP VPN|Point-to-Point Tunneling Protocol (Legacy VPN)|
|8443|HTTPS Alternative|Web services and APIs|
|8080|HTTP Alternate|Used by some web applications|
|3306|MySQL|MySQL Database connections|
|1433|MSSQL|Microsoft SQL Server|
|1521|Oracle DB|Oracle Database connections|
|27017|MongoDB|NoSQL database connections|
| - | **AWS** |- |
|2049|AWS EFS|Network File System (NFS) for Amazon EFS|
|6379|Amazon ElastiCache (Redis)|Redis database|
|8081|AWS CodeArtifact|AWS Package Manager|
|8983|Amazon OpenSearch|Elasticsearch-based managed service|
|9090|Prometheus|Monitoring and metrics|
| - | **Azure** |- |
|1433|Azure SQL|Azure SQL Database|
|5671, 5672|Azure Service Bus|Message queueing|
|6380|Azure Redis|Secure Redis Cache|
|1883, 8883|IoT Hub (MQTT)|IoT device messaging|
| _ | **Kubernetes** |- |
|10250|Kubelet API|Secure communication between nodes and master|
|10255|Kubelet Read-Only API|Deprecated but still used in some cases|
|6443|Kubernetes API Server|Cluster control plane communication|
|2379-2380|etcd|Kubernetes cluster state storage|
|30000-32767|NodePort|Kubernetes service exposing apps externally|
|-  | **Monitoring** |- |
|9200|Elasticsearch|Log aggregation|
|1514, 514|Syslog|System logging|
|5601|Kibana|Elasticsearch Dashboard|
|8086|InfluxDB|Time-series database|

### RCA  main info - Root Cause Analysis
#### RCA - critical production outage - Memory leak
- there was **critical production outage** that impacted customer transactions. This issue was caused by a **memory leak** in a microservice running on Kubernetes (EKS), leading to pod crashes and increased response times.
- We identified the issue by analyzing Kubernetes logs, Prometheus metrics, and application logs. The root cause was improper memory allocation due to an unoptimized Java application.
- To resolve the issue,
     1. Increased memory limits and requests in the Kubernetes deployment.
     2. Implemented JVM garbage collection tuning to prevent excessive memory usage.
     3. Set up alerts in Prometheus and Grafana to monitor memory utilization trends.
- As a preventive measure, we conducted load testing and implemented auto-scaling policies to handle traffic spikes efficiently. This RCA helped us improve system stability and observability.

#### RCA - during a high-traffic event - slow response
- there was critical production outage that affected application **during a high-traffic event**. Customers experienced **slow response** times and transaction failures.
- Root Cause Analysis (RCA):
     - After investigating AWS CloudWatch logs, Kubernetes (EKS) metrics, and Prometheus alerts, we found that CPU and memory usage spiked abnormally.
     - The root cause was a memory leak in a microservice, which led to excessive garbage collection and eventually caused the pods to crash.
     - This happened because the Java application had unbounded in-memory caching, leading to out-of-memory (OOM) errors.
- Resolution Steps:
     1. Increased Kubernetes memory limits and requests to avoid premature OOM kills.
     2. Optimized Java garbage collection (G1GC tuning) to reduce heap memory pressure.
     3. Implemented a circuit breaker pattern using Istio to gracefully degrade service instead of failing completely.
     4. Scaled out EKS worker nodes to handle load spikes.
- Preventive Actions:
     - Set up memory utilization alerts in Prometheus and Grafana to detect anomalies earlier.
     - Performed stress testing to simulate high-traffic scenarios.
     - Introduced an automatic pod restart mechanism to handle memory leaks proactively.
- As a result, we reduced downtime by 60%, improved system resilience, and prevented further customer-impacting incidents.

#### RCA - Database Connection Failures in Production
- Incident Summary:
     - Users reported intermittent application failures.
     - Error logs showed “Too many connections” to the database.
- **Root Cause Analysis (RCA):**
- Investigation Steps:
     - Checked application logs (/var/log/app.log) → Found frequent database connection timeouts.
     - Monitored database connections (SHOW PROCESSLIST in MySQL) → Max connections limit exceeded.
     - Kubernetes readiness probes were failing due to slow queries.
- Root Cause:
     - The application was not closing database connections properly, leading to connection leaks.
     - Auto-scaling was not configured, leading to increased load on a limited set of database instances.
- Resolution Steps:
     - Implemented connection pooling in the application.
     - Increased max_connections in MySQL (from 200 to 500).
     - Configured AWS RDS Auto Scaling to handle increased load.
     - Set up Grafana alerts for high connection usage.
- Preventive Measures:
     - Introduced automated database connection cleanup.
     - Conducted load testing to ensure the fix worked.

#### RCA - High Latency in API Gateway Requests
- Incident Summary:
     - API response times increased from 100ms to 2s+, impacting user experience.
     - AWS ALB logs showed a spike in 504 Gateway Timeout errors.
- **Root Cause Analysis (RCA):**
- Investigation Steps:
     - Checked API Gateway logs → Requests were reaching backend but took longer to respond.
     - Examined EC2 instance metrics → High CPU utilization (~95%).
     - Analyzed application logs → Found that the service was waiting for an external API (third-party dependency).
- Root Cause:
     - A third-party API dependency was slow, causing bottlenecks.
	 - Retry logic was misconfigured, leading to increased CPU load.
- Resolution Steps:
     - Implemented a timeout mechanism for API calls.
     - Introduced circuit breaker pattern (using Istio).
     - Optimized auto-scaling policies to add more instances dynamically.
- Preventive Measures:
     - Implemented caching to reduce API calls.
     - Added SLO-based alerting to catch high latency early.

#### RCA - Data Loss Due to S3 Bucket Misconfiguration
- **Problem:** A developer accidentally deletes important objects from an S3 bucket that had no versioning enabled.
- Solution:
     - **Restore from Backups:** If AWS Backup or replication is enabled, restore from the latest backup.
     - **Enable Versioning & MFA Delete:** Prevent accidental deletions in the future.
     - **Use S3 Object Lock:** Enable retention policies for critical objects.
     - **Configure IAM Policies:** Implement least privilege access and enable logging using AWS CloudTrail.

#### RCA - High Latency in RDS Database Queries
- **Problem:** An Amazon RDS database suddenly experiences high latency, affecting application performance.
- Solution:
     - **Check CloudWatch Metrics:** Look for high CPU, memory, or IOPS utilization.
     - **Enable Performance Insights:** Identify slow queries and optimize them using indexing or query refactoring.
     - **Increase Instance Size:** Upgrade to a larger instance type if the load is high.
     - **Use Read Replicas & Caching:** Implement RDS Read Replicas and Redis/Memcached for better performance.
     - **Review Connection Pooling:** Ensure efficient use of database connections.

#### RCA - Auto Scaling Group Fails to Launch New Instances
- **Problem:** An Auto Scaling Group (ASG) is set up to scale automatically, but it fails to launch new instances when demand increases.
- Solution:
     - **Check IAM Role Permissions:** Ensure ASG has permissions to launch EC2 instances.
     - **Verify Launch Template/Configuration:** Ensure it references the correct AMI, instance type, and security groups.
     - **Check Quotas & Limits:** AWS account limits may prevent new instances from being created.
     - **Check AZ Availability:** If a specific AZ is out of capacity, allow multi-AZ deployment.
     - **Inspect Health Checks:** If ELB health checks fail, adjust thresholds or instance warm-up times.

#### RCA - Lambda Function Execution Timeout
- **Problem:** A Lambda function fails due to timeout issues, causing incomplete execution.
- Solution:
     - **Increase Timeout Limit:** Default is 3 seconds; extend it as needed (max 15 minutes).
     - **Optimize Code Execution:** Reduce response times by optimizing loops, API calls, and database queries.
     - **Use Asynchronous Invocation:** If processing is long, switch to SQS or SNS for async execution.
     - **Enable Provisioned Concurrency:** Reduce cold start delays for latency-sensitive functions.
     - **Monitor with CloudWatch Logs:** Identify bottlenecks using AWS X-Ray and CloudWatch

#### Production issues
#### RCA - critical production outage
- there was **critical production outage** that impacted customer transactions. This issue was caused by a **memory leak** in a microservice running on Kubernetes (EKS), leading to pod crashes and increased response times.
- We identified the issue by analyzing Kubernetes logs, Prometheus metrics, and application logs. The root cause was improper memory allocation due to an unoptimized Java application.
- To resolve the issue,
     - Increased memory limits and requests in the Kubernetes deployment.
     - Implemented JVM garbage collection tuning to prevent excessive memory usage.
     - Set up alerts in Prometheus and Grafana to monitor memory utilization trends.
- As a preventive measure, we conducted load testing and implemented auto-scaling policies to handle traffic spikes efficiently. This RCA helped us improve system stability and observability.

#### RCA high-traffic application slow response
- there was critical production outage that affected application **during a high-traffic event**. Customers experienced **slow response** times and transaction failures.
- Root Cause Analysis (RCA):
     - After investigating AWS CloudWatch logs, Kubernetes (EKS) metrics, and Prometheus alerts, we found that CPU and memory usage spiked abnormally.
     - The root cause was a memory leak in a microservice, which led to excessive garbage collection and eventually caused the pods to crash.
     - This happened because the Java application had unbounded in-memory caching, leading to out-of-memory (OOM) errors.
- Resolution Steps:
     1. Increased Kubernetes memory limits and requests to avoid premature OOM kills.
     2. Optimized Java garbage collection (G1GC tuning) to reduce heap memory pressure.
     3. Implemented a circuit breaker pattern using Istio to gracefully degrade service instead of failing completely.
     4. Scaled out EKS worker nodes to handle load spikes.
- Preventive Actions:
     - Set up memory utilization alerts in Prometheus and Grafana to detect anomalies earlier.
     - Performed stress testing to simulate high-traffic scenarios.
     - Introduced an automatic pod restart mechanism to handle memory leaks proactively.
- As a result, we reduced downtime by 60%, improved system resilience, and prevented further customer-impacting incidents.

#### RCA - EC2 Instance Suddenly Stops Responding
- Pinging the instance shows no response, and SSH access fails.
- Solution:
     - **Check AWS Console & CloudWatch Logs:** Verify if the instance is running and check system logs.
     - **Verify Security Groups & NACLs:** Ensure inbound/outbound rules allow necessary traffic.
     - **Check CPU & Memory Utilization:** If high, try restarting the instance.
     - **Check EBS Volume Health:** If the root volume is corrupted, detach it, attach it to another instance, and repair it.
     - **Recreate the Instance:** If nothing works, create a new instance from a backup or AMI.

#### Security & Compliance
How do you implement security best practices in a cloud-native environment?
- Security in a distributed system must be proactive, automated, and enforced at multiple layers.
1. Identity & Access Management (IAM):
     - Implement least privilege access (RBAC, ABAC).
     - Use OIDC, OAuth2, and AWS IAM roles for secure authentication.
2.  Network Security:
     - Enforce VPC isolation, private networking, and service mesh (Istio, Linkerd).
     - Implement Web Application Firewalls (AWS WAF, Azure Front Door).
3. Data Security & Compliance:
     - Encrypt data at rest (AWS KMS, HashiCorp Vault) and in transit (TLS 1.2+).
     - Ensure compliance with SOC 2, HIPAA, GDPR using policy-as-code.
4. Automated Security Scanning:
     - Embed SAST, DAST, and dependency scanning in CI/CD.
     - Enable runtime security monitoring (Falco, Sysdig).

#### POC - Proof of Concept
#### POC - Automate infrastructure provisioning in Azure
- Detail
     - Created a POC using Terraform to provision Azure Kubernetes Service (AKS), VNets, NSGs, and storage accounts.
     - Integrated Terraform with Azure DevOps pipelines for CI/CD.
     - Demonstrated state management using remote backend (Azure Storage with state locking).
- **Outcome:** Standardized infrastructure provisioning across dev and QA environments.

#### POC - Build observability into microservices
- Detail
     - Set up Prometheus and Grafana in a sandbox environment (isolated testing environment) to monitor Kubernetes workloads.
     - Created custom alerts for CPU/memory/disk thresholds and latency metrics.
     - Integrated Alertmanager with Slack for team notifications.
- **Outcome:** Enabled real-time alerts and historical data visualization

#### POC - Achieve zero-downtime deployments
- detail
     - Built a POC using Azure DevOps Pipelines to perform blue/green deployments on AKS.
     - Used service selectors to shift traffic between environments.
     - Integrated health checks and manual approval before promoting new versions.
- **Outcome**: Demonstrated controlled rollout and easy rollback during failures.


#### security best practices
- List
     - Least Privilege Access: Implement RBAC (Role-Based Access Control) in Kubernetes and cloud IAM policies.
     - Secrets Management: Store secrets in HashiCorp Vault, Azure Key Vault, or AWS Secrets Manager.
     - Security Scanning in CI/CD: Integrate SAST (Static Analysis Security Testing) and DAST (Dynamic Analysis Security Testing) tools like SonarQube, Snyk, or Trivy.
     - Zero Trust Architecture: Enforce authentication using mTLS (Mutual TLS), JWT, or OAuth2.0.
     - Compliance & Audit Logging: Use AWS CloudTrail, Azure Security Center, or Falco for Kubernetes security auditing.

### SOP - Standard Operating Procedures
#### SOP - recurring application crashes
- There was a pod failures in production because our on-call team was struggling with troubleshooting **application crashes efficiently**.
- SOP: Kubernetes Pod Failure Troubleshooting Guide
- Steps included:
     - Checking pod health and status (kubectl get pods --all-namespaces)
     - Fetching logs for debugging (kubectl logs <pod-name> -n <namespace>)
     - Checking resource utilization (kubectl top pods)
     - Restarting a problematic pod (kubectl delete pod <pod-name>)
     - Rolling back a deployment if needed (kubectl rollout undo deployment/<deployment-name>)
     - Escalation matrix: When to escalate an issue and whom to contact.
- Impact of the SOP:
     - Reduced incident resolution time by 40% as new engineers could quickly follow steps.
     - Improved on-call effectiveness, as they now had clear guidance.
     - Reduced unnecessary escalations, allowing senior engineers to focus on complex tasks.
- This SOP was well-documented in Confluence and shared across teams via Slack and email, ensuring quick accessibility

#### SOP - Pod gets stuck in CrashLoopBackOff or Pending state.
- SOP Steps:
     - Check the pod status:			#      - **kubectl get pods -n <namespace>**
     - Describe the pod to get error details:	#      - **kubectl describe pod <pod-name> -n <namespace>**
     - Check logs for failure reasons:		#      - **kubectl logs <pod-name> -n <namespace>**
     - Restart the pod manually (if required):	#      - **kubectl delete pod <pod-name> -n <namespace>**
     - Check node status (if pod scheduling fails):	#      - **kubectl get nodes**
     - Escalate to cloud provider if there is an underlying infrastructure issue.
- Outcome: This SOP reduced pod recovery time from 30 minutes to 5 minutes.

#### SOP - Restoring a Failed Jenkins Job, A CI/CD pipeline fails frequently, blocking deployments
- SOP Steps:
     - Identify the failed job in Jenkins UI.
     - Check the logs under "Console Output".
     - Verify SCM connectivity:			#      - **git pull origin main**
     - Check disk space on the Jenkins server:	#      - **df -h**
     - Restart Jenkins service if necessary:	#      - **systemctl restart jenkins**
     - Rerun the pipeline and validate the fix.
- Outcome: This SOP helped junior engineers resolve issues independently.

#### High availability and Fault Tolerance
- Steps
     - **Multi-Region Deployment**: Deploy workloads across multiple regions using Azure Traffic Manager or AWS Route 53.
     - **Auto Scaling & Load Balancing**: Use AKS Horizontal Pod Autoscaler (HPA), Azure Load Balancer, and Kubernetes Cluster Autoscaler.
     - **Circuit Breaker Pattern**: Prevent cascading failures using Istio or Linkerd.
     - **Database Replication & Failover**: Implement Active-Passive or Active-Active replication for databases (e.g., Azure SQL Geo-Replication, AWS Aurora Multi-AZ).
     - **Chaos Engineering**: Conduct failure injection using Gremlin or Chaos Mesh to test system resilience.

#### Migration
1. **Cloud Migration Strategy**
2. **Observability & Performance Monitoring**
3. **validate that an application is production-ready after migration**
4. **security best practices during cloud migration**
5. **Post-Migration Reliability & Cost Optimization**

**Cloud Migration Strategy**
1. **Assess & Plan:**
     - Define migration goals (performance, cost, resilience).
     - Choose the right migration strategy: Rehost (Lift & Shift), Replatform, Refactor, or Rebuild.
     - Perform dependency mapping (via CloudEndure, StratoZone, or AWS Migration Hub).
2. **Document Migration Plans:**
     - Create Application Runbooks (network, storage, scaling, backup)
     - Define SLIs, SLOs, and SLAs for performance & reliability tracking.
     - Maintain pre-migration architecture vs. post-migration state diagrams.
3. **Automate Migration Process:**
     - Use Terraform, Ansible, CloudFormation for infrastructure automation.
     - Implement CI/CD pipelines for app redeployment in the cloud.

**Observability & Performance Monitoring**
1. **Instrument Applications for Tracing & Logging**:
     - Implement OpenTelemetry for distributed tracing (Jaeger, Zipkin).
     - Use structured logging (JSON logs, centralized ELK stack, or Datadog).
2. **Monitor Infrastructure & Workloads:**
     - Deploy Prometheus + Grafana dashboards for real-time metrics.
     - Use AWS CloudWatch / Azure Monitor for cloud-native insights.
     - Set up automated anomaly detection (AI-based insights in Splunk, Datadog APM).
3. **Define Performance Baselines:**
     - Benchmark app performance before & after migration.
     - Conduct load testing with Locust, K6, or JMeter.
     - Set up alerting (Prometheus AlertManager, PagerDuty, Slack notifications).

**validate that an application is production-ready after migration**
1. **Define Acceptance Criteria:**
     - Validate latency, throughput, scalability, failover mechanisms.
     - Run performance benchmarks comparing pre- vs. post-migration SLIs.
2. **Test Across Different Environments:**
     - Canary Deployments to reduce risk.
     - Run chaos engineering tests (Gremlin, LitmusChaos) to check fault tolerance.
     - Conduct failover & disaster recovery tests.
3. **Create Automated Validation Pipelines:**
     - Use Terraform Compliance or OPA to check infrastructure policies.
     - Implement API contract testing (Postman, Pact) for service compatibility.
     - Conduct real-user monitoring (RUM) with New Relic, Google Lighthouse for web apps.

**security best practices during cloud migration**
1. **IAM & Access Controls:**
     - Implement least privilege IAM policies (AWS IAM, Azure AD, GCP IAM).
     - Enforce multi-factor authentication (MFA) & SSO.
2. **Data Security & Encryption:**
     - Encrypt data at rest & transit (TLS 1.2+, AWS KMS, HashiCorp Vault).
     - Mask sensitive data using DLP tools & automated scans.
3. **Security Automation & Compliance Checks:**
     - Use AWS Config, Azure Policy, or HashiCorp Sentinel for compliance as code.
     - Automate CIS benchmarks & security audits with tools like Trivy, Snyk.

**Post-Migration Reliability & Cost Optimization**
1. **Implement Auto-Scaling & Right-Sizing:**
     - Use HPA/VPA for Kubernetes workloads.
     - Optimize resource allocation with AWS Compute Optimizer, Azure Advisor.
2. **FinOps & Cost Monitoring:**
     - Use AWS Cost Explorer, GCP Billing, Azure Cost Management for tracking.
     - Implement budget alerts & anomaly detection.
3. **Continuous Optimization & Reliability Engineering:**
     - Run chaos experiments regularly.
     - Establish SLOs & error budgets to balance feature velocity & reliability.

#### On-prem to Azure
1. Pre-Migration Planning
- Assessment & Discovery
     - Inventory all on-prem workloads (servers, databases, applications, storage, network).
     - Use Azure Migrate or 3rd party tools for:
          - Performance data collection
          - Dependency mapping
          - Application compatibility checks
     - Cost Estimation
          - Use Azure Pricing Calculator and Total Cost of Ownership (TCO) Calculator
          - Estimate compute, storage, network, and licensing costs.
     - Define Migration Goals
          - Cost savings? Scalability? Modernization?
          - Choose strategy per workload: Rehost, Refactor, Rearchitect, Rebuild, Replace (5 Rs)
     - Select Migration Strategy
          - Lift and Shift (Rehost)
          - Hybrid Cloud (for phased transition)
          - Cloud-Native (modernized workloads)
2. Architecture & Design
- Azure Environment Setup
     - Landing Zone setup using Terraform or Bicep:
          - Azure Subscriptions
          - Management Groups
          - RBAC (IAM)
          - Resource Groups
          - Networking (VNet, Subnets, NSGs)
          - Security (Azure Defender, Firewall, WAF)
          - Monitoring (Azure Monitor, Log Analytics)
     - Identity and Access
          - Azure AD setup
          - Hybrid identity with Azure AD Connect or AD DS extension
          - Role-based access control (RBAC)
     - Security & Compliance
          - Governance via Azure Policy
          - Encryption at rest & in transit
          - Secure credentials using Key Vault
     - Networking
          - Site-to-Site VPN or ExpressRoute
          - DNS, IP planning
          - Azure Firewall, NSGs, Application Gateway
3. Migration Execution
- Phase 1: Infrastructure Migration
     - a. VMs (Lift-and-Shift)
          - Use Azure Migrate: Server Migration
          - Validate dependencies and sizing
          - Replicate and test
          - Cutover during planned downtime
     - b. Storage
          - Use Azure File Sync, AzCopy, or Data Box
          - Migrate unstructured data to:
               - Blob Storage
               - Azure Files
               - Azure NetApp Files (if performance sensitive)
- Phase 2: Application & Database Migration
     - a. Databases
          - Use Azure Database Migration Service (DMS)
          - Choose between:
               - Azure SQL Database
               - Azure SQL Managed Instance
               - Cosmos DB, PostgreSQL, MySQL (PaaS)
     - b. Applications
          - Containerize using Docker + Azure Kubernetes Service (AKS)
          - Use App Service for Web Apps
          - Migrate APIs using Azure API Management
          - Refactor legacy apps into microservices if needed
     - CI/CD Integration
          - Implement Azure DevOps Pipelines or GitHub Actions
          - Deploy infrastructure using Terraform or Bicep
          - Integrate secrets using Azure Key Vault
4. Testing & Validation
     - Pre-Cutover Testing
          - Functional testing of workloads
          - Network latency and performance
          - Authentication and access validation
          - Failover and rollback test
     - Data Consistency
          - Verify migrated data using checksum or hash comparison
     - DR Testing
          - Simulate regional failure
          - Test recovery times
          - Document RTO/RPO
5. Cutover & Rollout Plan
     - Cutover Steps
          - Schedule during off-hours or maintenance window
          - Final sync (delta data)
          - Redirect DNS and firewall rules
          - Disable on-prem systems
     - Rollout Plan
          - Roll out by business unit or environment:
               - Dev → Test → UAT → Prod
               - Use feature flags for safe app rollout
               - Monitor KPIs (CPU, Memory, Disk I/O, App availability)
6. Disaster Recovery (DR) Plan
     - Backup Strategy:
          - Azure Backup (VMs, Files, DBs)
          - Snapshot strategy for storage
     - DR Sites:
          - Geo-redundant storage (GRS)
          - Active-passive replication across Azure regions
     - Automation:
          - Azure Site Recovery (ASR) for VM replication and failover
7. Post-Migration Optimization
     - Cost Optimization:
          - Reserved Instances (RIs)
          - Auto-shutdown non-prod
          - Rightsize VMs
     - Monitoring:
          - Application Insights
          - Log Analytics
          - Azure Monitor Alerts
      - Security:
          Azure Security Center recommendations
     - Governance:
          - Azure Policy
          - Cost Management & Billing

#### On-prem to AWS
1. Assessment & Planning
     - Stakeholder Alignment
          - Identify stakeholders (business, IT, security, compliance).
          Define migration goals (cost savings, scalability, DR, etc.).
     - Current Environment Assessment
          - Inventory all workloads (apps, databases, VMs, storage, network).
          Categorize: criticality, dependencies, performance, compliance.
     - AWS Account Setup
          - Setup AWS Organizations, separate accounts for Dev, Test, Prod.
          Define IAM policies, RBAC, AWS Config, GuardDuty.
     - TCO and Cost Estimation
          - Use AWS Pricing Calculator.
          - Consider savings plans, reserved instances, S3 lifecycle policies.
2. Design
     - Architecture Design
          - Choose between Lift-and-Shift, Replatform, or Refactor.
          - Define landing zone: VPC, subnets, NAT, VPN/Direct Connect.
          - Design for:
               - High Availability (HA)
               - Auto-scaling
               - Multi-AZ/Multi-region setup
     - Security and Compliance
          - Map to industry standards (HIPAA, PCI-DSS, ISO, etc.)
          - Define encryption policies (S3, RDS, EBS).
          - Implement logging and monitoring (CloudTrail, CloudWatch, AWS Config).
     - Disaster Recovery Strategy
          - Define RPO/RTO per workload.
          - Choose DR method:
               - Backup & Restore
               - Pilot Light
               - Warm Standby
               - Multi-site Active-Active
          - Test DR plan before go-live.
3. Migration Execution
     - Pilot Migration
          - Select non-critical workloads as pilot.
          - Validate tools: AWS Migration Hub, Application Migration Service, Database Migration Service (DMS).
          - Monitor for performance, errors, rollback strategy.
     - Full-Scale Migration
          - Tools:
               - Servers/VMs: AWS Application Migration Service (MGN)
               - Databases: AWS DMS, native replication
               - Filesystems: AWS DataSync, Snowball for large data
          - Key Steps:
               - Sync data continuously before cutover.
               - Replicate configurations (DNS, firewall rules, etc.).
               - Update environment variables, credentials, DNS records.
4. Testing
     - Validation
          - Perform functional and regression testing.
          - Validate security controls, backups, and monitoring.
          - Load testing to ensure performance.
     - User Acceptance Testing (UAT)
          - Business users test applications.
          - Gather feedback and optimize configurations.
5. Cutover & Rollout
     - Cutover Plan
          - Define downtime windows.
          - Final sync & cutover: switch DNS, terminate old systems gracefully.
          - Have rollback plan ready.
     - Rollout Strategy
          - Big Bang: All at once (risky, good for small stacks).
          - Phased: By business unit/app tier (safer, controlled).
          - Communicate milestones to all teams.
6. Post-Migration Optimization
     - Monitoring and Tuning
          - Use CloudWatch, X-Ray, Trusted Advisor, Compute Optimizer.
          - Cost tracking via Cost Explorer, Budgets.
     - Security Hardening
          - Apply least privilege principle.
          - Enable AWS WAF, Shield, Security Hub.
     - Decommission On-Prem
          - Backup old systems.
          - Securely wipe and decommission legacy infra.

#### Migrating AWS to Azure
- multi-phase project that requires careful planning, execution, post-migration validation

1. Assessment & Discovery
     - Inventory & Assessment
          - Use tools like Azure Migrate, Azure Assessment and Migration Toolkit (AAMT), or Movere to scan AWS resources.
          - Document workloads: EC2, RDS, Lambda, S3, Route53, IAM, VPC, EBS, ELB, etc.
          - Tag workloads for criticality, dependency, and licensing.
     - Dependency Mapping
          - Use tools like Dynatrace, Application Dependency Mapping, or AWS X-Ray to visualize app dependencies.
          - Identify tightly coupled systems and network latencies.
     - Cost Analysis
          - Estimate Azure cost with the Azure Pricing Calculator.
          - Consider Reserved Instances, Hybrid Use Benefits, and savings plans.
     - Regulatory & Compliance
          - Map data residency and compliance (HIPAA, GDPR, SOC2) requirements.
2. Migration Strategy & Design
     - Migration Approaches
          - Lift and Shift (Rehost): Quickest, minimal changes.
          - Replatform: Replace AWS services with Azure equivalents (e.g., RDS → Azure SQL).
          - Refactor: Modify the app to use Azure-native PaaS or serverless options.
3. Execution Plan
     - Networking
          - Set up Azure Virtual Network with appropriate subnets, NSGs, route tables.
          - Connect AWS and Azure via VPN or ExpressRoute for hybrid cloud or migration bridge.
     - Identity Management
          - Sync AWS IAM roles/groups with Azure Active Directory.
          - Use Azure AD Connect for hybrid authentication (if on-prem AD exists).
     - Storage Migration
          - Use AzCopy, Azure Storage Migration Tool, or Blobfuse to move S3 → Blob.
          - or file storage, use Azure File Sync or Robocopy over VPN.
     - Compute Migration
          - For EC2 VMs:
               - Use Azure Migrate: Server Migration Tool or Veeam.
               - Validate VM sizing with Azure SKU recommendations.
               - Migrate test/staging instances first.
          - For Containers:
               - AWS ECS/EKS → Azure AKS
               - Use Helm, ArgoCD, Terraform, or Flux for GitOps-enabled deployments.
     - Database Migration
          - Use Azure Database Migration Service (DMS).
          - Map engine compatibility (e.g., PostgreSQL, MySQL, SQL Server).
          - Ensure downtime window or perform online replication before cutover.
     - Application & API
          - Re-deploy application layers to Azure App Services / Functions.
          - Use Application Gateway or Azure Front Door for routing and CDN.
          - Validate configs: secrets, environment variables, endpoints.
     - Monitoring & Logging
          - Replace CloudWatch with Azure Monitor, Log Analytics, Application Insights.
          - Set alerts and dashboards for health checks and metrics.
     - Data Backup & Replication
          - Set up Azure Backup and Azure Site Recovery (ASR) for VMs.
          - Replicate critical databases using Geo-redundant storage (GRS).
4. Rollback, DR, and Cutover
     - Rollback Plan
          - Maintain AWS environment in a read-only or standby state for initial weeks.
          - Document and test rollback steps to revert DNS, apps, and data to AWS.
          - Automate rollback via scripts or CI/CD pipelines.
     - Disaster Recovery Plan
          - Set up ASR (Azure Site Recovery) to replicate to a secondary region.
          - Design RTO (Recovery Time Objective) and RPO (Recovery Point Objective) for each workload.
          - Periodic DR drills with stakeholders.
     - Cutover Plan
          - Perform DNS cutover during off-peak hours.
          - Monitor application performance and error logs post-switch.
          - Validate every tier—storage, compute, DB, API, frontend.
          - Notify stakeholders of completion and failback window.
5. Post-Migration & Optimization
     - Stabilization
          - Monitor for errors, latency, performance degradation.
          - Implement autoscaling policies.
     - Cost Optimization
          - Review Azure Cost Management dashboards.
          - Set up budgets and alerts.
          - Right-size VMs, reserve instances, use spot VMs where possible.
     - Governance & Security
          - Enforce Azure Policies, RBAC, Security Center.
          - Integrate with Microsoft Defender for Cloud for security posture.
     - Documentation & Handoff
          - Update all documentation, diagrams, runbooks.
          - Train operations and support teams on Azure tools.

#### Troubleshooting Cloud Migration Issues
Scenario 1: **High Latency After Migration**
- Issue: Users report increased latency in the cloud environment.
     - Check networking configuration (VPC peering, security groups).
     - Verify DNS resolution (Route 53, Azure DNS).
     - Optimize database connections (RDS connection pooling, read replicas).
     - Use CDN (CloudFront, Azure CDN) for static assets.
- Fix: Enabled AWS Global Accelerator → Reduced response time by 40%.

Scenario 2: **Data Loss During Migration**
- Issue: Some database records are missing after migration.
     - Check data integrity before & after migration (checksum validation).
     - Enable CDC (Change Data Capture) for real-time sync.
     - Implement DB snapshots & rollbacks.
     - Use AWS DMS / Azure DMS for seamless migration.
- Fix: Re-ran migration with incremental sync → No data loss reported.

Scenario 3: **Cost Spikes After Migration**
- Issue: Cloud costs unexpectedly increased post-migration.
     - Identify unused resources (orphaned instances, unoptimized storage).
     - Right-size instances with AWS Compute Optimizer / Azure Advisor.
     - Move workloads to spot instances / reserved instances.
     - Enable budget alerts & anomaly detection.
- Fix: Moved 50% of workloads to Spot instances → Reduced cost by 35%.

Scenario 4: **Application Downtime After Cutover**
- Issue: Users report frequent timeouts after the final cutover.
     - Check load balancer health checks (ALB, NLB).
     - Verify DNS propagation delays.
     - Debug database connection timeouts.
     - Use auto-healing mechanisms (Kubernetes liveness probes, self-healing scripts).
- Fix: Adjusted LB health check thresholds → Fixed downtime issues.

#### Critical service is unresponsive (HTTP 500 errors spike).
     - Alert fires in Prometheus Alertmanager.
     - Lambda function triggers a Kubernetes job that restarts pods.
     - If issue persists, an auto-remediation script runs a rollback via ArgoCD.
     - On-call SRE is notified via alert messaging

#### Versions Software
- version
     - Docker: 20.10.17 (released February 15, 2022)
     - Kubernetes: 1.25.0 (released August 31, 2022)
     - Jenkins: 2.361 (released February 8, 2023)
     - GitLab CI/CD: 15.0 (released May 22, 2022)
     - CircleCI: 2.1 (released November 15, 2022)
     - Git: 2.39.1 (released February 13, 2023)
     - Prometheus: 2.39.1 (released February 15, 2023)
     - Grafana: 9.3.6 (released February 14, 2023)
     - ELK Stack (Elasticsearch, Logstash, Kibana): 8.6.0 (released February 1, 2023)
     - Terraform: 1.3.7 (released February 16, 2023)
     - Ansible: 2.14.1 (released February 7, 2023)

#### HTTP Error Codes

|Error Code|Category|Meaning|Possible Cause|
|-|-|-|-|
|400|Client Error (4xx)|Bad Request|Invalid input, malformed request|
|401|Client Error (4xx)|Unauthorized|Authentication required but not provided|
|403|Client Error (4xx)|Forbidden|User does not have permission|
|404|Client Error (4xx)|Not Found|Resource does not exist|
|405|Client Error (4xx)|Method Not Allowed|HTTP method (GET, POST, etc.) is incorrect|
|408|Client Error (4xx)|Request Timeout|Client took too long to send a request|
|429|Client Error (4xx)|Too Many Requests|Rate limiting exceeded|
| 500 | Server Error (5xx) | Internal Server Error | Generic server failure | 
| 502 | Server Error (5xx) | Bad Gateway | Invalid response from upstream server | 
| 503 | Server Error (5xx) | Service Unavailable | Server is overloaded or under maintenance | 
| 504 | Server Error (5xx) | Gateway Timeout | Timeout waiting for a response from another service |

#### Git Commands

|Command|Description|
|-|-|
|git init|Initialize a new Git repository|
|git clone <repo_url>|Clone an existing repository|
|git status|Check the status of your working directory|
|git add <file>|Stage a file for commit|
|git add .|Stage all changes for commit|
|git commit -m "message"|Commit changes with a message|
|git commit --amend -m "new message"|Modify the last commit message|
|git log|View commit history|
|git show <commit_hash>|Show details of a specific commit|
|git diff|See unstaged changes|
|git diff --staged|See staged changes|
|git branch|List all branches|
|git branch <branch_name>|Create a new branch|
|git checkout <branch_name>|Switch to another branch|
|git checkout -b <branch_name>|Create and switch to a new branch|
|git merge <branch_name>|Merge a branch into the current branch|
|git rebase <branch_name>|Rebase one branch onto another|
|git branch -d <branch_name>|Delete a local branch|
|git push origin --delete <branch_name>|Delete a remote branch|
|git remote -v|List remote repositories|
|git remote add origin <repo_url>|Add a remote repository|
|git push origin <branch_name>|Push a branch to a remote repository|
|git fetch origin|Fetch changes from a remote repository|
|git pull origin <branch_name>|Pull updates from a remote repository|
|git push --force origin <branch_name>|Force push changes (use with caution)|
|git stash|Save uncommitted changes temporarily|
|git stash pop|Apply the last stashed changes|
|git stash list|View all stashed changes|
|git stash drop|Delete a specific stash|
|git clean -f|Remove untracked files|
|git reset <file>|Unstage a file|
|git reset --hard|Reset the working directory to the last commit|
|git reset --soft HEAD~1|Undo the last commit but keep changes|
|git revert <commit_hash>|Revert a commit and create a new commit|
|git checkout -- <file>|Discard changes in a file|
|git tag|List all tags|
|git tag -a v1.0 -m "Version 1.0"|Create an annotated tag|
|git push origin --tags|Push tags to a remote repository|
|git config --global user.name "Your Name"|Set your Git username|
|git config --global user.email "you-example.com"|Set your Git email|
|git config --global core.editor "vim"|Set the default editor|
|git reflog|View history of HEAD changes|
|git bisect start|Start binary search for a bug|
|git cherry-pick <commit_hash>|Apply a specific commit to the current branch|

#### cherry-pick
- git cherry-pick is a Git command used to apply a specific commit from one branch onto another.
- Unlike merging or rebasing, which bring in multiple commits, cherry-picking allows you to select and apply only specific commits.
- Common Use Cases
     - Applying a bug fix from one branch (e.g., hotfix) to another (e.g., main).
     - Copying a feature commit from one branch to another without merging the entire branch.
     - Selectively integrating commits from different branches.

#### Git Merge
- git merge: Combines histories of two branches while preserving all commits. Creates a new "merge commit"
     - You want to retain the original history.
     - Collaboration is ongoing, and you want to avoid rewriting public commits.

#### Git Rebase
- git rebase: Rewrites commit history by placing your branch’s commits on top of another branch, producing a linear history.
     - You need a clean, linear history.
     - Before merging feature branches to avoid clutter in the main branch.

#### PR - Pull Request
- it is to propose, review, merge changes from one branch into another—usually from a feature branch into a main branch (main, master).
- PR Works (Simplified Flow):
     - Developer creates a new branch from the main/develop.
     - Makes changes and pushes the branch to the remote repo.
     - Opens a Pull Request to merge the branch into main or develop.
     - CI/CD pipelines are triggered automatically (e.g., tests, lint, build, security scan).
     - Reviewers (code owners/team) provide feedback or approve.
     - Once all checks and approvals pass, the PR is merged.
     - Optionally, the branch is deleted post-merge.
- PRs are Important
     - **Code Quality & Peer Review**: Enforces code reviews for consistency and maintainability.
     - **Automated Validation**: Triggers CI/CD pipelines (e.g., Jenkins, GitHub Actions, Azure DevOps) to test changes.
     - **Auditability**: Each PR records who made what change, when, and why.
     - **Security Compliance**: Allows integration of tools like secrets scanners, dependency checkers, and policy as code.
     - **Promotes Collaboration**: Encourages team feedback, especially in infrastructure as code (Terraform, Ansible, Helm).

#### Git conflicts during a pull request - PR
- Answer:
     - Identify the conflicting files using the GitHub/GitLab interface or CLI.
     - Use git status to locate conflict markers (<<<<<<<, =======, >>>>>>>).
     - Manually resolve the conflicts by editing the files.
     - Run git add <file> for resolved files and commit with a clear message.
     - Push the updated branch to update the PR.
     - Optionally rebase or squash to maintain a clean commit history.

#### Git - Rollback in Git if a PR breaks the pipeline
- Answer:
     - Identify the bad commit using git log.
     - Revert the merge: - git revert -m 1 <merge_commit_sha> (for PR merges).
     - Or rollback to a previous state: - git reset --hard <previous_commit> (not recommended on shared branches).
     - Push changes and trigger rollback CI/CD workflows.
     - Alternatively, use tagged release rollbacks in GitOps pipelines.

#### Layers
- **TCP/IP Model (real-world stack) simplifies it into 4 layers:**
     - Application (L7-L5) → HTTP, DNS
     - Transport (L4) → TCP, UDP
     - Internet (L3) → IP, BGP, ICMP
     - Network Interface (L2-L1) → Ethernet, Wi-Fi
       
- **Layer 7 (Application Layer)**
     - Load Balancers (L7 Load Balancing) – AWS ALB, Nginx, HAProxy manage HTTP traffic.
     - Web Security – WAF (Web Application Firewall) protects against SQL Injection, XSS.
     - API Gateways – AWS API Gateway, Kong manage API traffic.
- **Layer 4 (Transport Layer)**
     - L4 Load Balancing – AWS NLB, F5, and IP-based routing.
     - TCP vs UDP Decision Making – TCP (reliable), UDP (faster for streaming).
     - Connection Timeout Handling – Managing retries, congestion control.
- **Layer 3 (Network Layer)**
     - IP Addressing & Subnetting – Configuring AWS VPCs, Kubernetes networking.
     - BGP & Routing – Troubleshooting cross-region traffic (e.g., AWS Transit Gateway).
     - Firewalls & Security Groups – Allow/block traffic using IP rules.
- **Layer 2 (Data Link Layer)**
     - VLANs & Switches – Segmenting traffic in private networks.
     - MAC Address Filtering – Restricting devices in secured environments.

### Kafka 
- Apache Kafka is a distributed event streaming platform used for building real-time data pipelines and streaming applications.
- It is designed to handle high throughput, fault tolerance, scalability.
- Key Components:
     - **Producer** – they Sends messages to Kafka topics
     - **Broker** – Kafka server that stores and serves messages
     - **Topic** – those are Logical grouping of messages.
     - **Partition** – those splits topics into partitions for parallel processing.
     - **Consumer** – Reads messages from kafka topics.
     - **Zookeeper** – it coordination cluster, Manages metadata, leader election, health checks.
- Kafka is log-based, ensuring high durability and performance, making it ideal for real-time data processing and event-driven architectures.

#### Kafka Data Flow
     - Step-by-step message flow in Kafka:
          - Producer sends a message → Kafka selects a partition (round-robin or key-based).
          - Broker stores the message in a partition and replicates it to followers.
          - Consumers pull messages from the topic at their own pace.
          - Offsets are committed (either automatically or manually).
          - Messages expire based on retention policies (log.retention.ms).

#### ArgoCD Architecture
- its a declarative, GitOps-based continuous delivery (CD) tool for Kubernetes. It automates application deployment, syncing, and rollback in a Kubernetes environment.
     - GitOps-based deployment (declarative & version-controlled).
     - Automatic syncing of applications from Git repositories.
     - Supports Helm, Kustomize, Jsonnet, and raw YAML.
     - Provides a GUI, CLI, and API for managing deployments.
     - Supports multi-cluster deployments from a single ArgoCD instance.
     - Self-healing: Detects drift and automatically restores the desired state
- **components**:
     - **API Server**
          - Provides a UI, CLI, and API for managing applications.
          - Handles authentication & authorization.
     - **Repository Server**
          - Fetches application manifests from Git repositories.
          - Supports Helm charts, Kustomize overlays, Jsonnet, and raw YAML files.
     - **Application Controller**
          - Monitors the desired state from Git and compares it to the cluster’s actual state.
          - Syncs applications by applying Kubernetes manifests.
          - Triggers rollbacks if a deployment fails.
     - **Dex (Optional)**
          - Handles Single Sign-On (SSO) authentication (OAuth, LDAP, GitHub, SAML).

- ArgoCD is a Kubernetes controller :
     - Watches a Git repository for changes.
     - Synchronizes the cluster state with the Git repository.
     - Provides real-time status monitoring & rollback capabilities.

#### ArgoCD Workflow
     - Developers push changes to a Git repository.
     - ArgoCD detects the changes and applies them to the Kubernetes cluster.
     - The application state is reconciled, ensuring that the live cluster matches the desired state

#### ArgoCD Key Features
     - Declarative and GitOps-driven – Follows GitOps best practices.
     - Automated Syncing – Ensures Kubernetes manifests are applied consistently.
     - Multi-cluster Support – Manages multiple Kubernetes clusters.
     - Web UI & CLI – Offers a user-friendly interface and powerful CLI.
     - RBAC & SSO – Secure access with role-based access control and single sign-on.
     - Rollback & History Tracking – Reverts to previous states easily.

#### ArgoCD steup
- Install ArgoCD
```
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
```
Access the ArgoCD UI
```
kubectl port-forward svc/argocd-server -n argocd 8080:443
```
Login to ArgoCD
```
kubectl get secret -n argocd argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
argocd login localhost:8080 --username admin --password <password>
```
Deploy an Application
```
argocd app create my-app \
  --repo https://github.com/my-org/my-app.git \
  --path k8s-manifests \
  --dest-server https://kubernetes.default.svc \
  --dest-namespace default
```
Sync the Application
```
argocd app sync my-app
```

#### Nexus
- Nexus Repository Manager (by Sonatype) is a universal binary and artifact repository used to store, manage, and distribute artifacts such as:
- It acts as a centralized repository for artifacts and dependencies, improving build performance, security, and artifact version control in CI/CD pipelines.
     - Maven (Java JARs)
     - Docker images
     - NPM (JavaScript packages)
     - Helm charts (Kubernetes)
     - Python (PyPI), RubyGems, NuGet, etc
- Key Features:
     - Artifact Storage: Stores JARs, Docker images, Helm charts, etc.
     - Proxy Repositories: Caches remote repositories like Maven Central, Docker Hub
     - Security & Access Control: Role-based access control (RBAC)
     - CI/CD Integration: Works with Jenkins, Azure DevOps, GitHub Actions
- Use Cases:
     - CI/CD Pipelines → Store and pull dependencies (Maven, Docker, Helm)
     - Artifact Versioning → Keep track of releases
     - Cache External Dependencies → Speed up builds by caching artifacts

#### JFrog Artifactory 
- it is a universal binary and artifact management tool used to store, manage, and distribute software artifacts across multiple technologies, including:
     - Maven (JARs)
     - Docker images
     - NPM (JavaScript)
     - Helm charts (Kubernetes)
     - PyPI (Python), NuGet (.NET), RubyGems, etc.
     - It optimizes CI/CD pipelines by acting as a centralized repository, providing artifact versioning, security, caching, and integration with DevOps tools.
- Key Features of JFrog Artifactory
     - Universal Repository: Supports multiple artifact types (Docker, Helm, Maven, etc.)
     - Remote Caching: Acts as a proxy for repositories like Maven Central, Docker Hub
     - Security & Access Control: Role-based access control (RBAC)
     - CI/CD Integration: Works with Jenkins, Azure DevOps, GitHub Actions
     - Replication & High Availability: Sync artifacts across multiple regions
- Create Repositories
     - JFrog Artifactory has three types of repositories:
          - Local Repositories → Stores internally built artifacts
          - Remote Repositories → Proxies external repositories (e.g., Maven Central)
          - Virtual Repositories → Combines local and remote repositories for unified access
     - Example: Create a Maven Local Repository (maven-local).

- Use Cases for JFrog Artifactory
     - Centralized Artifact Management → Store Java, Python, Docker, Helm artifacts
     - CI/CD Pipeline Integration → Jenkins, GitHub Actions, Azure DevOps
     - Performance Optimization → Cache dependencies for faster builds
     - Secure Software Supply Chain → Scan for vulnerabilities using JFrog Xray

#### GitHub
- GitHub is a cloud-based Git repository hosting service that allows teams to manage version control, collaborate on code, and integrate CI/CD pipelines.
- Key Features
     - Code versioning and collaboration.
     - GitHub Actions for CI/CD.
     - Webhooks and API integrations.
     - Branch protection rules for code security.
     - Package and artifact management.
- Use Case:
     - Version control for microservices development.
     - Automated CI/CD pipelines using GitHub Actions.
     - Managing infrastructure-as-code repositories (Terraform, Ansible).
- Example : Hosting a Java project that is built using Maven and deployed via GitHub Actions.
- Main Configuration Files
     - **.github/workflows/*.yml** -	Defines GitHub Actions CI/CD pipelines
     - **.gitignore** -	Excludes specific files from Git tracking
     - **README.md** -	Documentation for the repository

#### GitHub Actions
- CI/CD and automation tool that allows you to create workflows triggered by GitHub events like push, pull_request, release, etc.
- It uses YAML files stored in .github/workflows/.  ( <workflow-name>.yml )
- key components
     - Workflow: .yaml file defining the automation
     - Job: A set of steps that runs on the same runner
     - Step: A command or action executed in a job
     - Runner: Machine that executes jobs (GitHub-hosted or self-hosted)
     - Action: Reusable unit of code (Docker, JavaScript, composite)

#### GitHub Actions - trigger workflows
```
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:  # manual trigger
```

#### GitHub-hosted and self-hosted runners
- GitHub-hosted: Managed by GitHub; supports Ubuntu, macOS, Windows.
- Self-hosted: Managed by you; can be used for custom environments, private networks, or persistent tooling.

#### GitHub-hosted - Pass secrets securely
- Define secrets in Repo Settings > Secrets and variables > Actions.
- Access them in workflows like this:
     - env: API_KEY: ${{ secrets.API_KEY }} 

#### GitHub-hosted - matrix build
- Run jobs in parallel with different configurations.
- Useful for testing across:
     - Multiple OS versions
     - Multiple language runtimes
     - Different feature toggles

```
strategy:
  matrix:
    node: [14, 16, 18]
```

#### GitHub Actions with Terraform integration
- Use the official hashicorp/setup-terraform action:
- Use secrets to inject cloud credentials securely.
```
- uses: hashicorp/setup-terraform@v2
- run: terraform init && terraform plan
``` 

#### GitHub Actions - handle failures and rollbacks
- Add error handling with continue-on-error or custom rollback steps.
- Integrate with monitoring/alerts (e.g., Slack, Datadog).
- Use approval steps for production to prevent accidental deploys
  
#### GitHub Actions -Key Features of GitHub Actions
     - YAML-based Workflows: Define automation as code using .github/workflows/*.yml
     - Supports Multi-Stage CI/CD Pipelines: Build, test, and deploy applications
     - Runs on GitHub-Hosted or Self-Hosted Runners: Uses Linux, Windows, or macOS
     - Built-in Integration with GitHub: Automate PRs, code scanning, and deployments
     - Marketplace for Actions: Reusable workflows from the GitHub community
     
#### GitHub Actions - Common Use Cases:
     - CI/CD Pipelines: Build, test, and deploy apps (Docker, Kubernetes, Terraform, Helm)
     - Automated Security Scans: Run SonarQube, Trivy, Snyk before deployment
     - Infrastructure as Code (IaC) Deployment: Terraform, Ansible, ARM templates
     - Monitoring & Alerts: Automate alerting with Prometheus & Grafana
     - Automated Rollbacks: Revert to previous versions if a deployment fails
     
#### GitHub Actions - Setting Up - Create a GitHub Actions Workflow
     - Inside your GitHub repository, go to Actions → New workflow.
     - Create a YAML file inside .github/workflows/ci-cd-pipeline.yml.
     - Define your workflow steps.
     
#### GitHub Actions - Security Best Practices
     - Use Secrets for Credentials: Store sensitive values in GitHub Secrets
     - Use Least Privilege Access: Restrict permissions for runners and workflows
     - Scan Dependencies for Vulnerabilities: Use Snyk, Trivy, or Dependabot
     - Limit Workflow Triggers: Avoid triggering workflows on all branches
     - Rotate and scope secrets
     - Use OIDC for cloud auth (e.g., AWS/GCP)
     - Avoid untrusted actions (pin action versions)
     - Set branch protection rules
     - Leverage Environment protection rules + manual approvals

#### GitHub Actions YAML – Build & Push Docker Image to Docker Hub
- .github/workflows/docker-build-push.yml
```
name: Build and Push Docker Image
on:
  push:
    branches:
      - main
  workflow_dispatch:
env:
  IMAGE_NAME: myapp
  DOCKER_HUB_USERNAME: ${{ secrets.DOCKER_HUB_USERNAME }}
  DOCKER_HUB_PASSWORD: ${{ secrets.DOCKER_HUB_PASSWORD }}
jobs:
  docker:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout Code
      uses: actions/checkout@v3
    - name: Log in to Docker Hub
      run: echo "${DOCKER_HUB_PASSWORD}" | docker login -u "${DOCKER_HUB_USERNAME}" --password-stdin
    - name: Build Docker Image
      run: docker build -t ${DOCKER_HUB_USERNAME}/${IMAGE_NAME}:latest .
    - name: Push Docker Image
      run: docker push ${DOCKER_HUB_USERNAME}/${IMAGE_NAME}:latest
```

- Go to your repo → Settings > Secrets and variables > Actions > New repository secret:
     - DOCKER_HUB_USERNAME → your Docker Hub username
     - DOCKER_HUB_PASSWORD → your Docker Hub access token or password

#### GitHub Actions fit into a CI/CD pipeline and how would you design one using it
- GitHub Actions integrates natively with repositories to enable seamless CI/CD. I’d structure a typical pipeline like this:
- CI Stage:
     - Trigger on push or pull_request
     - Run tests, linters, and security scans
     - Build artifacts or Docker images
- CD Stage:
     - Trigger on main merge or a tagged release
     - Deploy to staging/QA with manual approval to production
     - Use environment protection rules and secrets
- Example: A Node.js app can use matrix builds for multiple Node versions, push Docker images to GitHub Container Registry, deploy to Kubernetes using kubectl or helm.

#### GitHub Actions - optimize performance & costs 
- Key strategies include:
     - Use job-level caching (actions/cache) for dependencies (npm, pip, etc.)
     - Run tests in parallel using matrix strategy
     - Use self-hosted runners for long builds or special environments
     - Avoid unnecessary triggers (paths, if conditions)
     - Break large monolith workflows into modular reusable workflows
- Example: In one setup, we saved 40% build time by caching node_modules and reduced cloud cost by running integration tests on cheaper spot VMs with GitHub self-hosted runners.

#### GitLab 
- its a version control, CI/CD, issue tracking, security, monitoring, and Kubernetes integration in a single application.
- While GitHub primarily focuses on version control and collaboration, GitLab offers a built-in CI/CD pipeline engine and deeper integration for SRE/DevOps tasks without needing third-party tools.
- GitLab CI/CD automates software build, test, and deploy pipelines using the .gitlab-ci.yml file at the root of a repo.
- It works via:
     - Runners that execute jobs
     - Pipelines: Composed of stages and jobs
     - Artifacts: For sharing data between jobs
     - Triggers/Schedules: For automation
- use cases
     - CI/CD Pipelines: Automate builds, testing, security scans & deployments
     - GitOps for Kubernetes: Deploy applications using Helm & ArgoCD
     - Automated Security Scanning: Detect vulnerabilities using GitLab’s SAST & DAST
     - Infrastructure as Code (IaC): Deploy Terraform & Ansible configurations
     - Monitor Deployments: Track errors & observability metrics

#### GitLab Runner
- is a lightweight agent that executes CI/CD jobs defined in the .gitlab-ci.yml.
     - Types:
          - Shared Runners (provided by GitLab)
          - Specific Runners (custom deployed, Docker/Kubernetes/Shell)
     - Runners pull jobs from GitLab and execute them in isolated environments.
- .gitlab-ci.yml
     - Stages define the pipeline order.
     - Jobs belong to stages and run scripts.
     - Rules/Only/Except control job execution.
- it is a complete DevOps platform that provides:
     - Git-based Source Code Management (SCM)
     - Built-in CI/CD pipelines
     - Infrastructure as Code (IaC) automation
     - Security scanning (SAST, DAST, dependency scanning)
     - Monitoring & observability tools

 #### Gitlab Key Features
     - Git-based Version Control → Manage source code & repositories
     - Built-in CI/CD Pipelines → Automate testing & deployment
     - Container Registry → Store & manage Docker images
     - Security & Compliance → SAST, DAST, license scanning, secrets detection
     - Infrastructure as Code (IaC) → Automate Terraform & Kubernetes deployments
     - Self-hosted or SaaS → Install on-prem or use GitLab Cloud

#### GitLab Security Best Practices
- Details
     - Use Access Control (RBAC): Assign roles for Developers, Maintainers, Admins
     - Enable 2FA (Two-Factor Authentication): Secure GitLab logins
     - Use GitLab Security Scans: Enable SAST, DAST, Dependency Scanning
     - Restrict Pipeline Runners: Use Self-Hosted Runners for sensitive environments
- config files
     - **/etc/gitlab/gitlab.rb** - Main GitLab configuration (services, authentication, email, SSL, etc.)
     - .rb - Ruby Script
     - **/var/opt/gitlab/gitlab-rails/etc/gitlab.yml** - Application settings (repository storage, feature flags, hooks)
     - **.gitlab-ci.yml** (per repo) - Defines GitLab CI/CD pipelines
     - **/var/opt/gitlab/nginx/conf/gitlab-http.conf** - Manages GitLab web access using Nginx
     - **/etc/gitlab-runner/config.toml** - Configures GitLab Runners (Docker, Kubernetes, etc.)
     - .toml (Tom's Obvious, Minimal Language) file is a configuration file format designed for readability and ease of use

#### Webhook 
- its a mechanism that allows one system to notify another in real-time when an event occurs. It works by sending an HTTP POST request to a specified URL whenever a specific trigger occurs. Webhooks enable event-driven automation.
- Key Features of Webhooks
     - Asynchronous Communication – No need to poll for updates
     - Real-time Event Notifications – Instant updates when an event happens
     - Lightweight and Efficient – Uses HTTP requests instead of constant API polling
     - Customizable Actions – Triggered based on specific events

#### Webhooks Workflow 
     - **Event Occurs**: A user pushes code, a CI/CD pipeline finishes, or an issue is created.
     - **Webhook Triggers**: The system (e.g., GitHub, GitLab, Jenkins) sends an HTTP request.
     - **Target Endpoint Receives Data**: A service or application processes the payload.
     - **Action is Taken**: Automation scripts, CI/CD jobs, or alerts are triggered.

#### Webhooks in GitHub 
- Use Case: Trigger Jenkins Pipeline when a commit is pushed
     - Steps to Configure Webhooks in GitHub
          - Go to GitHub Repository → Settings → Webhooks → Add Webhook
          - Enter Payload URL → Example: http://jenkins.example.com/github-webhook/
          - Choose Content Type: application/json
          - Select Events to Trigger Webhook:
               - Push events (Code commit)
               - Pull request events
          - Click "Add Webhook"
     - Configuration File: .github/workflows/webhook.yml
#### Webhooks in GitLab 
- Use Case: Notify a monitoring system when a pipeline fails
     - Steps to Configure Webhooks in GitLab
          - Go to GitLab Project → Settings → Integrations
          - Enter Webhook URL: https://monitoring.example.com/webhook/
          - Select Trigger Events:
               - Pipeline events (When CI/CD fails)
               - Push events
          - Save Webhook
     - Configuration File: .gitlab-ci.yml

#### Webhooks in Kubernetes 
- Use Case: Restart a pod when a new container image is pushed
     - Steps to Configure Webhooks in Kubernetes
          - Deploy a Webhook Receiver (e.g., a small Python API)
          - Configure DockerHub Webhook: Send a request to the Kubernetes API
          - Webhook Server triggers kubectl rollout restart
     - Kubernetes Webhook Receiver (webhook-server.py)

#### Maven
- Maven is a Java-based build automation tool that manages project dependencies, builds, and deployments.
- Key Features
     - Dependency management using pom.xml.
     - Automated build lifecycle (compile, test, package, deploy).
     - Supports plugins for testing, reporting, and code quality.
- Use Case:
     - Compiling, testing, and packaging Java applications.
     - Managing dependencies via a central repository.
     - Deploying Java artifacts to repositories like Nexus or JFrog Artifactory.
- Example : Building a Spring Boot application and deploying it using Jenkins.
- Main Configuration Files
     - **pom.xml** - Defines dependencies, plugins, and build lifecycle

#### Gradle
- Gradle is a modern, flexible build automation tool used for Java, Kotlin, and Android projects.
- Key Features
     - Supports Groovy & Kotlin DSL.
     - Faster builds than Maven using incremental builds.
     - Dependency management via build.gradle.
     - Can integrate with Docker, Kubernetes, and CI/CD pipelines.
- Use Case:
     - Building Android applications.
     - Managing multi-module Java/Kotlin projects.
     - CI/CD automation for large-scale applications.
- Example : Automating builds for an Android app before deployment.
- Main Configuration Files
     - **build.gradle** -	Defines dependencies and build tasks
     - **settings.gradle** -	Configures Gradle project structure

#### NPM (Node Package Manager)
- npm is a package manager for JavaScript used to install, update, and manage dependencies for Node.js applications.
- Key Features
     - Manages JavaScript libraries and frameworks.
     - Dependency resolution via package.json.
     - Supports scripts for build, test, and deployment automation.
- Use Case:
     - Managing dependencies in React, Angular, and Vue.js projects.
     - Automating builds and tests using npm scripts.
     - CI/CD pipelines using GitHub Actions and Jenkins.
- Example : Deploying a React application using Docker and Kubernetes.
- Main Configuration Files
     - **package.json** -	Defines dependencies and project metadata
     - **package-lock.json** - Locks dependency versions

#### Postman
- Postman is an API testing tool used for designing, testing, debugging, and monitoring REST APIs.
- Key Features
     - Automates API testing.
     - Supports API mocking and documentation.
     - Integration with CI/CD pipelines for automated API testing.
- Use Case:
     - Validating API responses in a microservices environment.
     - Running Postman tests in GitHub Actions or Jenkins pipelines.
- Example : Testing a Spring Boot API that interacts with a PostgreSQL database.
- Main Configuration Files
     - **.postman_collection.json** -	Stores API test cases
     - **.postman_environment.json** -	Stores environment variables

#### JUnit
- JUnit is a unit testing framework for Java applications.
- Key Features
     - Automates unit testing in Java.
     - Integrates with Maven, Gradle, Jenkins, and GitHub Actions.
     - Supports test coverage reports using Jacoco.
- Use Case:
     - Writing unit tests for a Spring Boot microservice.
     - Running automated tests before deployment in CI/CD.
- Example : A JUnit test for a simple Java function.
- Main Configuration Files
     - **pom.xml** -	Configures JUnit dependencies
     - **build.gradle** -	Configures JUnit for Gradle projects

#### Selenium
- Selenium is an automation testing framework for UI testing of web applications.
- Key Features
     - Automates web browsers (Chrome, Firefox, Edge).
     - Supports Java, Python, JavaScript, and more.
     - Integrates with Jenkins for automated UI tests.
- Use Case:
     - End-to-end testing for a web application.
     - Running Selenium tests in CI/CD pipelines.
- Example : Automating login functionality testing in a React application.
- Main Configuration Files
     - **selenium-config.yaml** -	Stores browser configurations
     - **testNG.xml** -	Defines test execution sequence

#### JMeter
- Apache JMeter is a load testing tool for performance testing of APIs and web applications.
- Key Features
     - Simulates multiple users to test API or application performance.
     - Measures response time, throughput, and failures.
     - Integrates with CI/CD pipelines for automated performance testing.
- Use Case:
     - Testing API performance under heavy load.
     - Ensuring web application scalability before deployment.
- Example : Load testing a Kubernetes-based e-commerce application.
- Main Configuration Files
     - **test-plan.jmx** -	Defines the JMeter test plan
     - **jmeter.properties** -	Configures test execution

#### Node.js
- Node.js is a JavaScript runtime that allows developers to build scalable, server-side applications.
- Key Features
     - Asynchronous, event-driven architecture.
     - Used for backend services (APIs, microservices).
     - Package management via npm (Node Package Manager).
- Use Case:
     - Building REST APIs with Express.js.
     - Running microservices in Kubernetes.
- Example : Deploying a Node.js backend for a chat application.
- Main Configuration Files
     - **package.json** -	Defines project dependencies
     - **server.js** -	Contains application logic

#### SaaS Paas Iaas
#### Infrastructure as a Service (IaaS)
- IaaS provides virtualized computing resources over the internet. It offers essential infrastructure components such as compute, storage, and networking on a pay-as-you-go basis, allowing users to deploy and manage virtual machines and networks without maintaining physical hardware.
     - On-demand scalability of resources.
     - Pay-per-use pricing model.
     - Full control over infrastructure (OS, networking, and storage).
     - Provides virtual machines, storage, networking, and firewalls.

|Cloud Provider	|IaaS Services|
|  --- | --- |
|AWS	|EC2, S3, VPC, EBS, ELB, Route 53, IAM|
|Azure	|Virtual Machines, Blob Storage, Virtual Network, Load Balancer|
|Oracle Cloud	|OCI Compute, OCI Block Storage|
- Use Cases of IaaS:
     - Hosting websites and applications.
     - Creating development and testing environments.
     - Running big data and analytics workloads.
     - Backup and disaster recovery.

##### Platform as a Service (PaaS)
- PaaS provides a platform that allows developers to build, deploy, and manage applications without worrying about underlying infrastructure. It includes services such as databases, development frameworks, middleware, and runtime environments.
     - Provides a development and deployment environment.
     - Supports multiple programming languages and frameworks.
     - Auto-scaling and managed infrastructure.
     - Includes middleware, databases, and application hosting.

|Cloud Provider	|PaaS Services|
| -- | --- |
|AWS	|Elastic Beanstalk, AWS Lambda, RDS, API Gateway|
|Azure	|Azure App Service, Functions, Azure SQL Database|
- Use Cases of PaaS:
     - Rapid application development.
     - Microservices and serverless computing.
     - API management and integration.

#### Software as a Service (SaaS)
- SaaS delivers fully functional software applications over the internet. Users can access SaaS applications via a web browser without installing or maintaining them.
     - Hosted and managed by the provider.
     - Subscription-based pricing.
     - Accessible via web browsers or mobile apps.
     - Regular updates and maintenance by the provider.
- Use Cases of SaaS:
     - Business communication and collaboration.
     - Customer relationship management (CRM).
     - Project management and task tracking.
     - Cloud storage and file sharing.

### Automation

#### Automation - Infrastructure Provisioning Using Terraform & Ansible
- Problem Statement:
     - Manually provisioning AWS EC2 instances, configuring security groups, and setting up applications was time-consuming and error-prone.
- Solution:
     - Automated the entire infrastructure provisioning and configuration management using Terraform and Ansible.
- Implementation Steps:
     - Terraform to Create Infrastructure (EC2, Security Groups, S3, RDS)
          - Wrote a Terraform script to provision AWS EC2 instances and other required resources:
     - Ansible to Configure the Instances (Install Web Server, Setup Monitoring)
          - Created an Ansible playbook (**webserver.yml**) to configure the EC2 instances:
     - Automated Monitoring Setup (Prometheus & Grafana)
          - Used Ansible to deploy Prometheus and Grafana on the same instance.
          - Set up alerting for high CPU/memory usage.

#### Automation - CI/CD Pipeline Using GitHub Actions & Kubernetes
- Problem Statement
     -  Developers had to manually build, test, and deploy applications to Azure Kubernetes Service (AKS).
- Solution:
     - Automated the entire CI/CD process using GitHub Actions and Kubernetes.
- Implementation Steps:
     - GitHub Actions Workflow (.github/workflows/deploy.yml)
          - Created a GitHub Actions pipeline to build and deploy a Java Spring Boot application to AKS:
     - Integrated AKS with GitHub Actions
          - Used kubectl to deploy the new container image.
          - Used Azure Container Registry (ACR) to store images.
     - Automated Rollback on Failure
          - Used kubectl rollout undo deployment/myapp to revert to the previous working version if the new deployment failed.

#### Automation - Cloud Cost Optimization using Lambda & Terraform
- Problem Statement:
     -  Cloud resources (EC2, RDS, S3) were left running after office hours, causing unnecessary costs in a non-production AWS environment.
- Solution:
     -  Automated resource shutdown and startup using AWS Lambda and Terraform.
- Implementation Steps:
     - Terraform to Deploy AWS Lambda for Auto Shutdown
          - Created a Lambda function that stops EC2 instances outside business hours.
          - Terraform script to deploy Lambda & schedule it using AWS EventBridge:
     - Lambda Python Script (stop_instances.py)
          - Stops instances with a specific tag (Environment=Dev):
     - Automated Alerts using AWS SNS
          - Configured SNS to send alerts when instances stop.

#### Automation - Security Compliance Scanning with Trivy & Ansible
- Problem Statement:
     - Developers were pushing insecure Docker images to the AWS ECR registry, leading to security vulnerabilities.
- Solution:
     - Automated Docker image vulnerability scanning using Trivy & Ansible before deployment.
- Implementation Steps:
     - Integrated Trivy with Jenkins Pipeline
          - Added Trivy to the Jenkins pipeline to scan images before deployment:
          - Pipeline blocks the deployment if high-risk vulnerabilities are found.
     - Ansible to Enforce Security Hardening
	  - Created an Ansible playbook (harden-docker.yml) to enforce security best practices in Docker images:
     - Automated Compliance Reports
	  - Configured Trivy to send compliance reports via Slack for security teams.

#### Automation - AWS/Azure Auto-Scaling for Cost Optimization
- Problem Statement:
     - A high-traffic application had unpredictable spikes, leading to over-provisioned resources and increased cloud costs.
- Solution:
     - Implemented auto-scaling policies in AWS (Auto Scaling Groups) & Azure (VMSS) to dynamically adjust compute capacity.
- Implementation Steps (AWS Example):
     - Terraform to Define Auto Scaling Group & Policies:
     - Set up Scaling Policies:
     - Azure Equivalent (VM Scale Sets):
          - Used Azure VMSS (Virtual Machine Scale Sets) with Terraform.

#### Automation - Kubernetes Auto-Healing Using Self-Healing Pods
- Problem Statement:
     - If a Kubernetes pod crashed or became unhealthy, it required manual intervention to restart.
- Solution:
     - Used Kubernetes liveness probes & readiness probes to automatically restart failing pods.
- Implementation Steps:
     - Defined Health Probes in the Pod Definition:
     - Kubernetes automatically restarts the pod if it fails.
     - Implemented Horizontal Pod Autoscaler (HPA) to auto-scale pods:

#### Automation - Serverless Function for Auto-Cleanup of Stale Resources
- Problem Statement:
     - Unused AWS EBS volumes were left running, leading to wasted cloud costs.
- Solution:
     - Used AWS Lambda + CloudWatch to automatically delete unused EBS volumes after 7 days.
- Implementation Steps:
     - Python Lambda Function to Identify & Delete Unused Volumes:
     - Scheduled Lambda Execution Using CloudWatch (Runs Weekly)
     - Terraform to Deploy Lambda & EventBridge Trigger:

#### Automation - CI/CD Security Enforcement Using SonarQube & Trivy
- Problem Statement:
     - Developers accidentally pushed insecure code and vulnerable Docker images into production.
- Solution:
     -  Integrated SonarQube & Trivy into the CI/CD pipeline to block bad code & insecure images.
- Implementation Steps:
     - Added SonarQube to GitHub Actions Pipeline:
     - Added Trivy to Scan Docker Images:
     - Configured Pipeline to Fail if Critical Issues Are Found:

#### Automation - Infrastructure Compliance Auditing
- Problem Statement:
     - Teams deployed cloud resources without security best practices, leading to compliance violations.
- Solution:
     -  Used Cloud Custodian to enforce security policies automatically.
- Implementation Steps:
     - Created a Policy to Detect Unencrypted S3 Buckets:
     - Ran Cloud Custodian Periodically Using a Lambda Function

#### Branching Strategies
- I followed multiple branching strategies based on the project requirements, team size, and CI/CD workflows
- Branches:
     - **main (or master)** – Stable production branch.
     - **develop** – Integration branch for features.
     - **feature/*** – Individual feature branches.
     - **release/*** – Prepares code for a new release.
     - **hotfix/*** – For urgent production bug fixes.
- Workflow:
     - Developers create feature branches from develop.
     - Features are merged into develop after review.
     - Release branch is created from develop, tested, and merged into main & develop.
     - Hotfixes are made directly from main and merged back into develop.
- Which Strategy Did I Use
     - GitFlow for large-scale applications with multiple teams and staged releases.
     - GitHub Flow for fast, continuous deployment projects.
     - Trunk-Based Development for microservices running in Kubernetes (AKS/EKS).
     - Release Branching for enterprise clients requiring long-term support

#### High Latency in Production API
- Issue: A customer-facing API experiences a sudden increase in latency, leading to poor user experience. The issue occurs intermittently, but it’s becoming more frequent.
- **Investigation:**
     - Monitor logs and metrics: Used Prometheus + Grafana to check API response times.
     - Load analysis: Found that requests spiked before latency increased.
     - Database queries: Observed slow queries in PostgreSQL logs.
     - Infrastructure: Checked Kubernetes pod resource usage and noticed high CPU throttling.
- **Resolution:**
     - Optimized slow queries by adding indexes and reducing joins.
     - Increased CPU limits for Kubernetes pods to prevent throttling.
     - Enabled autoscaling to handle load spikes dynamically.
     - Implemented circuit breakers using Istio to handle failures gracefully.

#### Outage Due to Expired TLS Certificate
- Issue: A service goes down suddenly, returning TLS handshake failure errors. Customers report that they can’t connect.
- **Investigation:**
     - Checked logs: Found certificate expired error in Nginx logs.
     - Verified certificate: Ran openssl s_client to check expiry.
     - Looked at renewal process: Found that the Let’s Encrypt auto-renewal cron job had failed silently.
- **Resolution:**
     - Manually renewed the certificate to restore service (certbot renew).
     - Fixed cron job issues and added alerting if renewal fails.
     - Enabled certificate monitoring to avoid future outages using Cert-Manager in Kubernetes.

#### Pod CrashLoopBackOff
- Issue: A microservice running in Kubernetes is continuously restarting with CrashLoopBackOff status.
- **Investigation:**
     - Ran kubectl logs <pod> and found OOMKilled (Out of Memory).
     - Checked kubectl describe pod <pod> and saw memory limit exceeded.
     - Verified resource requests/limits in the Helm chart.
- **Resolution:**
     - Increased memory limits for the pod in Kubernetes YAML.
     - Implemented horizontal pod autoscaling (HPA) to distribute load.
     - Optimized the application to use less memory (e.g., reducing in-memory cache size).
 
#### Database Deadlocks Causing Downtime
- Issue: A high-traffic e-commerce platform experiences frequent database deadlocks, causing API timeouts.
- Investigation:
     - Checked database slow query logs and found frequent deadlocks.
     - Analyzed query patterns and noticed multiple updates to the same rows.
     - Reviewed transaction isolation levels and found unnecessary use of SERIALIZABLE.
- **Resolution:**
     - Changed transactions to use lower isolation levels (READ COMMITTED).
     - Refactored queries to reduce lock contention by using optimistic locking.
     - Introduced caching (Redis) to reduce database load.

#### CICD Pipeline Failure Before Deployment
- Issue: A CI/CD pipeline (GitHub Actions + ArgoCD) fails before deploying to production.
- Investigation:
     - Checked CI logs: Found build failures due to a missing dependency.
     - Reviewed Dockerfile: Saw an outdated base image that no longer exists.
     - Checked Kubernetes manifest changes: Found a misconfigured ConfigMap reference.
- **Resolution:**
     - Updated Docker base image to a maintained version.
     - Fixed the ConfigMap reference in kustomization.yaml.
     - Added unit tests & integration tests to prevent future failures.

#### DDoS Attack Causing Service Downtime
- Issue: A critical service is suddenly unreachable, with a spike in traffic from unknown sources. The CPU and memory of the servers are maxed out.
- **Investigation:**
     - Checked cloud provider logs (AWS WAF, GCP Cloud Armor, or Cloudflare).
     - Noticed a huge number of requests from specific IP ranges.
     - Found suspicious traffic patterns (e.g., thousands of requests per second to a single API endpoint).
- **Resolution:**
     - Blocked malicious IPs using a Web Application Firewall (WAF).
     - Rate-limited requests with Nginx and API Gateway.
     - Enabled autoscaling to absorb sudden load spikes.
     - Implemented bot detection to block automated traffic.

#### Kafka Consumers Lagging Behind
- Issue: A Kafka-based event-driven service is processing messages too slowly, causing lag to accumulate.
- **Investigation:**
     - Used kafka-consumer-groups.sh to check consumer lag.
     - Found high backlog in a few partitions.
     - Monitored consumer logs and saw timeouts and slow processing.
- **Resolution:**
     - Increased the number of consumer instances to parallelize processing.
     - Tuned Kafka fetch size and batch sizes to improve efficiency.
     - Optimized the application to process messages faster (e.g., by reducing external API calls).
     - Verified offset commits to ensure consumers weren’t getting stuck.

#### Incident Response for Database Disk Full
- Issue: A PostgreSQL database stops working because the disk is 100% full.
- **Investigation:**
     - Checked df -h and found the primary disk was out of space.
     - Ran du -sh * to identify large files and found huge WAL logs.
     - Looked at pg_stat_activity and saw long-running queries not vacuuming properly.
- **Resolution:**
     - Archived WAL logs to S3 and cleaned up old files.
     - Expanded disk size on the cloud provider (AWS/GCP/Azure).
     - Tuned autovacuum settings to prevent future bloat.
     - Implemented disk usage monitoring alerts using Prometheus

#### Nodes NotReady
- Issue: Multiple Kubernetes nodes go into NotReady state, causing service disruption.
- **Investigation:**
     - Ran kubectl get nodes and saw multiple nodes in NotReady.
     - Checked kubectl describe node <node> and found disk pressure and kubelet errors.
     - Logged into the node and found /var/lib/kubelet filled with orphaned containers.
- **Resolution:**
     - Cleared orphaned Docker containers using docker system prune.
     - Resized the node's disk storage in cloud settings.
     - Configured Node Autoscaler to prevent future resource exhaustion.
     - Set up alerts for disk usage and container failures.

#### Deployment Rollback After a Failed Release
- Issue: A new deployment to Kubernetes introduces a bug, causing 500 errors in production.
- **Investigation:**
     - Checked logs using kubectl logs -f <pod>.
     - Found a new environment variable missing, causing an application failure.
     - Validated with kubectl get configmap and saw an incorrect value.
- **Resolution:**
     - Used kubectl rollout undo deployment <deployment> to roll back to the previous working version.
     - Fixed the ConfigMap issue and redeployed safely.
     - Added pre-deployment checks in CI/CD to catch missing configurations.

#### Memory Leak in a Microservice
- Issue: A Go-based microservice running in Kubernetes keeps crashing due to out-of-memory (OOM) errors.
- **Investigation:**
     - Used kubectl top pods to check memory consumption.
     - Found a single pod consuming 5x more memory than expected.
     - Ran pprof to analyze memory usage and discovered unreleased goroutines causing memory leaks.
- **Resolution:**
     - Fixed the code to properly close goroutines and release memory.
     - Introduced heap profiling with pprof for future debugging.
     - Set memory limits in Kubernetes to prevent crashes.

#### CICD Pipeline Slows Down Due to Large Docker Images
- Issue: Developers complain that CI/CD pipelines take too long to deploy due to large Docker images.
- **Investigation:**
     - Analyzed Docker layers with docker history.
     - Found that the base image included unnecessary libraries.
     - Saw multiple COPY commands that increased image size.
- **Resolution:**
     - Switched to a lighter base image (e.g., Alpine instead of Ubuntu).
     - Used multi-stage builds to keep final images smaller.
     - Cached dependencies properly to speed up builds.

#### AWS S3 Latency Issues Impacting Web Application
- Issue: Users report slow loading times for images and assets stored in S3.
- **Investigation:**
     - Used AWS CloudWatch to check S3 request latencies.
     - Found that S3 requests were being routed to a different region.
     - Identified a misconfigured S3 bucket policy causing unnecessary redirects.
- **Resolution:**
     - Moved assets to an S3 bucket in the correct region.
     - Implemented CloudFront CDN caching to reduce latency.
     - Set lifecycle rules to optimize storage performance.     

#### Redis Connection Issues Leading to API Failures
- Issue: A Node.js application using Redis experiences intermittent connection timeouts.
- **Investigation:**
     - Checked Redis logs and saw maxclients errors.
     - Used redis-cli info to check connection stats.
     - Found that application instances weren’t closing idle Redis connections.
- **Resolution:**
     - Increased maxclients in Redis to allow more concurrent connections.
     - Added connection pooling with ioredis to prevent excessive connections.
     - Set timeouts and retries to handle transient Redis failures.

#### Slow Elasticsearch Queries Affecting Search Performance
- Issue: A search feature using Elasticsearch becomes slow, taking 5+ seconds to return results.
- **Investigation:**
     - Used /_cat/indices?v to check index health.
     - Found high shard count, leading to excessive overhead.
     - Checked /_cat/nodes?v and noticed high CPU usage on Elasticsearch master nodes.
- **Resolution:**
     - Rebalanced shards to optimize distribution.
     - Reduced replica count on low-traffic indices.
     - Used Elasticsearch caching for frequently queried data.
     - Added query profiling to optimize slow searches.

#### Cloudflare 502 Bad Gateway Errors on Production
- Issue: Users report frequent 502 errors when accessing the website through Cloudflare.
- **Investigation:**
     - Checked Cloudflare analytics and found increased 502 errors.
     - Looked at origin server logs and saw timeouts on API requests.
     - Used curl -v -H "Host: example.com" <origin-IP> to test direct access.
     - Found that the backend Kubernetes ingress controller was overloaded.
**Resolution:**
     - Increased backend timeouts in Nginx ingress.
     - Enabled origin keep-alive to reuse connections.
     - Deployed Cloudflare Argo Smart Routing for optimized traffic.
     - Scaled out backend API pods to handle increased load.

#### RDS Failover Causes Brief Downtime
- Issue: A production PostgreSQL RDS instance in AWS fails over unexpectedly, causing brief downtime.
- **Investigation:**
     - Checked AWS CloudWatch and found DB instance rebooted due to high CPU.
     - Queried pg_stat_activity and found long-running queries blocking writes.
     - Verified the Multi-AZ failover event in AWS.
- **Resolution:**
     - Optimized slow queries and added indexing to improve performance.
     - Implemented connection pooling (PgBouncer) to handle high loads.
     - Enabled read replicas to reduce the load on the primary instance.
     - Configured automatic retries in the application to handle failovers.

#### NetworkPolicy Blocks Internal Communication
- Issue: A newly deployed service cannot communicate with other services in the cluster.
- **Investigation:**
     - Used kubectl logs <pod> and found connection refused errors.
     - Ran kubectl describe networkpolicy <policy-name> and saw a restrictive deny-all policy.
     - Used kubectl get pods -o wide to check if the pod was scheduled correctly.
- **Resolution:**
     - Updated NetworkPolicy to allow egress and ingress for the affected service.
     - Verified connectivity using kubectl exec <pod> -- curl <service-url>.
     - Implemented Pod-to-Pod NetworkPolicy rules for better security.

#### Out-of-Sync Replica in MongoDB Cluster
- Issue: A MongoDB replica set member falls behind in replication, causing stale reads.
- **Investigation:**
     - Ran rs.status() and saw one secondary was lagging by 10 minutes.
     - Checked mongod.log and found write locks were slowing replication.
     - Noticed high disk I/O on the affected node.
- **Resolution:**
     - Increased write concern for critical operations to reduce replication lag.
     - Moved the affected replica to an SSD-backed instance.
     - Tuned journal settings to optimize writes.
     - Enabled oplog size monitoring to prevent replication issues.

#### AKS Cluster Autoscaler Not Working
- Issue: Kubernetes pods remain in Pending state despite autoscaling being enabled.
- **Investigation:**
     - Ran kubectl get events and found "Insufficient CPU" errors.
     - Used kubectl get nodes and saw no new nodes were being added.
     - Checked AKS logs and found a quota limit was hit.
- **Resolution:**
     - Increased VM quota limits in Azure.
     - Adjusted cluster autoscaler settings for faster scale-up.
     - Optimized resource requests to fit existing nodes better.

#### CICD Deployment Stuck Due to Helm Lock
- Issue: A Helm-based deployment hangs, and developers can’t release updates.
- **Investigation:**
     - Ran helm list and saw a pending release.
     - Used kubectl get pods and found init containers were stuck.
     - Checked kubectl describe pod and saw a failed ConfigMap mount.
- **Resolution:**
     - Deleted the stuck Helm release using helm rollback <release-name>.
     - Fixed the broken ConfigMap reference.
     - Enabled Helm automatic rollback for failed deployments.

#### RabbitMQ Queue Backlog Causes API Slowness
- Issue: A RabbitMQ queue accumulates millions of messages, slowing down API responses.
- **Investigation:**
     - Ran rabbitmqctl list_queues and found one queue was massively overloaded.
     - Used rabbitmqctl list_consumers and saw some consumers were down.
     - Checked dead-letter queues (DLQs) and found many messages were failing.
- **Resolution:**
     - Restarted failed consumers to start processing messages again.
     - Optimized message batching to process events more efficiently.
     - Implemented dead-letter queue reprocessing to prevent message buildup.

#### Terraform Apply Fails Due to Drift
- Issue: A Terraform apply command fails because the state file is out of sync with the infrastructure.
- **Investigation:**
     - Ran terraform plan and saw unmanaged changes.
     - Used terraform state list and found missing resources.
     - Discovered that a manual change was made in AWS, causing drift.
- **Resolution:**
     - Ran terraform import to sync the manually created resources.
     - Implemented GitOps for Terraform to prevent out-of-band changes.
     - Set up Terraform Cloud state locking to avoid conflicts.

#### Nginx Rate Limiting Blocks Legitimate Traffic
- Issue: A recently implemented Nginx rate limit blocks real users, causing HTTP 429 errors.
- **Investigation:**
     - Checked nginx.conf and found aggressive limit_req settings.
     - Reviewed logs and saw legitimate users hitting the limit.
     - Noticed API calls with high concurrency were getting blocked.
- **Resolution:**
     - Increased rate limits to match normal user behavior.
     - Implemented user-specific quotas instead of global rate limiting.
     - Used Redis-based rate limiting for more flexibility.

#### Lambda Cold Start Delays
- Issue: A serverless Lambda function experiences high latency due to cold starts.
- **Investigation:**
     - Checked AWS X-Ray traces and saw long initialization times.
     - Found the function was using a large container image.
     - Discovered the function wasn’t warm before traffic spikes.
- **Resolution:**
     - Reduced the Lambda package size by switching to a smaller runtime.
     - Used Provisioned Concurrency to keep functions warm.
     - Optimized dependencies to load faster.

#### Horizontal Pod Autoscaler (HPA) Not Scaling
- Issue: A service under high load isn’t scaling, even though HPA is enabled.
- **Investigation:**
     - Used kubectl describe hpa and saw that CPU usage wasn’t crossing the threshold.
     - Checked kubectl top pods and noticed CPU metrics weren’t being collected.
     - Found that metrics-server wasn’t running properly.
- **Resolution:**
     - Restarted metrics-server to restore monitoring.
     - Adjusted HPA threshold to react more dynamically.
     - Enabled custom metrics scaling for better accuracy

#### fault-tolerant architecture for a high-traffic application
- To ensure high availability (HA) and fault tolerance:
     - Microservices-based architecture with Kubernetes (EKS, AKS, GKE).
     - Load Balancers (AWS ALB/ELB, Nginx, HAProxy) for distributing traffic.
     - Auto-scaling (Kubernetes HPA/VPA, AWS Auto Scaling).
     - Multi-region deployment with cloud providers for failover.
     - Database replication & sharding (RDS Multi-AZ, MongoDB Replicas).

#### key components of a DevOps architecture
- DevOps architecture includes:
     - Version Control: Git, GitHub, GitLab, Bitbucket
     - CI/CD Pipeline: Jenkins, GitHub Actions, GitLab CI/CD, ArgoCD
     - Infrastructure as Code (IaC): Terraform, Ansible, CloudFormation
     - Containerization & Orchestration: Docker, Kubernetes, Helm
     - Monitoring & Logging: Prometheus, Grafana, ELK, Datadog
     - Cloud Providers: AWS, Azure, Google Cloud (GCP)
     - Security & Compliance: DevSecOps, HashiCorp Vault, SAST/DAST tools

#### implement a zero-downtime deployment
- steps
     - Blue-Green Deployment: Two environments (blue=active, green=staging) & switch traffic via load balancer.
     - Canary Deployment: Deploy to a small % of users before full rollout.
     - Rolling Updates: Gradually replace old pods with new ones in Kubernetes.
     - Feature Flags: Enable/disable features dynamically without redeploying.

#### Secrets in a DevOps pipeline
- points
     - Vault-Based Solutions: HashiCorp Vault, AWS Secrets Manager, Azure Key Vault.
     - Kubernetes Secrets: Encrypted at rest with RBAC.
     - Environment Variables: Store secrets outside the repo.
     - GitHub/GitLab Secrets: Secure storage for CI/CD pipelines.

#### Handle Kubernetes Pod failures
- steps
     - Use Probes: livenessProbe & readinessProbe.
     - Auto-scaling: Kubernetes HPA scales pods based on CPU/memory.
     - Cluster Autoscaler: Adds/removes nodes based on demand.
     - Restart Policy: Always, OnFailure, Never in pod specs.

#### Secure a DevOps pipeline
- Points:
     - Code Scanning: SAST (SonarQube), DAST (OWASP ZAP).
     - IAM Best Practices: Least privilege for AWS/GCP users.
     - Container Security: Use Trivy, Grype to scan images.
     - RBAC in Kubernetes: Restrict access to sensitive resources.

#### Disaster recovery in cloud architecture
- Points :
     - **Automated Backups**: RDS Snapshots, EBS Snapshots.
     - **Multi-Region Deployment**: Active-Active, Active-Passive setups.
     - **Chaos Engineering**: Simulate failures using Chaos Monkey, LitmusChaos.
     - **Failover Mechanisms**: Route 53 DNS failover, Load balancer health checks.
-  I’ve worked on a disaster recovery (DR) project where our goal was to ensure business continuity in the event of a regional failure or major outage. The application was hosted on AWS, and we implemented a multi-region DR strategy.
- To complete the project, I started by identifying the critical components that needed to be recovered — including EC2 instances, RDS databases, EKS clusters, S3 data, and Route 53 DNS configurations. We used Terraform to replicate infrastructure across the DR region, which made the setup consistent and manageable.
- For data, we configured cross-region replication for S3 and automated snapshots for RDS and EBS volumes. We also implemented backup validation scripts to ensure that backups were not only taken but also restorable.
- I helped build a runbook with clear recovery steps, including RTO (Recovery Time Objective) and RPO (Recovery Point Objective) targets. We regularly tested failover and failback scenarios to verify the DR plan worked as expected.
- Some of the key suggestions I made to improve the success of the DR activity included:
     - Automate everything — from infrastructure provisioning to DNS failover using tools like Route 53 health checks.
     - Regularly test the DR plan — we scheduled quarterly DR drills to validate readiness.
     - Monitor backup integrity — not just taking backups, but validating restores.
     - Document runbooks clearly — so anyone in the team can execute recovery steps under pressure.
     - Introduce chaos testing — to simulate real-world outages and measure recovery responses.
- The project was a success, those recommendations helped improve the reliability and confidence in our DR strategy.

#### RTO – Recovery Time Objective
- Definition: RTO is the maximum acceptable amount of time a system or service can be down after a failure before it must be restored to avoid significant business impact.
- In simple terms: How quickly do we need to recover?
- Use Case Example:
     - A payment gateway service for an e-commerce app has an RTO of 30 minutes. That means after a failure, the system must be fully restored and operational within 30 minutes to avoid lost revenue or customer trust.
 
#### RPO – Recovery Point Objective
- Definition: RPO is the maximum acceptable amount of data loss, measured in time, that can be tolerated due to an incident.
- In simple terms: How much data can we afford to lose?
- Use Case Example:
     - A customer order management system has an RPO of 5 minutes. This means backups or replication must happen frequently enough (every 5 minutes or less) so that if there's a failure, at most 5 minutes of data would be lost.

#### Design a multi-cloud architecture for high availability
- A multi-cloud architecture ensures fault tolerance and reduces vendor lock-in. Best practices:
     - **Global Load Balancing**: Use Cloudflare, AWS Route 53, or GCP Traffic Director.
     - **Multi-Region Deployments**: Deploy Kubernetes clusters in AWS (EKS), Azure (AKS), and GCP (GKE).
     - **Data Replication**: Use Google Spanner, AWS Aurora Global, or CockroachDB for multi-region DB replication.
     - **CI/CD Pipeline**: Use GitOps (ArgoCD, FluxCD) for consistency across clouds.

#### Microservices communicate efficiently in a distributed architecture
- To ensure low latency and reliable communication between microservices:
     - **Service Mesh**: Use Istio or Linkerd for traffic control, retries, and observability.
     - **Event-Driven Architecture**: Use Kafka, RabbitMQ, or AWS SNS/SQS for async messaging.
     - **API Gateway**: Deploy Kong, Apigee, or AWS API Gateway to centralize API requests.
     - **gRPC over REST**: Use gRPC for fast, binary communication when services are internal.

#### Implement a disaster recovery (DR) strategy for a mission-critical application?
- A disaster recovery plan should cover:
     - **Backup & Restore**: Use AWS Backup, Velero for Kubernetes, and RDS snapshots.
     - **Active-Passive Architecture**: Run services in primary & secondary regions, failover via Route 53 DNS.
     - **Chaos Engineering**: Test resilience using Gremlin, Chaos Monkey, or LitmusChaos.
     - **RTO & RPO Definition**: Define Recovery Time Objective (RTO) and Recovery Point Objective (RPO).

#### Implement GitOps in a Kubernetes environment
- GitOps automates deployments using Git as the single source of truth.
     - **Use ArgoCD or FluxCD**: Automatically sync Kubernetes manifests.
     - **Separate Repositories**: Maintain app code & infrastructure code separately.
     - **Declarative IaC**: Store Terraform/Kubernetes manifests in Git.
     - **Automated Rollbacks**: ArgoCD can roll back failed deployments.

#### Optimize CICD pipelines for large-scale applications
- Steps :
     - **Parallel Execution**: Run tests & builds in parallel (Jenkins, GitHub Actions).
     - **Build Caching**: Use Docker layer caching & dependency caching (npm, pip, Maven).
     - **Incremental Builds**: Only build changed modules (e.g., monorepo setups).
     - **Self-Hosted Runners**: Use GitHub Actions Self-Hosted Runners for faster performance.

#### Implement a secure Infrastructure as Code (IaC) pipeline?
- Steps
     - **Linting & Security Scanning**: Use Checkov, tfsec for Terraform, and Ansible Lint.
     - **Plan & Approval Stages**: Use terraform plan before applying changes.
     - **State Management**: Store Terraform state in AWS S3 with DynamoDB locking.
     - **Least Privilege IAM**: Use service accounts with minimal permissions.

#### Manage multi-environment deployments (dev/staging/prod) in Terraform
- Steps : 
     - Use Terraform Workspaces: Separate states for each environment.
     - Directory Structure: Use terraform/dev, terraform/staging, terraform/prod.
     - Backend State Separation: Store Terraform state separately per environment.
     - Use Variables: Manage environment-specific configurations with terraform.tfvars.

#### Design a Kubernetes cluster for scalability and reliability
- Steps : 
     - Auto-scaling: Use HPA (Horizontal Pod Autoscaler) & Cluster Autoscaler.
     - Resource Limits: Set CPU/Memory requests & limits to prevent noisy neighbors.
     - Pod Disruption Budgets (PDBs): Ensure minimum service availability.
     - Node Affinity & Taints/Tolerations: Distribute workloads efficiently.
     - Multi-Cluster Management: Use Rancher, KubeFed, or Istio Multi-Cluster.

#### Kubernetes networking issues
- Steps :
     - **Check Service & Pod Connectivity**: kubectl exec -it <pod> -- curl <service>
     - **Use kubectl get events**: Identify networking failures.
     - **Check Network Policies**: Ensure correct ingress/egress rules.
     - **Verify CNI Plugin Health**: (calicoctl node status, weave --status)
     - **Use traceroute & tcpdump**: Debug network paths & dropped packets.

#### secure a Kubernetes cluster
- Points :
     - **RBAC (Role-Based Access Control)**: Assign least privilege roles.
     - **Pod Security Policies**: Restrict root access & enforce network policies.
     - **Use Secrets Properly**: Store secrets in Kubernetes Secrets or HashiCorp Vault.
     - **Network Segmentation**: Enforce NetworkPolicies to isolate services.
     - **Scan Container Images**: Use Trivy, Clair, or Anchore.

#### Reduce Kubernetes startup time for large applications
- Points
     - Optimize Readiness & Liveness Probes:
          - Ensure pods don’t restart unnecessarily.
     - Use Pre-Warmed Containers:
          - Reduce cold start times.
     - Efficient Logging:
          - Use structured logs instead of excessive stdout logging.
     - Optimize Resource Requests:
          - Prevent over-provisioning.

#### security-first DevOps strategy (DevSecOps)
- points
     - Shift Left Security: Perform code scans during development (SAST).
     - Runtime Security: Use Falco, Aqua Security for container security.
     - IAM & Access Control: Enforce least privilege across AWS, GCP, Azure.
     - Secrets Management: Store secrets securely with HashiCorp Vault.

#### Application Gateway
- its one of the most powerful Layer 7 load balancers in Azure, especially when combined with AKS for secure ingress traffic. It provides SSL termination, URL-based routing, WAF, and host-based routing, making it a go-to choice for secure, scalable applications.

#### GitOps
its a DevOps practice that uses Git as the single source of truth for managing infrastructure and applications. It ensures that:
     - The desired state of the system is version-controlled in Git.
     - Changes are made via pull requests and automatically deployed.
     - The system continuously reconciles the actual state with the declared state.
- Key GitOps Principles:
     - **Declarative Configuration** – Everything is defined as code.
     - **Version Control** – Git tracks all changes.
     - **Automated Deployment** – Changes trigger deployments automatically.
     - **Continuous Reconciliation** – The system continuously ensures desired and actual states match

#### GitOps Works
- work flow
     1. Developers push changes to Git - Code, configuration, and Kubernetes manifests are stored in a Git repository.
     2. GitOps Operator (ArgoCD/FluxCD) detects the change - The GitOps tool monitors the repo and detects changes automatically.
     3. GitOps tool applies changes to the cluster - ArgoCD/FluxCD syncs the new state to Kubernetes.
     4. Continuous Reconciliation - If someone modifies a Kubernetes resource manually, GitOps detects drift and reverts it.
- Best Practices for GitOps
     - **Separate Repositories:** Maintain different repos for app code and infrastructure.
     - **Use Branching Strategies:** Use feature branches and PR approvals for deployments.
     - **Implement RBAC & Access Control:** Restrict who can modify Git repositories.
     - **Monitor GitOps Pipelines:** Use Prometheus, Loki, and Grafana to track deployments.
     - **Automate Secret Management:** Use Sealed Secrets, HashiCorp Vault, or AWS Secrets Manager instead of committing secrets to Git.

#### On-prem or onprem
- on-premises (on-prem) infrastructure, covering configuration management, monitoring, scaling, networking, disaster recovery, and hybrid cloud
- manage configuration and deployments
     - In on-prem environments, I typically use tools like Ansible, Puppet, or Chef for configuration management. These help enforce consistent state across nodes (e.g., package versions, system configurations).
     - For deployments, we use CI/CD servers like Jenkins, GitLab CI, or custom bash/Python scripts over SSH or WinRM. Deployment strategies like blue/green or canary can still be applied using reverse proxies like HAProxy or Nginx on-prem.
 
#### on-prem disaster recovery (DR)
- DR involves a mix of data backups, infrastructure snapshots, and redundant hardware setups:
     - Use rsync, Bacula, or Veeam for backups
     - Use RAID and clustering for data and hardware fault tolerance
     - Maintain a secondary data center or offsite cold standby
     - Document and automate infra rebuilds using Ansible or Terraform (for hybrid DR)

#### onprem vs Cloud - key challenges in managing
- Answer:
     - Scaling: Hardware procurement takes weeks vs minutes in cloud
     - Maintenance: Patching, power, cooling are manual
     - Monitoring: You own the stack end-to-end
     - Automation: Harder if legacy systems lack APIs
     - Security: Physical and network layer are your responsibility
 
#### on-prem - integrate with cloud - hybrid architecture
- We use VPN tunnels (IPsec, OpenVPN) or Azure ExpressRoute / AWS Direct Connect to connect on-prem with the cloud.
- Examples:
     - CI/CD in cloud deploying to on-prem via SSH or WinRM
     - Hybrid DNS with cloud providers syncing records from on-prem
     - Data sync via rsync or cloud storage gateways

#### User access and security
- Answer:
     - Use LDAP/Active Directory for centralized user management
     - Apply sudo policies, SSH key restrictions, and audit logs
     - Harden servers via CIS benchmarks, firewall rules (iptables/ufw), intrusion detection systems (IDS)
     - Enforce MFA through bastion hosts or VPN entry points

#### SDK - Software Development Kit
- its a python based , its a collection of tools, libraries, documentation, code samples, APIs that developers use to build applications for a specific platform or service.
- its development by providing pre-built components and standardized interfaces.
- SDK Components we do have :
     - APIs or libraries
     - Command-line tools
     - Documentation
     - Code samples / templates
     - Debugging or testing tools
- Use Cases of SDK
- Cloud Infrastructure Automation - AWS SDK for Python (boto3)
     - Use case: Automate EC2 instance creation, manage S3 buckets, handle IAM policies in your DevOps workflows.
- Integrating Payment Gateways - Stripe SDK (for Java, Node.js, etc.)
     - Use case: Quickly integrate payment processing in a web or mobile app.
- Monitoring & Observability - Datadog SDK / New Relic SDK
     - Use case: Send custom metrics or application traces from code to observability platforms.
- Mobile App Development - Android SDK
     - Use case: Build, test, and debug Android applications
- Custom Authentication / Identity - Azure AD SDK or Auth0 SDK
     - Use case: Integrate secure login, MFA, or token-based access in applications.
- CI/CD Integrations - GitHub Actions SDK
     - Use case: Build custom GitHub Actions using Node.js to automate builds, tests, or deployments.

#### SDKs help in CI/CD pipelines
- SDKs allow us to:
     - Automate infrastructure and cloud resource provisioning.
     - Integrate monitoring tools (e.g., push custom metrics via SDK).
     - Build custom logic in pipelines (e.g., uploading artifacts to S3 or Azure Blob Storage).
     - Trigger external services (e.g., send alerts via Slack SDK).

#### SDKs be used for automation
- Using the AWS SDK (boto3), we can write a Python script to:
     - Launch EC2 instances
     - Configure IAM roles
     - Deploy Lambda functions
     - Tag resources for cost tracking
     - This can then be integrated into Jenkins, GitHub Actions, or Azure DevOps pipelines.
